<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.112.3"><meta name=description content="Official documentation for DIAMBRA, the platform where coders create AI agents to compete in video games tournaments."><meta name=author content="DIAMBRA Team"><meta name=keywords content="Documentation,Reinforcement Learning,Deep Reinforcement Learning,AI Tournaments,Video Games Environments,RL Environments,DIAMBRA,Dueling AI Arena,DIAMBRA Documentation,AI Competitions,Artificial Intelligence"><meta itemprop=name content="DIAMBRA Docs"><meta itemprop=description content="Official documentation for DIAMBRA, the platform where coders create AI agents to compete in video games tournaments."><meta itemprop=image content="https://docs.diambra.ai/images/logoMeta.png"><meta property="og:site_name" content="DIAMBRA Docs"><meta property="og:title" content="DIAMBRA Docs"><meta property="og:description" content="Official documentation for DIAMBRA, the platform where coders create AI agents to compete in video games tournaments."><meta property="og:image" content="https://docs.diambra.ai/images/logoMeta.png"><meta property="og:url" content="https://docs.diambra.ai"><meta property="og:type" content="website"><meta name=publish_date property="og:publish_date" content="2022-01-15T00:00:00-0600"><meta charset=utf-8><link rel=icon href=/images/favicon.png type=image/png><title>Ray RLlib :: DIAMBRA Docs</title><link href=/css/nucleus.css?1685233622 rel=stylesheet><link href=/css/fontawesome-all.min.css?1685233622 rel=stylesheet><link href=/css/hybrid.css?1685233622 rel=stylesheet><link href=/css/featherlight.min.css?1685233622 rel=stylesheet><link href=/css/perfect-scrollbar.min.css?1685233622 rel=stylesheet><link href=/css/auto-complete.css?1685233622 rel=stylesheet><link href=/css/atom-one-dark-reasonable.css?1685233622 rel=stylesheet><link href=/css/theme.css?1685233622 rel=stylesheet><link href=/css/tabs.css?1685233622 rel=stylesheet><link href=/css/hugo-theme.css?1685233622 rel=stylesheet><link href=/css/theme-diambra.css?1685233622 rel=stylesheet><script src=/js/jquery-3.3.1.min.js?1685233622></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style><style type=text/css>pre code{white-space:pre}</style></head><body data-url=/handsonreinforcementlearning/rayrllib/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=https://docs.diambra.ai><img src=https://docs.diambra.ai/images/logo.png></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=/js/lunr.min.js?1685233622></script>
<script type=text/javascript src=/js/auto-complete.js?1685233622></script>
<script type=text/javascript>var baseurl="https://docs.diambra.ai"</script><script type=text/javascript src=/js/search.js?1685233622></script></div><section id=homelinks><ul><li><a class=padding href=https://docs.diambra.ai><i class='fas fa-home'></i> Home</a></li></ul></section><div class=highlightable><ul class=topics><li data-nav-id=/gettingstarted/ title="Getting Started" class=dd-item><a href=/gettingstarted/>Getting Started
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/gettingstarted/examples/ title=Examples class=dd-item><a href=/gettingstarted/examples/>Examples
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/gettingstarted/examples/singleplayerenv/ title="Single Player Environment" class=dd-item><a href=/gettingstarted/examples/singleplayerenv/>Single Player Environment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/gettingstarted/examples/multiplayerenv/ title="Multi Player Environment" class=dd-item><a href=/gettingstarted/examples/multiplayerenv/>Multi Player Environment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/gettingstarted/examples/wrappersoptions/ title="Wrappers Options" class=dd-item><a href=/gettingstarted/examples/wrappersoptions/>Wrappers Options
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/gettingstarted/examples/humanexperiencerecorder/ title="Human Experience Recorder" class=dd-item><a href=/gettingstarted/examples/humanexperiencerecorder/>Human Experience Recorder
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/gettingstarted/examples/imitationlearning/ title="Imitation Learning" class=dd-item><a href=/gettingstarted/examples/imitationlearning/>Imitation Learning
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li><li data-nav-id=/envs/ title=Environments class=dd-item><a href=/envs/>Environments
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/envs/games/ title="Games & Specifics" class=dd-item><a href=/envs/games/>Games & Specifics
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/envs/games/doapp/ title="Dead Or Alive ++" class=dd-item><a href=/envs/games/doapp/>Dead Or Alive ++
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/envs/games/sfiii3n/ title="Street Fighter III 3rd Strike" class=dd-item><a href=/envs/games/sfiii3n/>Street Fighter III 3rd Strike
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/envs/games/tektagt/ title="Tekken Tag Tournament" class=dd-item><a href=/envs/games/tektagt/>Tekken Tag Tournament
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/envs/games/umk3/ title="Ultimate Mortal Kombat 3" class=dd-item><a href=/envs/games/umk3/>Ultimate Mortal Kombat 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/envs/games/samsh5sp/ title="Samurai Showdown 5 Special" class=dd-item><a href=/envs/games/samsh5sp/>Samurai Showdown 5 Special
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/envs/games/kof98umh/ title="The King of Fighers '98 UMH" class=dd-item><a href=/envs/games/kof98umh/>The King of Fighers '98 UMH
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li><li data-nav-id=/wrappers/ title=Wrappers class=dd-item><a href=/wrappers/>Wrappers
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/utils/ title=Utils class=dd-item><a href=/utils/>Utils
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/imitationlearning/ title="Imitation Learning" class=dd-item><a href=/imitationlearning/>Imitation Learning
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/handsonreinforcementlearning/ title="Hands-on Reinforcement Learning" class="dd-item
parent"><a href=/handsonreinforcementlearning/>Hands-on Reinforcement Learning
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/handsonreinforcementlearning/stablebaselines3/ title="Stable Baselines 3" class=dd-item><a href=/handsonreinforcementlearning/stablebaselines3/>Stable Baselines 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/handsonreinforcementlearning/rayrllib/ title="Ray RLlib" class="dd-item
active"><a href=/handsonreinforcementlearning/rayrllib/>Ray RLlib
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/competitionplatform/ title="Competition Platform" class=dd-item><a href=/competitionplatform/>Competition Platform
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/competitionplatform/basicagentscript/ title="Basic Agent Script" class=dd-item><a href=/competitionplatform/basicagentscript/>Basic Agent Script
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/competitionplatform/submissionevaluation/ title="Submission Evaluation" class=dd-item><a href=/competitionplatform/submissionevaluation/>Submission Evaluation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/competitionplatform/howtosubmitanagent/ title="How To Submit An Agent" class=dd-item><a href=/competitionplatform/howtosubmitanagent/>How To Submit An Agent
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/competitionplatform/howtosubmitanagent/submitprebuiltagents/ title="Submit Pre-Built Agents" class=dd-item><a href=/competitionplatform/howtosubmitanagent/submitprebuiltagents/>Submit Pre-Built Agents
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/competitionplatform/howtosubmitanagent/submityourownagent/ title="Submit Your Own Agent" class=dd-item><a href=/competitionplatform/howtosubmitanagent/submityourownagent/>Submit Your Own Agent
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/competitionplatform/howtosubmitanagent/customdependenciesimage/ title="Custom Dependencies Image" class=dd-item><a href=/competitionplatform/howtosubmitanagent/customdependenciesimage/>Custom Dependencies Image
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/competitionplatform/argumentsandcommands/ title="Arguments and Commands" class=dd-item><a href=/competitionplatform/argumentsandcommands/>Arguments and Commands
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/competitionplatform/testyouragentlocally/ title="Test Your Agent Locally" class=dd-item><a href=/competitionplatform/testyouragentlocally/>Test Your Agent Locally
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/projects/ title=Projects class=dd-item><a href=/projects/>Projects
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/projects/rlztournament/ title="RLZ Tournament" class=dd-item><a href=/projects/rlztournament/>RLZ Tournament
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/projects/gamepainter/ title="Game Painter" class=dd-item><a href=/projects/gamepainter/>Game Painter
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://diambra.ai target=_blank><i class='fas fa-globe'></i> Website</a></li><li><a class=padding href=https://github.com/diambra/ target=_blank><i class='fab fa-fw fa-github'></i> GitHub</a></li><li><a class=padding href=https://diambra.ai/discord target=_blank><i class='fab fa-discord'></i> Discord</a></li><li><a class=padding href=https://www.twitch.tv/diambra_ai target=_blank><i class='fab fa-twitch'></i> Twitch</a></li><li><a class=padding href=https://www.linkedin.com/company/diambra target=_blank><i class='fab fa-linkedin'></i> Linkedin</a></li><li><a class=padding href=https://www.youtube.com/c/diambra_ai target=_blank><i class='fab fa-youtube'></i> YouTube</a></li><li><a class=padding href=https://twitter.com/diambra_ai target=_blank><i class='fab fa-fw fa-twitter'></i> Twitter</a></li><li><a class=padding href=https://streamlabs.com/diambra_ai/tip target=_blank><i class='fas fa-heart'></i> Support</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><center><a class=github-button href=https://github.com/diambra/arena/archive/refs/heads/main.zip data-icon=octicon-download aria-label="Download DIAMBRA Arena from GitHub">Download</a>
<a class=github-button href=https://github.com/diambra/arena data-icon=octicon-star data-show-count=true aria-label="Star DIAMBRA Arena on GitHub">Star</a>
<a class=github-button href=https://github.com/diambra/arena/fork data-icon=octicon-repo-forked data-show-count=true aria-label="Fork DIAMBRA Arena on GitHub">Fork</a></center>
<script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i></a></span>
<span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=/>Home</a> > <a href=/handsonreinforcementlearning/>Hands-on Reinforcement Learning</a> > Ray RLlib</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><ul><li><a href=#index>Index</a></li><li><a href=#getting-ready>Getting Ready</a></li><li><a href=#basic>Basic</a></li><li><a href=#advanced>Advanced</a></li></ul></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Ray RLlib</h1><div style=font-size:1.125rem><h3 id=index>Index</h3><ul><li><a href=./#getting-ready>Getting Ready</a><ul><li><a href=./#native-interface>Native Interface</a></li></ul></li><li><a href=./#basic>Basic</a><ul><li><a href=./#basic-example>Basic Example</a></li><li><a href=./#saving-loading-and-evaluating>Saving, Loading and Evaluating</a></li><li><a href=./#parallel-environments>Parallel Environments</a></li></ul></li><li><a href=./#advanced>Advanced</a><ul><li><a href=./#dictionary-observations>Dictionary Observations</a></li><li><a href=./#agent-script-for-competition>Agent Script for Competition</a></li></ul></li></ul></div><div class="notices tip"><p>The source code of all examples described in this section is available in our <a href=https://github.com/diambra/agents/tree/main/ray_rllib target=_blank>DIAMBRA Agents</a> repository.</p></div><h3 id=getting-ready>Getting Ready</h3><p>We highly recommend using virtual environments to isolate your python installs, especially to avoid conflicts in dependencies. In what follows we use Conda but any other tool should work too.</p><p>Create and activate a new dedicated virtual environment:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>conda create -n diambra-arena-ray python<span style=color:#f92672>=</span>3.8
</span></span><span style=display:flex><span>conda activate diambra-arena-ray
</span></span></code></pre></div><p>Install DIAMBRA Arena with Ray RLlib interface:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pip install diambra-arena<span style=color:#f92672>[</span>ray-rllib<span style=color:#f92672>]</span>
</span></span></code></pre></div><p>This should be enough to prepare your system to execute the following examples. You can refer to the official <a href=https://docs.ray.io/en/latest/rllib/index.html target=_blank>Ray RLlib documentation</a> or reach out on our <a href=https://diambra.ai/discord target=_blank>Discord server</a> for specific needs.</p><p>All the examples presented below are available here: <a href=https://github.com/diambra/agents/tree/main/ray_rllib target=_blank>DIAMBRA Agents - Ray RLlib.</a> They have been created following the high level approach found on <a href=https://docs.ray.io/en/latest/rllib/rllib-examples.html target=_blank>Ray RLlib examples</a> page and their related <a href=https://github.com/ray-project/ray/tree/master/rllib/examples target=_blank>repository collection,</a> thus allowing to easily extend them and to understand how they interface with the different components.</p><p>These examples only aims at demonstrating the core functionalities and high level aspects, they will not generate well performing agents, even if the training time is extended to cover a large number of training steps. The user will need to build upon them, exploring aspects like: policy network architecture, algorithm hyperparameter tuning, observation space tweaking, rewards wrapping and other similar ones.</p><h4 id=native-interface>Native interface</h4><p>DIAMBRA Arena native interface with Ray RLlib covers a wide range of use cases, automating handling of key things like parallelization. In the majority of cases it will be sufficient for users to directly import and use it, with no need for additional customization.</p><div class="notices note"><p>For the interface low level details, users can review the correspondent source code <a href=https://github.com/diambra/arena/blob/main/diambra/arena/ray_rllib/ target=_blank>here</a>.</p></div><h3 id=basic>Basic</h3><p>For all the basic examples, the environment will be used in <code>hardcore</code> mode, so that the observation space will be only of type <code>Box</code> composed by screen pixels, as in the majority of simple examples found in tutorials and docs. This allows to directly use it without the need of further processing.</p><h4 id=basic-example>Basic Example</h4><p>This example demonstrates how to:</p><ul><li>Build the config dictionary for Ray RLlib</li><li>Interface one of Ray RLlib&rsquo;s algorithms with DIAMBRA Arena using the native interface</li><li>Train the algorithm</li><li>Run the trained agent in the environment for one episode</li></ul><p>It uses the PPO algorithm and, for demonstration purposes, the algorithm is trained for only 200 steps, so the resulting agent will be far from optimal.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> diambra.arena
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> diambra.arena.ray_rllib.make_ray_env <span style=color:#f92672>import</span> DiambraArena, preprocess_ray_config
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.rllib.algorithms.ppo <span style=color:#f92672>import</span> PPO
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Settings</span>
</span></span><span style=display:flex><span>    settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;hardcore&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;frame_shape&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define and configure the environment</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env&#34;</span>: DiambraArena,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env_config&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;game_id&#34;</span>: <span style=color:#e6db74>&#34;doapp&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;settings&#34;</span>: settings,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;num_workers&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;train_batch_size&#34;</span>: <span style=color:#ae81ff>200</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update config file</span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> preprocess_ray_config(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the RLlib Agent.</span>
</span></span><span style=display:flex><span>    agent <span style=color:#f92672>=</span> PPO(config<span style=color:#f92672>=</span>config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Run it for n training iterations</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting training ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Training iteration:&#34;</span>, idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        agent<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74> .. training completed.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Run the trained agent (and render each timestep output).</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting trained agent execution ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    env <span style=color:#f92672>=</span> diambra<span style=color:#f92672>.</span>arena<span style=color:#f92672>.</span>make(<span style=color:#e6db74>&#34;doapp&#34;</span>, settings)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    observation <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        env<span style=color:#f92672>.</span>render()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>compute_single_action(observation)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        observation, reward, done, info <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>step(action)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> done:
</span></span><span style=display:flex><span>            observation <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>... trained agent execution completed.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    env<span style=color:#f92672>.</span>close()
</span></span></code></pre></div><p>How to run it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>diambra run python basic.py
</span></span></code></pre></div><h4 id=saving-loading-and-evaluating>Saving, loading and evaluating</h4><p>In addition to what seen in the previous example, this one demonstrates how to:</p><ul><li>Print out the policy network architecture</li><li>Save a trained agent</li><li>Load a saved agent</li><li>Evaluate an agent on a given number of episodes</li><li>Print training and evaluation results</li></ul><p>The same conditions of the previous example for algorithm, policy and training steps are used in this one too.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> diambra.arena
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> diambra.arena.ray_rllib.make_ray_env <span style=color:#f92672>import</span> DiambraArena, preprocess_ray_config
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.rllib.algorithms.ppo <span style=color:#f92672>import</span> PPO
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.tune.logger <span style=color:#f92672>import</span> pretty_print
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Settings</span>
</span></span><span style=display:flex><span>    settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;hardcore&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;frame_shape&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define and configure the environment</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env&#34;</span>: DiambraArena,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env_config&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;game_id&#34;</span>: <span style=color:#e6db74>&#34;doapp&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;settings&#34;</span>: settings,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;num_workers&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;train_batch_size&#34;</span>: <span style=color:#ae81ff>200</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;framework&#34;</span>: <span style=color:#e6db74>&#34;torch&#34;</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update config file</span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> preprocess_ray_config(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the RLlib Agent.</span>
</span></span><span style=display:flex><span>    agent <span style=color:#f92672>=</span> PPO(config<span style=color:#f92672>=</span>config)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Policy architecture =</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(agent<span style=color:#f92672>.</span>get_policy()<span style=color:#f92672>.</span>model))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Run it for n training iterations</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting training ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Training iteration:&#34;</span>, idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        results <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74> .. training completed.&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Training results:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(pretty_print(results)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Save the agent</span>
</span></span><span style=display:flex><span>    checkpoint <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>save()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Checkpoint saved at </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(checkpoint))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>del</span> agent  <span style=color:#75715e># delete trained model to demonstrate loading</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load the trained agent</span>
</span></span><span style=display:flex><span>    agent <span style=color:#f92672>=</span> PPO(config<span style=color:#f92672>=</span>config)
</span></span><span style=display:flex><span>    agent<span style=color:#f92672>.</span>restore(checkpoint)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Agent loaded&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Evaluate the trained agent (and render each timestep to the shell&#39;s</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># output).</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting evaluation ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>evaluate()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>... evaluation completed.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Evaluation results:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(pretty_print(results)))
</span></span></code></pre></div><p>How to run it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>diambra run python saving_loading_evaluating.py
</span></span></code></pre></div><h4 id=parallel-environments>Parallel Environments</h4><p>In addition to what seen in previous examples, this one demonstrates how to:</p><ul><li>Run training and evaluation using parallel environments</li></ul><p>This example runs multiple environments. In order to properly execute it, the user needs to specify the correct number of environments instances to be created via DIAMBRA CLI when running the script. In particular, in this case, 6 different instances are needed:</p><ul><li>2 rollout workers with 2 environments each, accounting for 4 environments</li><li>1 evaluation worker with 2 environments, accounting for the remaining 2 environments</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> diambra.arena
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> diambra.arena.ray_rllib.make_ray_env <span style=color:#f92672>import</span> DiambraArena, preprocess_ray_config
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.rllib.algorithms.ppo <span style=color:#f92672>import</span> PPO
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.tune.logger <span style=color:#f92672>import</span> pretty_print
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Settings</span>
</span></span><span style=display:flex><span>    settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;hardcore&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;frame_shape&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define and configure the environment</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env&#34;</span>: DiambraArena,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env_config&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;game_id&#34;</span>: <span style=color:#e6db74>&#34;doapp&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;settings&#34;</span>: settings,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;train_batch_size&#34;</span>: <span style=color:#ae81ff>200</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Use 2 rollout workers</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;num_workers&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        <span style=color:#75715e># Use a vectorized env with 2 sub-envs.</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;num_envs_per_worker&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Evaluate once per training iteration.</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;evaluation_interval&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        <span style=color:#75715e># Run evaluation on (at least) two episodes</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;evaluation_duration&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... using one evaluation worker (setting this to 0 will cause</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># evaluation to run on the local evaluation worker, blocking</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># training until evaluation is done).</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;evaluation_num_workers&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        <span style=color:#75715e># Special evaluation config. Keys specified here will override</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the same keys in the main config, but only for evaluation.</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;evaluation_config&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#75715e># Render the env while evaluating.</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Note that this will always only render the 1st RolloutWorker&#39;s</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># env and only the 1st sub-env in a vectorized env.</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;render_env&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update config file</span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> preprocess_ray_config(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the RLlib Agent.</span>
</span></span><span style=display:flex><span>    agent <span style=color:#f92672>=</span> PPO(config<span style=color:#f92672>=</span>config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Run it for n training iterations</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting training ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>2</span>):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Training iteration:&#34;</span>, idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        results <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74> .. training completed.&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Training results:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(pretty_print(results)))
</span></span></code></pre></div><p>How to run it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>diambra run -s<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span> python parallel_envs.py
</span></span></code></pre></div><h3 id=advanced>Advanced</h3><p>The nex example make use of the complete observation space of our environments. This is of type <code>Dict</code>, in which different elements are organized as key-value pairs and they can be of different type.</p><h4 id=dictionary-observations>Dictionary Observations</h4><p>In addition to what seen in previous examples, this one demonstrates how to:</p><ul><li>Activate a complete set of environment wrappers</li><li>How to properly handle dictionary observations for Ray RLlib</li></ul><p>There are two main things to note in this example: how to handle observation normalization and dictionary observations. As it can be seen from the snippet below, the normalization wrapper is applied on all elements prescribing one-hot encoding to be applied on binary discrete observations too. This is usually not needed nor suggested, but it is requested by Ray RLlib to automatically handle this observation type. On the other hand, the library does not have constraints on dictionary observation spaces, being able to handle nested ones too.</p><p>The policy network is automatically generated, properly handling different types of inputs. Model architecture is then printed to the console output, allowing to clearly identify all the different contributions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> diambra.arena
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> diambra.arena.ray_rllib.make_ray_env <span style=color:#f92672>import</span> DiambraArena, preprocess_ray_config
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.rllib.algorithms.ppo <span style=color:#f92672>import</span> PPO
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.tune.logger <span style=color:#f92672>import</span> pretty_print
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Settings</span>
</span></span><span style=display:flex><span>    settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;frame_shape&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;characters&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#e6db74>&#34;Kasumi&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Wrappers Settings</span>
</span></span><span style=display:flex><span>    wrappers_settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;reward_normalization&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;actions_stack&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;frame_stack&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;scale&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;process_discrete_binary&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define and configure the environment</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env&#34;</span>: DiambraArena,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env_config&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;game_id&#34;</span>: <span style=color:#e6db74>&#34;doapp&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;settings&#34;</span>: settings,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;wrappers_settings&#34;</span>: wrappers_settings,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;num_workers&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;train_batch_size&#34;</span>: <span style=color:#ae81ff>200</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;framework&#34;</span>: <span style=color:#e6db74>&#34;torch&#34;</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update config file</span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> preprocess_ray_config(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the RLlib Agent.</span>
</span></span><span style=display:flex><span>    agent <span style=color:#f92672>=</span> PPO(config<span style=color:#f92672>=</span>config)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Policy architecture =</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(agent<span style=color:#f92672>.</span>get_policy()<span style=color:#f92672>.</span>model))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Run it for n training iterations</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting training ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Training iteration:&#34;</span>, idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        results <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74> .. training completed.&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Training results:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(pretty_print(results)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Evaluate the trained agent (and render each timestep to the shell&#39;s</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># output).</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Starting evaluation ...</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>evaluate()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>... evaluation completed.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Evaluation results:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(pretty_print(results)))
</span></span></code></pre></div><p>How to run it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>diambra run python dict_obs_space.py
</span></span></code></pre></div><h4 id=agent-script-for-competition>Agent Script for Competition</h4><p>Finally, after the agent training is completed, besides running it locally in your own machine, you may want to submit it to our competition platform! To do so, you can use the following script that provides a ready to use, flexible example that can accommodate different models, games and settings.</p><div class="notices tip"><p>To submit your trained agent to our platform, compete for the first leaderboard positions, and unlock our achievements, follow the simple steps described in the <a href=/competitionplatform/howtosubmitanagent/>&ldquo;How to Submit an Agent&rdquo;</a> section.</p></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> yaml
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> diambra.arena
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> diambra.arena.ray_rllib.make_ray_env <span style=color:#f92672>import</span> DiambraArena, preprocess_ray_config
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.rllib.algorithms.ppo <span style=color:#f92672>import</span> PPO
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Reference: https://github.com/ray-project/ray/blob/ray-2.0.0/rllib/examples/inference_and_serving/policy_inference_after_training.py</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;This is an example agent based on RL Lib.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Usage:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>diambra run python agent.py --trainedModel /absolute/path/to/checkpoint/ --envSpaces /absolute/path/to/environment/spaces/descriptor/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    parser <span style=color:#f92672>=</span> argparse<span style=color:#f92672>.</span>ArgumentParser()
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;--trainedModel&#34;</span>, type<span style=color:#f92672>=</span>str, required<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, help<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Model path&#34;</span>)
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;--envSpaces&#34;</span>, type<span style=color:#f92672>=</span>str, required<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, help<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Environment spaces descriptor file path&#34;</span>)
</span></span><span style=display:flex><span>    opt <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse_args()
</span></span><span style=display:flex><span>    print(opt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    time_dep_seed <span style=color:#f92672>=</span> int((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> int(time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span>)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Settings</span>
</span></span><span style=display:flex><span>    settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;frame_shape&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    settings[<span style=color:#e6db74>&#34;characters&#34;</span>] <span style=color:#f92672>=</span> (<span style=color:#e6db74>&#34;Kasumi&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Wrappers Settings</span>
</span></span><span style=display:flex><span>    wrappers_settings <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;reward_normalization&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;actions_stack&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;frame_stack&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;scale&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    wrappers_settings[<span style=color:#e6db74>&#34;process_discrete_binary&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#75715e># Define and configure the environment</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env&#34;</span>: DiambraArena,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;env_config&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;game_id&#34;</span>: <span style=color:#e6db74>&#34;doapp&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;settings&#34;</span>: settings,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;wrappers_settings&#34;</span>: wrappers_settings,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;load_spaces_from_file&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;env_spaces_file_name&#34;</span>: opt<span style=color:#f92672>.</span>envSpaces,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;num_workers&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;train_batch_size&#34;</span>: <span style=color:#ae81ff>200</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;framework&#34;</span>: <span style=color:#e6db74>&#34;torch&#34;</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Update config file</span>
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> preprocess_ray_config(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load the trained agent</span>
</span></span><span style=display:flex><span>    agent <span style=color:#f92672>=</span> PPO(config<span style=color:#f92672>=</span>config)
</span></span><span style=display:flex><span>    agent<span style=color:#f92672>.</span>restore(opt<span style=color:#f92672>.</span>trainedModel)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Agent loaded&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the agent policy architecture</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Policy architecture =</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(agent<span style=color:#f92672>.</span>get_policy()<span style=color:#f92672>.</span>model))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    env <span style=color:#f92672>=</span> diambra<span style=color:#f92672>.</span>arena<span style=color:#f92672>.</span>make(<span style=color:#e6db74>&#34;doapp&#34;</span>, settings, wrappers_settings)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    obs <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        env<span style=color:#f92672>.</span>render()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> agent<span style=color:#f92672>.</span>compute_single_action(observation<span style=color:#f92672>=</span>obs, explore<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, policy_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;default_policy&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        obs, reward, done, info <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>step(action)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> done:
</span></span><span style=display:flex><span>            obs <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> info[<span style=color:#e6db74>&#34;env_done&#34;</span>]:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Close the environment</span>
</span></span><span style=display:flex><span>    env<span style=color:#f92672>.</span>close()
</span></span></code></pre></div><p>How to run it locally:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>diambra run python agent.py --trainedModel /absolute/path/to/checkpoint/ --envSpaces /absolute/path/to/environment/spaces/descriptor/
</span></span></code></pre></div><footer class=footline></footer></div></div><div id=navigation></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=/js/clipboard.min.js?1685233622></script>
<script src=/js/perfect-scrollbar.min.js?1685233622></script>
<script src=/js/perfect-scrollbar.jquery.min.js?1685233622></script>
<script src=/js/jquery.sticky.js?1685233622></script>
<script src=/js/featherlight.min.js?1685233622></script>
<script src=/js/highlight.pack.js?1685233622></script>
<script>hljs.initHighlightingOnLoad()</script><script src=/js/modernizr.custom-3.6.0.js?1685233622></script>
<script src=/js/learn.js?1685233622></script>
<script src=/js/hugo-learn.js?1685233622></script>
<script src=https://unpkg.com/mermaid@8.8.0/dist/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105947713-1","auto"),ga("send","pageview")</script></body></html>