[{"uri":"https://docs.diambra.ai/envs/games/doapp/","title":"Dead Or Alive ++","tags":[],"description":"","content":" Index Game Specific Info Game Specific Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID doapp Original ROM Name doapp.zip SHA256 Checksum d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Search Keywords DEAD OR ALIVE ++ [JAPAN], dead-or-alive-japan, 80781, wowroms Game Resolution\n(H X W X C) 480px X 512px X 3 Number of Moves and Attack Actions\n(Without Buttons Combination) 9, 8 (4)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-7): (No-Attack, Hold, Punch, Kick), Hold+Punch, Hold+Kick, Punch+Kick, Hold+Punch+Kick Max Difficulty (1P Mode) 4 Number of Characters (Selectable) 11 (11) Max Number of Outfits 4 Number of Stages (1P Mode) 8 Game Specific Settings Key Type Default Value(s) Value Range difficulty int 3 [1, 4] characters str or tuple of maximum three str Random Kasumi, Zack, Hayabusa, Bayman, Lei-Fang, Raidou, Gen-Fu, Tina, Bass, Jann-Lee, Ayane char_outfits int 1 [1, 4] characters and char_outfits need to be provided as tuples of two elements (the first for P1 and the second for P2) when using this environment in two players mode.\nAction Spaces Type Attack Buttons\nCombination Space Size (Number of Actions) Discrete Not active 9 (moves) + 4 (attacks) - 1 (no-action counted twice) = 12 Discrete Active 9 (moves) + 8 (attacks) - 1 (no-action counted twice) = 16 MultiDiscrete Not active 9 (moves) X 4 (attacks) = 36 MultiDiscrete Active 9 (moves) X 8 (attacks) = 72 Observation Space Some examples of Dead Or Alive ++ RAM states Global Key Type Value Range Description frame Box [0, 255] X [480 X 512 X 3] Latest game frame (RGB pixel screen) stage Box [1, 8] Current stage of the game Player specific Key Type Value Range Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, 2] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, 10] Index of first character selected (since in this game only one character is selected, these values are the same as \u0026ldquo;character in use\u0026rdquo;)\n0: Kasumi, 1: Zack, 2: Hayabusa, 3: Bayman, 4: Lei-Fang, 5: Raidou, 6: Gen-Fu, 7: Tina, 8: Bass, 9: Jann-Lee, 10: Ayane ownChar/oppChar Discrete [0, 10] Index of character in use\n0: Kasumi, 1: Zack, 2: Hayabusa, 3: Bayman, 4: Lei-Fang, 5: Raidou, 6: Gen-Fu, 7: Tina, 8: Bass, 9: Jann-Lee, 10: Ayane ownHealth/oppHealth Box [0, 208] Health bar value actions+move Discrete [0, 8] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, 7] or [0, 3] Index of latest attack action performed (no-attack, hold, punch, etc.) with, respectively, attack buttons combination active or not "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/singleplayerenv/","title":"Single Player Environment","tags":[],"description":"","content":"This example focuses on:\nBasic environment usage (Standard RL, Single Player Mode) Environment settings configuration Algorithm - Environment interaction loop Gym observation visualization A dedicated section describing environment settings is presented here, while more info on gym observation visualization utils are presented here. They both provide additional details on usage and purpose.\nModules import import diambra.arena Environment settings # Settings settings = {} # Player side selection: P1 (left), P2 (right), Random (50% P1, 50% P2) settings[\u0026#34;player\u0026#34;] = \u0026#34;P2\u0026#34; # Number of steps performed by the game # for every environment step, bounds: [1, 6] settings[\u0026#34;step_ratio\u0026#34;] = 6 # Native frame resize operation settings[\u0026#34;frame_shape\u0026#34;] = (128, 128, 0) # RBG with 128x128 size # settings[\u0026#34;frame_shape\u0026#34;] = (0, 0, 1) # Grayscale with original size # settings[\u0026#34;frame_shape\u0026#34;] = (0, 0, 0) # Deactivated (Original size RBG) # Game continue logic (0.0 by default): # - [0.0, 1.0]: probability of continuing game at game over # - int((-inf, -1.0]): number of continues at game over # before episode to be considered done settings[\u0026#34;continue_game\u0026#34;] = 0.0 # If to show game final when game is completed settings[\u0026#34;show_final\u0026#34;] = False # If to use hardcore mode in which observations are only made of game frame settings[\u0026#34;hardcore\u0026#34;] = False Game-specific settings # Game difficulty level settings[\u0026#34;difficulty\u0026#34;] = 4 # Character to be used, automatically extended with \u0026#34;Random\u0026#34; for games # required to select more than one character (e.g. Tekken Tag Tournament) settings[\u0026#34;characters\u0026#34;] = (\u0026#34;Random\u0026#34;) # Character outfit settings[\u0026#34;char_outfits\u0026#34;] = 2 Action space settings # Action space # If to use discrete or multi_discrete action space settings[\u0026#34;action_space\u0026#34;] = \u0026#34;multi_discrete\u0026#34; # If to use attack buttons combinations actions settings[\u0026#34;attack_but_combination\u0026#34;] = True Environment execution env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) observation = env.reset() env.show_obs(observation) while True: actions = env.action_space.sample() print(\u0026#34;Actions: {}\u0026#34;.format(actions)) observation, reward, done, info = env.step(actions) env.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if done: observation = env.reset() env.show_obs(observation) break env.close() "},{"uri":"https://docs.diambra.ai/competitionplatform/basicagentscript/","title":"Basic Agent Script","tags":[],"description":"","content":"The central element of a submission is the agent python script. Its structure is always composed by two main parts, highlighted in the picture below: the preparation step, where the agent and the environment setup is completed, and the interaction loop, where the classical agent-environment interaction happens.\nAfter a first call to the reset method, you start iterating alternating action selection and environment stepping, resetting the environment when the episode is done. That\u0026rsquo;s it, we take care of the rest.\nThere is one thing that is worth noticing: since we want your submission to be the same no matter how many episodes are needed to evaluate it, you need to implement the while loop in a way that it keeps iterating indefinitely (while True:) and only exits (the break statement) when the value info[\u0026quot;env_done\u0026quot;] is true. This value is set by us and used to let the agent know that the evaluation has been completed. In this way, the same script can be used to run evaluations made of 3, 5, 10 or whatever number of episodes you want, without changing a single line.\nStructure of an Agent script ready to be submitted "},{"uri":"https://docs.diambra.ai/projects/rlztournament/","title":"RLZ Tournament","tags":[],"description":"","content":" Project summary In 2021, we organized the very first AI Tournament leveraging DIAMBRA. It has been organized in collaboration with Reinforcement Learning Zurich (RLZ), a community of researchers, data scientists and software engineers interested in applications of Reinforcement Learning and AI.\nParticipants trained an AI algorithm to effectively play Dead Or Alive ++. The three best algorithms participated in the final event and competed for the 1400 CHF prize pool!\nAuthor(s) Dr. Claus Horn - RLZ (linkedin) Mark Rowan - RLZ (linkedin) Alessandro Palmas - DIAMBRA (linkedin) References Tournament Website: https://diambra.gitlab.io/website/aitournament/ RLZ Linkedin Page: https://www.linkedin.com/company/reinforcement-learning-zurich/ RLZ Meetup Page: https://www.meetup.com/Reinforcement-Learning-Zurich "},{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/stablebaselines3/","title":"Stable Baselines 3","tags":[],"description":"","content":" Index Getting Ready Basic Advanced Getting Ready We highly recommend using virtual environments to isolate your python installs, especially to avoid conflicts in dependencies. In what follows we use Conda but any other tool should work too.\nCreate and activate a new dedicated virtual environment:\nconda create -n diambra-arena-sb3 python=3.8 conda activate diambra-arena-sb3 Install DIAMBRA Arena with Stable Baselines 3 interface:\npip install diambra-arena[stable-baselines3] This should be enough to prepare your system to execute the following examples. You can refer to the official Stable Baselines 3 documentation or reach out on our Discord server for specific needs.\nAll the examples presented below are available here: DIAMBRA Agents - Stable Baselines 3. They have been created following the high level approach found on Stable Baselines 3 examples page, thus allowing to easily extend them and to understand how they interface with the different components.\nThese examples only aims at demonstrating the core functionalities and high level aspects, they will not generate well performing agents, even if the training time is extended to cover a large number of training steps. The user will need to build upon them, exploring aspects like: policy network architecture, algorithm hyperparameter tuning, observation space tweaking, rewards wrapping and other similar ones.\nNative interface DIAMBRA Arena native interface with Stable Baselines 3 covers a wide range of use cases, automating handling of vectorized environments and monitoring wrappers. In the majority of cases it will be sufficient for users to directly import and use it, with no need for additional customization. Below is reported its interface and a table describing its arguments.\ndef make_sb3_env(game_id: str, env_settings: dict={}, wrappers_settings: dict={}, use_subprocess: bool=True, seed: int=0, log_dir_base: str=\u0026#34;/tmp/DIAMBRALog/\u0026#34;, start_index: int=0, allow_early_resets: bool=True, start_method: str=None, no_vec: bool=False) Argument Type Default Value(s) Description game_id str - Game environment identifier env_settings dict {} Environment settings (see more) wrappers_settings dict {} Wrappers settings (see more) use_subprocess bool True If to use subprocesses for multi-threaded parallelization seed int 0 Random number generator seed log_dir_base str \u0026quot;/tmp/DIAMBRALog/\u0026quot; Folder where to save execution logs start_index int 0 Starting process rank index allow_early_resets bool True Monitor wrapper argument to allow environment reset before it is done start_method str None Method to spawn subprocesses when active (see more) no_vec bool False If True avoids using vectorized environments (valid only when using a single instance) For the interface low level details, users can review the correspondent source code here.\nBasic For all the basic examples, the environment will be used in hardcore mode, so that the observation space will be only of type Box composed by screen pixels, as in the majority of simple examples found in tutorials and docs. This allows to directly use it without the need of further processing.\nBasic Example This example demonstrates how to:\nInstantiate a new DIAMBRA Arena environment with its settings Interface it with one of Stable Baselines 3\u0026rsquo;s algorithms Train the algorithm Run the trained agent in the environment for one episode It uses the A2C algorithm, with a CnnPolicy policy network to properly process the game frame observation as input. For demonstration purposes, the algorithm is trained for only 200 steps, so the resulting agent will be far from optimal.\nimport diambra.arena from stable_baselines3 import A2C if __name__ == \u0026#34;__main__\u0026#34;: env = diambra.arena.make(\u0026#34;doapp\u0026#34;, {\u0026#34;hardcore\u0026#34;: True, \u0026#34;frame_shape\u0026#34;: (128, 128, 1)}) print(\u0026#34;\\nStarting training ...\\n\u0026#34;) agent = A2C(\u0026#39;CnnPolicy\u0026#39;, env, verbose=1) agent.learn(total_timesteps=200) print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;\\nStarting trained agent execution ...\\n\u0026#34;) observation = env.reset() while True: env.render() action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) if done: observation = env.reset() break print(\u0026#34;\\n... trained agent execution completed.\\n\u0026#34;) env.close() How to run it:\ndiambra run python basic.py Saving, loading and evaluating In addition to what seen in the previous example, this one demonstrates how to:\nSave a trained agent Load a saved agent Evaluate an agent on a given number of episodes The same conditions of the previous example for algorithm, policy and training steps are used in this one too.\nimport diambra.arena from stable_baselines3 import A2C from stable_baselines3.common.evaluation import evaluate_policy if __name__ == \u0026#34;__main__\u0026#34;: # Create environment env = diambra.arena.make(\u0026#34;doapp\u0026#34;, {\u0026#34;hardcore\u0026#34;: True, \u0026#34;frame_shape\u0026#34;: (128, 128, 1)}) # Instantiate the agent agent = A2C(\u0026#39;CnnPolicy\u0026#39;, env, verbose=1) # Train the agent agent.learn(total_timesteps=200) # Save the agent agent.save(\u0026#34;a2c_doapp\u0026#34;) # Delete trained agent to demonstrate loading del agent # Load the trained agent # NOTE: if you have loading issue, you can pass `print_system_info=True` # to compare the system on which the agent was trained vs the current one # agent = A2C.load(\u0026#34;a2c_doapp\u0026#34;, env=env, print_system_info=True) agent = A2C.load(\u0026#34;a2c_doapp\u0026#34;, env=env) # Evaluate the agent # NOTE: If you use wrappers with your environment that modify rewards, # this will be reflected here. To evaluate with original rewards, # wrap environment in a \u0026#34;Monitor\u0026#34; wrapper before other wrappers. mean_reward, std_reward = evaluate_policy(agent, agent.get_env(), n_eval_episodes=3) print(\u0026#34;Reward: {} (avg) ± {} (std)\u0026#34;.format(mean_reward, std_reward)) # Run trained agent observation = env.reset() cumulative_reward = 0 while True: env.render() action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) cumulative_reward += reward if (reward != 0): print(\u0026#34;Cumulative reward =\u0026#34;, cumulative_reward) if done: observation = env.reset() break env.close() How to run it:\ndiambra run python saving_loading_evaluating.py Parallel Environments In addition to what seen in previous examples, this one demonstrates how to:\nLeverage DIAMBRA Arena native Stable Baselines 3 interface Activate environment wrappers Run training using parallel environments Print out the policy network architecture In this example, the PPO algorithm is used, with the same CnnPolicy seen before. This policy network works even if in this example an environment wrapper is used to stack multiple game frames, as they are piled along the channel dimension. In this example the policy architecture is also printed to the console output, allowing to visualize how inputs are processed and \u0026ldquo;translated\u0026rdquo; to actions probabilities.\nThis example also runs multiple environments, automatically detecting the number of instances created by DIAMBRA CLI when running the script.\nimport diambra.arena from diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env from stable_baselines3 import PPO if __name__ == \u0026#39;__main__\u0026#39;: # Settings settings = {} settings[\u0026#34;hardcore\u0026#34;] = True settings[\u0026#34;frame_shape\u0026#34;] = (128, 128, 1) settings[\u0026#34;characters\u0026#34;] = (\u0026#34;Kasumi\u0026#34;) # Wrappers Settings wrappers_settings = {} wrappers_settings[\u0026#34;reward_normalization\u0026#34;] = True wrappers_settings[\u0026#34;frame_stack\u0026#34;] = 5 # Create environment env, num_envs = make_sb3_env(\u0026#34;doapp\u0026#34;, settings, wrappers_settings) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) print(\u0026#34;Observation space shape =\u0026#34;, env.observation_space.shape) print(\u0026#34;Observation space type =\u0026#34;, env.observation_space.dtype) print(\u0026#34;Act_space =\u0026#34;, env.action_space) # Instantiate the agent agent = PPO(\u0026#39;CnnPolicy\u0026#39;, env, verbose=1) # Print policy network architecture print(\u0026#34;Policy architecture:\u0026#34;) print(agent.policy) # Train the agent agent.learn(total_timesteps=200) # Run trained agent observation = env.reset() cumulative_reward = [0.0 for _ in range(num_envs)] while True: env.render() action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) cumulative_reward += reward if any(x != 0 for x in reward): print(\u0026#34;Cumulative reward(s) =\u0026#34;, cumulative_reward) if done.any(): observation = env.reset() break env.close() How to run it:\ndiambra run -s=2 python parallel_envs.py Advanced The nex examples make use of the complete observation space of our environments. This is of type Dict, in which different elements are organized as key-value pairs and they can be of different type.\nDictionary Observations In addition to what seen in previous examples, this one demonstrates how to:\nActivate a complete set of environment wrappers How to properly handle dictionary observations for Stable Baselines 3 There are two main things to note in this example: how to handle observation normalization and dictionary observations. As it can be seen from the snippet below, the normalization wrapper is applied on all elements but the image frame, as Stable Baselines 3 automatically normalizes images and expects their pixels to be in the range [0 - 255]. The library also has a specific constraint on dictionary observation spaces: they cannot be nested. For this reason we provide a flattening wrapper that creates a shallow, not nested, dictionary from the original observation space, allowing in addition to filter it by keys.\nIn this case, the policy network needs to be of class MultiInputPolicy, since it will handle different types of inputs. Stable Baselines 3 automatically defines the network architecture, properly matching the input type. The architecture is then printed to the console output, allowing to clearly identify all the different contributions.\nimport diambra.arena from diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env from stable_baselines3 import PPO if __name__ == \u0026#34;__main__\u0026#34;: # Settings settings = {} settings[\u0026#34;frame_shape\u0026#34;] = (128, 128, 1) settings[\u0026#34;characters\u0026#34;] = (\u0026#34;Kasumi\u0026#34;) # Wrappers Settings wrappers_settings = {} wrappers_settings[\u0026#34;reward_normalization\u0026#34;] = True wrappers_settings[\u0026#34;actions_stack\u0026#34;] = 12 wrappers_settings[\u0026#34;frame_stack\u0026#34;] = 5 wrappers_settings[\u0026#34;scale\u0026#34;] = True wrappers_settings[\u0026#34;exclude_image_scaling\u0026#34;] = True wrappers_settings[\u0026#34;flatten\u0026#34;] = True wrappers_settings[\u0026#34;filter_keys\u0026#34;] = [\u0026#34;stage\u0026#34;, \u0026#34;P1_ownHealth\u0026#34;, \u0026#34;P1_oppHealth\u0026#34;, \u0026#34;P1_ownSide\u0026#34;, \u0026#34;P1_oppSide\u0026#34;, \u0026#34;P1_oppChar\u0026#34;, \u0026#34;P1_actions_move\u0026#34;, \u0026#34;P1_actions_attack\u0026#34;] # Create environment env, num_envs = make_sb3_env(\u0026#34;doapp\u0026#34;, settings, wrappers_settings) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) print(\u0026#34;Observation space =\u0026#34;, env.observation_space) print(\u0026#34;Act_space =\u0026#34;, env.action_space) # Instantiate the agent agent = PPO(\u0026#34;MultiInputPolicy\u0026#34;, env, verbose=1) # Print policy network architecture print(\u0026#34;Policy architecture:\u0026#34;) print(agent.policy) # Train the agent agent.learn(total_timesteps=200) # Run trained agent observation = env.reset() cumulative_reward = [0.0 for _ in range(num_envs)] while True: env.render() action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) cumulative_reward += reward if any(x != 0 for x in reward): print(\u0026#34;Cumulative reward(s) =\u0026#34;, cumulative_reward) if done.any(): observation = env.reset() break env.close() How to run it:\ndiambra run python dict_obs_space.py Complete Training Script In addition to what seen in previous examples, this one demonstrates how to:\nBuild a complete training script to be used with Stable Baselines via a config fila How to properly handle hyper-parameters scheduling via callbacks How to use callbacks for auto-saving How to control some policy network models and optimizer parameters This example show exactly how we trained our own models on these environments. It should be considered a starting point from where to explore and experiment, the following are just a few options among the most obvious ones:\nTweak hyper-parameters for the chosen algorithm Evolve the policy network architecture Test different algorithms, both on and off-policy Try to leverage behavioral cloning / imitation learning Modify the reward function to guide learning in other directions import os import time import yaml import json import argparse from diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env from diambra.arena.stable_baselines3.sb3_utils import linear_schedule, AutoSave from stable_baselines3 import PPO if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--cfgFile\u0026#34;, type=str, required=True, help=\u0026#34;Configuration file\u0026#34;) opt = parser.parse_args() print(opt) # Read the cfg file yaml_file = open(opt.cfgFile) params = yaml.load(yaml_file, Loader=yaml.FullLoader) print(\u0026#34;Config parameters = \u0026#34;, json.dumps(params, sort_keys=True, indent=4)) yaml_file.close() time_dep_seed = int((time.time() - int(time.time() - 0.5)) * 1000) base_path = os.path.dirname(os.path.abspath(__file__)) model_folder = os.path.join(base_path, params[\u0026#34;folders\u0026#34;][\u0026#34;parent_dir\u0026#34;], params[\u0026#34;settings\u0026#34;][\u0026#34;game_id\u0026#34;], params[\u0026#34;folders\u0026#34;][\u0026#34;model_name\u0026#34;], \u0026#34;model\u0026#34;) tensor_board_folder = os.path.join(base_path, params[\u0026#34;folders\u0026#34;][\u0026#34;parent_dir\u0026#34;], params[\u0026#34;settings\u0026#34;][\u0026#34;game_id\u0026#34;], params[\u0026#34;folders\u0026#34;][\u0026#34;model_name\u0026#34;], \u0026#34;tb\u0026#34;) os.makedirs(model_folder, exist_ok=True) # Settings settings = params[\u0026#34;settings\u0026#34;] # Wrappers Settings wrappers_settings = params[\u0026#34;wrappers_settings\u0026#34;] # Create environment env, num_envs = make_sb3_env(params[\u0026#34;settings\u0026#34;][\u0026#34;game_id\u0026#34;], settings, wrappers_settings, seed=time_dep_seed) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) print(\u0026#34;Observation space =\u0026#34;, env.observation_space) print(\u0026#34;Act_space =\u0026#34;, env.action_space) # Policy param policy_kwargs = params[\u0026#34;policy_kwargs\u0026#34;] # PPO settings ppo_settings = params[\u0026#34;ppo_settings\u0026#34;] gamma = ppo_settings[\u0026#34;gamma\u0026#34;] model_checkpoint = ppo_settings[\u0026#34;model_checkpoint\u0026#34;] learning_rate = linear_schedule(ppo_settings[\u0026#34;learning_rate\u0026#34;][0], ppo_settings[\u0026#34;learning_rate\u0026#34;][1]) clip_range = linear_schedule(ppo_settings[\u0026#34;clip_range\u0026#34;][0], ppo_settings[\u0026#34;clip_range\u0026#34;][1]) clip_range_vf = clip_range batch_size = ppo_settings[\u0026#34;batch_size\u0026#34;] n_epochs = ppo_settings[\u0026#34;n_epochs\u0026#34;] n_steps = ppo_settings[\u0026#34;n_steps\u0026#34;] if model_checkpoint == \u0026#34;0M\u0026#34;: # Initialize the agent agent = PPO(\u0026#34;MultiInputPolicy\u0026#34;, env, verbose=1, gamma=gamma, batch_size=batch_size, n_epochs=n_epochs, n_steps=n_steps, learning_rate=learning_rate, clip_range=clip_range, clip_range_vf=clip_range_vf, policy_kwargs=policy_kwargs, tensorboard_log=tensor_board_folder) else: # Load the trained agent agent = PPO.load(os.path.join(model_folder, model_checkpoint), env=env, gamma=gamma, learning_rate=learning_rate, clip_range=clip_range, clip_range_vf=clip_range_vf, policy_kwargs=policy_kwargs, tensorboard_log=tensor_board_folder) # Print policy network architecture print(\u0026#34;Policy architecture:\u0026#34;) print(agent.policy) # Create the callback: autosave every USER DEF steps autosave_freq = ppo_settings[\u0026#34;autosave_freq\u0026#34;] auto_save_callback = AutoSave(check_freq=autosave_freq, num_envs=num_envs, save_path=os.path.join(model_folder, model_checkpoint + \u0026#34;_\u0026#34;)) # Train the agent time_steps = ppo_settings[\u0026#34;time_steps\u0026#34;] agent.learn(total_timesteps=time_steps, callback=auto_save_callback) # Save the agent new_model_checkpoint = str(int(model_checkpoint[:-1]) + time_steps) + \u0026#34;M\u0026#34; model_path = os.path.join(model_folder, new_model_checkpoint) agent.save(model_path) # Close the environment env.close() How to run it:\ndiambra run python training.py --cfgFile path/to/config.yaml and the configuration file to be used with this training script is reported below:\nfolders: parent_dir: \u0026#34;./results/\u0026#34; model_name: \u0026#34;sr6_128x4_das_nc\u0026#34; settings: game_id: \u0026#34;doapp\u0026#34; characters: (\u0026#34;Kasumi\u0026#34;) difficulty: 3 step_ratio: 6 frame_shape: (128, 128, 1) continue_game: 0.0 action_space: \u0026#34;discrete\u0026#34; attack_but_combination: false char_outfits: 2 player: \u0026#34;Random\u0026#34; show_final: false wrappers_settings: frame_stack: 4 dilation: 1 actions_stack: 12 reward_normalization: true scale: true exclude_image_scaling: true flatten: true filter_keys: [ \u0026#34;stage\u0026#34;, \u0026#34;P1_ownHealth\u0026#34;, \u0026#34;P1_oppHealth\u0026#34;, \u0026#34;P1_ownSide\u0026#34;, \u0026#34;P1_oppSide\u0026#34;, \u0026#34;P1_oppChar\u0026#34;, \u0026#34;P1_actions_move\u0026#34;, \u0026#34;P1_actions_attack\u0026#34;, ] policy_kwargs: #net_arch: [{ pi: [64, 64], vf: [32, 32] }] net_arch: [64, 64] ppo_settings: gamma: 0.94 model_checkpoint: \u0026#34;0M\u0026#34; learning_rate: [2.5e-4, 2.5e-6] # To start clip_range: [0.15, 0.025] # To start #learning_rate: [5.0e-5, 2.5e-6] # Fine Tuning #clip_range: [0.075, 0.025] # Fine Tuning batch_size: 256 # 8 #nminibatches gave different batch size depending on # the number of environments: batch_size = (n_steps * n_envs) // nminibatches n_epochs: 4 n_steps: 128 autosave_freq: 256 time_steps: 512 "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/","title":"Examples","tags":[],"description":"","content":"This section, and the ones it links to, presents a detailed description of the examples that are provided with DIAMBRA Arena repository. They cover the most important use-cases, and can be used as templates and starting points to explore all the features of the software package.\nThese examples show how to leverage both single and two players modes, how to set up environment wrappers specifying all their options, how to record human expert demonstrations and how to load them to apply imitation learning.\nEvery example has a dedicated page that can be reached via the sidebar menu or the list below.\nSource code for examples described in what follows can be found in the code repository, here.\nExamples List Single Player Environment Multi Player Environment Wrappers Options Human Experience Recorder Imitation Learning "},{"uri":"https://docs.diambra.ai/envs/games/","title":"Games &amp; Specifics","tags":[],"description":"","content":" Dead or Alive ++ Street Fighter III 3rd Strike Tekken Tag Tournament Ultimate Mortal Kombat 3 Samurai Showdown 5 Sp The King of Fighers '98 UMH ... many more to come soon.\nGame Specific Info Game specific details provide useful information about each title. They are reported in every game-dedicated page, and summarized in the table below.\nParameter Description Game ID String identifying the game Original ROM Name Name of the original game ROM to be downloaded (if renaming is needed, it is indicated) SHA256 Checksum ROM file checksum used to validate it Search Keywords List of keywords that can be used to find the correct ROM file Game Resolution (H X W X C) Game frame resolution Number of Moves and Attack Actions\n(Without Buttons Combination) Number of moves and attack actions and their description Max Difficulty Maximum difficulty level available Number of Characters (Selectable) Number of characters featured in the game, and those that can actually be selected Max Number of Outfits Maximum number of different outfits available per each character Max Stage Maximum number of stages for the single player mode Whenever possible, games are released with all hidden/bonus characters unlocked.\nFor every released title, extensive testing has been carried out, being the minimum requirement for a game to be released in beta. After that, the next internal step is training a Deep RL agent to play, and possibly complete it, making sure the 1P mode is playable with no bugs up until game end. This is the condition under which titles are moved from beta to stable status.\nTitle Status Game Id Dead Or Alive ++ Stable1 doapp Street Fighter III 3rd Strike Stable1 sfiii3n Tekken Tag Tournament Stable1 tektagt Ultimate Mortal Kombat 3 Stable1 umk3 Samurai Showdown 5 Special Beta2 samsh5sp The King of Fighters \u0026lsquo;98: Ultimate Match Hero Beta2 kof98umh Stable = Successfully trained Deep RL agent in single player mode.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBeta = Passing all quality assurance tests.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://docs.diambra.ai/gettingstarted/","title":"Getting Started","tags":[],"description":"","content":"Index Prerequisites Running the Environment Basic Script DIAMBRA Command Line Interface (CLI) Script Execution Advanced Usage DIAMBRA CLI Advanced Options Using Python Notebooks Environment Native Rendering Running Multiple Environments in Parallel Run DIAMBRA Engine without CLI Prerequisites Installation completed and tested as described in Installation and Quickstart homepage sections ROMs downloaded and placed all in the same folder, whose absolute path will be referred in the following as your/roms/local/path To avoid specifying ROMs path at every run, you can define the environment variable DIAMBRAROMSPATH=your/roms/local/path, either temporarily in your current shell/prompt session, or permanently in your profile (e.g. on linux in ~/.bashrc).\nRunning the Environment Basic Script The most straightforward and simple script to use DIAMBRA Arena is reported below. It features a random agent playing Dead Or Alive ++, and it represents the general interaction schema to be used for every game and context of DIAMBRA Arena.\n# DIAMBRA Arena module import import diambra.arena # Environment creation env = diambra.arena.make(\u0026#34;doapp\u0026#34;) # Environment reset observation = env.reset() # Agent-Environment interaction loop while True: # (Optional) Environment rendering env.render() # Action random sampling actions = env.action_space.sample() # Environment stepping observation, reward, done, info = env.step(actions) # Episode end (Done condition) check if done: observation = env.reset() break # Environment close env.close() More complex and complete examples can be found in the Examples section.\nDIAMBRA Command Line Interface (CLI) DIAMBRA Arena comes with a very handy tool: the DIAMBRA Command Line Interface (DIAMBRA CLI). It provides different useful commands, with related options, that contribute to make running DIAMBRA Arena environments super easy.\nThe main use of the CLI is running a command after brining up DIAMBRA Arena containerized environment(s). It sets the DIAMBRA_ENVS environment variable to list the endpoints of all running environments.\nUsage:\ndiambra run [flags] \u0026lt;command-to-execute\u0026gt; The only flag needed for simple executions is listed below. Advanced usage and options can be found in the CLI Advanced Options section below.\nFlag Type Description -r, --path.roms str Path to ROMs (default to DIAMBRAROMSPATH env var if set) Script Execution To run a python script using the CLI, one can just execute the following command:\ndiambra run -r /path/to/roms/folder/ python diambra_arena_gist.py This will start a new container with the environment, load in the DIAMBRA_ENVS environment variable the port on which the environment accepts connections, and run the script where the DIAMBRA Arena python module is imported and used to instantiate a new environment, that will automatically retrieve the port and connect to it.\nAdvanced Usage In what follows, we will omit the -r flag for the CLI, assuming the user has set the DIAMBRAROMSPATH environment variable in his system.\nDIAMBRA CLI Advanced Options Run Command Usage:\ndiambra run [flags] \u0026lt;command-to-execute\u0026gt; It runs a command after brining up DIAMBRA Arena containerized environment(s). It sets the DIAMBRA_ENVS environment variable to list the endpoints of all running environments.\nThe table below lists all available options for this command.\nFlag Type Description -h, --help - Help for run command -r, --path.roms str Path to ROMs (default to DIAMBRAROMSPATH env var if set) -g, --engine.render* - Render graphics server side -l, --engine.lockfps - Lock FPS -n, --engine.sound - Enable sound -s, --env.scale int Number of environments to run (default 1) --path.credentials str Path to credentials file (default \u0026ldquo;$HOME/.diambra/credentials\u0026rdquo;) --env.image str Env image to use, omit to detect from diambra-arena version -p, --images.pull bool (Always) pull image before running (default true) --env.mount str Host mounts for env container (/host/path:/container/path) `-x, \u0026ndash;env.autoremove - Remove containers on exit (default true) --env.seccomp str Path to seccomp profile to use for env (may slow down environment). Set to \u0026quot;\u0026quot; for runtime\u0026rsquo;s default profile. (default \u0026ldquo;unconfined\u0026rdquo;) -i, --interactive - Open stdin for interactions with arena and agent (default true) *: Currently available only for Linux systems. For additional info and for Windows/MacOS alternatives, see Environment Native Rendering section below.\nArena Command Usage:\ndiambra arena [flags] [command] It brings up DIAMBRA Arena containerized environment(s) and returns to the terminal output the endpoints of all running environments.\nThe table below lists all available commands for this mode.\nFlags reported for the Run command above apply also to this mode.\nCommand Type Description down - Stop DIAMBRA Arena container(s) up - Start DIAMBRA Arena container(s) check-roms str Check rom file specified with the absolute path list-roms - List currently available roms status - Show status of DIAMBRA arena version - Show DIAMBRA Arena Version Using Python Notebooks DIAMBRA Arena can also be used inside python notebooks. There are two options to do it, explained here after.\nThe most straightfoward one is to launch Jupyter Notebook through the CLI as shown by the next command:\ndiambra run jupyter notebook This step is needed to boot up the environment container and load its connection port in the DIAMBRA_ENVS environment variable. This variable is thus accessible from within Jupyter Notebook, that looks like the following one:\nView in full screen If one wants to execute a notebook from somewhere else (for example from inside Visual Studio Code), it is not possible to leverage the DIAMBRA_ENVS environment variable for passing the connection port information. In such cases, one should activate the environment container and retrieve its port, and this can be done by means of the CLI command diambra arena up that returns the port address, as shown below:\ndiambra arena up Server listening on 0.0.0.0:50051 127.0.0.1:49153 This information is then passed to the make function of DIAMBRA Arena through the setting \u0026quot;env_address\u0026quot; as shown in the next jupyter notebook:\nView in full screen Once done, one can stop running container(s) as follows:\ndiambra arena down (bb1a) stopping container Environment Native Rendering It is possible to activate emulator native rendering while running environments (i.e. bringing up the emulator graphics window). The CLI provides a specific flag for this purpose, but currently this is supported only on Linux, while Windows and MacOS users have to configure a Xserver and link it to the environment container. The next tabs provide hints for each context.\nLinux Win MacOS On Linux, the CLI allows to render emulator natively on the host, the user only needs to add the -g flag to the run command, as follows:\ndiambra run -g python diambra_arena_gist.py Activating emulator native rendering will open a GUI where the game executes. Currently, this feature is affected by a problem: the mouse cursor disappears and remains constrained inside such window. To re-aquire control of the OS Xserver, one can circle through the active windows using the key combination ALT+TAB and highlight a different one.\nTo run environments with native emulator GUI support on Windows, currently requires the user to setup a virtual XServer and connect it to the container. We cannot provide support for this use case at the moment, but we plan to implement this feature in the near future.\nA virtual XServer that in our experience proved to be effective is VcXsrv Windows X Server.\nTo run environments with native emulator GUI support on MacOS, currently requires the user to setup a virtual XServer and connect it to the container. We cannot provide support for this use case at the moment, but we plan to implement this feature in the near future.\nA virtual XServer that in our experience proved to be effective is XQuartz 2.7.8 coupled to socat that can be installed via brew install socat.\nRunning Multiple Environments in Parallel It can be useful to run multiple environment instances in parallel, for example for Deep RL training. The CLI provides a flag to control this, it can be used both by the run and the arena commands. The former will load a string in the DIAMBRA_ENVS environment variable where connection addresses are listed and separated by a space, while the latter will print out in the terminal the same string. These values can then be used properly to setup multiple parallel connections.\nRunning a script after having started 16 containers:\ndiambra run -s=16 python training_script.py Starting 4 containers and printing their addresses in the terminal:\ndiambra arena -s=4 up Server listening on 0.0.0.0:50051 127.0.0.1:49154 127.0.0.1:49155 127.0.0.1:49156 127.0.0.1:49157 Run DIAMBRA Engine without CLI Agents connect via network using gRPC to DIAMBRA Engine running in a Docker container. The diambra CLI\u0026rsquo;s run command starts the DIAMBRA Engine in a Docker container and sets up the environment to make it easy to connect to the Engine. For troubleshooting it might be useful to run the Engine manually, using host networking.\nCreating the ~/.diambra and ~/.diambra/credentials is only needed when you never ran the diambra CLI before. Otherwise this step can be skipped.\nStart Engine Linux/MacOS Win (cmd) Win (PowerShell) mkdir ~/.diambra touch ~/.diambra/credentials docker run -d --rm --name engine \\ -v $HOME/.diambra/credentials:/tmp/.diambra/credentials \\ -v /path/to/roms:/opt/diambraArena/roms \\ --net=host docker.io/diambra/engine:latest mkdir %userprofile%/.diambra echo \u0026gt; %userprofile%/.diambra/credentials docker run --rm -ti --name engine ^ -v %userprofile%/.diambra/credentials:/tmp/.diambra/credentials ^ -v %userprofile%/.diambra/roms:/opt/diambraArena/roms ^ --net=host docker.io/diambra/engine:latest mkdir $Env:userprofile/.diambra echo \u0026#34;\u0026#34; \u0026gt; $Env:userprofile/.diambra/credentials docker run --rm -ti --name engine ` -v $Env:userprofile/.diambra/credentials:/tmp/.diambra/credentials ` -v $Env:userprofile/.diambra/roms:/opt/diambraArena/roms ` --net=host docker.io/diambra/engine:latest Connect to Engine Now you can run the script that uses DIAMBRA Arena by opening a new terminal and setting DIAMBRA_ENVS environment variable followed by the python command:\nDIAMBRA_ENVS=localhost:50051 python ./script.py "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/multiplayerenv/","title":"Multi Player Environment","tags":[],"description":"","content":"This example focuses on:\nTwo players mode environment usage (Competitive Multi-Agent, SelfPlay, Competitive Human-Agent) Environment settings configuration Algorithms - Environment interaction loop Gym observation visualization A dedicated section describing environment settings is presented here, while more info on gym observation visualization utils are presented here. They both provide additional details on usage and purpose.\nModules import import diambra.arena import numpy as np Environment settings # Environment settings settings = {} # 2 Players game settings[\u0026#34;player\u0026#34;] = \u0026#34;P1P2\u0026#34; Action space settings # If to use discrete or multi_discrete action space settings[\u0026#34;action_space\u0026#34;] = (\u0026#34;discrete\u0026#34;, \u0026#34;discrete\u0026#34;) # If to use attack buttons combinations actions settings[\u0026#34;attack_but_combination\u0026#34;] = (True, True) Environment execution env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) observation = env.reset() env.show_obs(observation) while True: actions = env.action_space.sample() actions = np.append(actions[\u0026#34;P1\u0026#34;], actions[\u0026#34;P2\u0026#34;]) print(\u0026#34;Actions: {}\u0026#34;.format(actions)) observation, reward, done, info = env.step(actions) env.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if done: observation = env.reset() env.show_obs(observation) break env.close() "},{"uri":"https://docs.diambra.ai/envs/games/sfiii3n/","title":"Street Fighter III 3rd Strike","tags":[],"description":"","content":" Index Game Specific Info Game Specific Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID sfiii3n Original ROM Name sfiii3n.zip SHA256 Checksum 7239b5eb005488db22ace477501c574e9420c0ab70aeeb0795dfeb474284d416 Search Keywords STREET FIGHTER III 3RD STRIKE: FIGHT FOR THE FUTUR [JAPAN] (CLONE), street-fighter-iii-3rd-strike-fight-for-the-futur-japan-clone, 106255, wowroms Game Resolution\n(H X W X C) 224px X 384px X 3 Number of Moves and Attack Actions\n(Without Buttons Combination) 9, 10 (7)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-9): (No-Attack, Low Punch, Medium Punch, High Punch, Low Kick, Medium Kick, High Kick), Low Punch+Low Kick, Medium Punch+Medium Kick, High Punch+High Kick Max Difficulty (1P Mode) 8 Number of Characters (Selectable) 20 (19) Max Number of Outfits 7 Number of Stages (1P Mode) 10 Game Specific Settings Key Type Default Value(s) Value Range difficulty int 3 [1, 8] characters str or tuple of maximum three str Random Alex, Twelve, Hugo, Sean, Makoto, Elena, Ibuki, Chun-Li, Dudley, Necro, Q, Oro, Urien, Remy, Ryu, Gouki, Yun, Yang, Ken char_outfits int 1 [1, 7] Extended Game Settings Key Type Default Value(s) Value Range Description super_art int 0 [0, 3] Selects the type of super move.\n0: Random, 1-2-3: Super move 1-2-3 characters, char_outfits and super_art need to be provided as tuples of two elements (the first for P1 and the second for P2) when using this environment in two players mode.\nAction Spaces Type Attack Buttons\nCombination Space Size (Number of Actions) Discrete Not active 9 (moves) + 7 (attacks) - 1 (no-action counted twice) = 15 Discrete Active 9 (moves) + 10 (attacks) - 1 (no-action counted twice) = 18 MultiDiscrete Not active 9 (moves) X 7 (attacks) = 63 MultiDiscrete Active 9 (moves) X 10 (attacks) = 90 Observation Space Some examples of Street Fighter III RAM states Global Key Type Value Range Description frame Box [0, 255] X [224 X 384 X 3] Latest game frame (RGB pixel screen) stage Box [1, 10] Current stage of the game Player specific Key Type Value Range Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, 2] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, 19] Index of first character selected (since in this game only one character is selected, these values are the same as \u0026ldquo;character in use\u0026rdquo;)\n0: Alex, 1: Twelve, 2: Hugo, 3: Sean, 4: Makoto, 5: Elena, 6: Ibuki, 7: Chun-Li, 8: Dudley, 9: Necro, 10: Q, 11: Oro, 12: Urien, 13: Remy, 14: Ryu, 15: Gouki, 16: Yun, 17: Yang, 18: Ken, 19: Gill ownChar/oppChar Discrete [0, 19] Index of character in use\n0: Alex, 1: Twelve, 2: Hugo, 3: Sean, 4: Makoto, 5: Elena, 6: Ibuki, 7: Chun-Li, 8: Dudley, 9: Necro, 10: Q, 11: Oro, 12: Urien, 13: Remy, 14: Ryu, 15: Gouki, 16: Yun, 17: Yang, 18: Ken, 19: Gill ownHealth/oppHealth Box [-1, 160] Health bar value actions+move Discrete [0, 8] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, 10] or [0, 7] Index of latest attack action performed (no-attack, low punch, medium punch, etc.) with, respectively, attack buttons combination active or not ownStunBar/oppStunBar Box [0, 72] Stun bar value ownStunned/oppStunned Discrete (Binary) [0, 1] Stunned flag ownSuperBar/oppSuperBar Box [0, 128] Super bar value ownSuperType/oppSuperType Discrete [0, 2] Selected type of super move\n0-1-2: Super Type 1-2-3 ownSuperCount/oppSuperCount Box [0, 3] Count of activated super moves ownSuperMax/oppSuperMax Box [1, 3] Maximum number of activated super moves "},{"uri":"https://docs.diambra.ai/projects/gamepainter/","title":"Game Painter","tags":[],"description":"","content":" Project summary This project is an experiment that applies in real-time the style of famous paintings to popular fighting retro games, which are provided as Reinforcement Learning environments by DIAMBRA.\nAuthor(s) Alessandro Palmas - DIAMBRA (linkedin) References GitHub Repo: https://github.com/alexpalms/diambra-game-painter Reference Paper: Perceptual Losses for Real-Time Style Transfer and Super-Resolution Paper Implementation GitHub Repo: https://github.com/1627180283/real-time-Style-Transfer "},{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/rayrllib/","title":"Ray RLlib","tags":[],"description":"","content":" Index Getting Ready Basic Advanced Getting Ready We highly recommend using virtual environments to isolate your python installs, especially to avoid conflicts in dependencies. In what follows we use Conda but any other tool should work too.\nCreate and activate a new dedicated virtual environment:\nconda create -n diambra-arena-ray python=3.8 conda activate diambra-arena-ray Install DIAMBRA Arena with Ray RLlib interface:\npip install diambra-arena[ray-rllib] This should be enough to prepare your system to execute the following examples. You can refer to the official Ray RLlib documentation or reach out on our Discord server for specific needs.\nAll the examples presented below are available here: DIAMBRA Agents - Ray RLlib. They have been created following the high level approach found on Ray RLlib examples page and their related repository collection, thus allowing to easily extend them and to understand how they interface with the different components.\nThese examples only aims at demonstrating the core functionalities and high level aspects, they will not generate well performing agents, even if the training time is extended to cover a large number of training steps. The user will need to build upon them, exploring aspects like: policy network architecture, algorithm hyperparameter tuning, observation space tweaking, rewards wrapping and other similar ones.\nNative interface DIAMBRA Arena native interface with Ray RLlib covers a wide range of use cases, automating handling of key things like parallelization. In the majority of cases it will be sufficient for users to directly import and use it, with no need for additional customization.\nFor the interface low level details, users can review the correspondent source code here.\nBasic For all the basic examples, the environment will be used in hardcore mode, so that the observation space will be only of type Box composed by screen pixels, as in the majority of simple examples found in tutorials and docs. This allows to directly use it without the need of further processing.\nBasic Example This example demonstrates how to:\nBuild the config dictionary for Ray RLlib Interface one of Ray RLlib\u0026rsquo;s algorithms with DIAMBRA Arena using the native interface Train the algorithm Run the trained agent in the environment for one episode It uses the PPO algorithm and, for demonstration purposes, the algorithm is trained for only 200 steps, so the resulting agent will be far from optimal.\nimport diambra.arena from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO if __name__ == \u0026#34;__main__\u0026#34;: # Settings settings = {} settings[\u0026#34;hardcore\u0026#34;] = True settings[\u0026#34;frame_shape\u0026#34;] = (84, 84, 1) config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: settings, }, \u0026#34;num_workers\u0026#34;: 0, \u0026#34;train_batch_size\u0026#34;: 200, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(1): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) # Run the trained agent (and render each timestep output). print(\u0026#34;\\nStarting trained agent execution ...\\n\u0026#34;) env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) observation = env.reset() while True: env.render() action = agent.compute_single_action(observation) observation, reward, done, info = env.step(action) if done: observation = env.reset() break print(\u0026#34;\\n... trained agent execution completed.\\n\u0026#34;) env.close() How to run it:\ndiambra run python basic.py Saving, loading and evaluating In addition to what seen in the previous example, this one demonstrates how to:\nPrint out the policy network architecture Save a trained agent Load a saved agent Evaluate an agent on a given number of episodes Print training and evaluation results The same conditions of the previous example for algorithm, policy and training steps are used in this one too.\nimport diambra.arena from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO from ray.tune.logger import pretty_print if __name__ == \u0026#34;__main__\u0026#34;: # Settings settings = {} settings[\u0026#34;hardcore\u0026#34;] = True settings[\u0026#34;frame_shape\u0026#34;] = (84, 84, 1) config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: settings, }, \u0026#34;num_workers\u0026#34;: 0, \u0026#34;train_batch_size\u0026#34;: 200, \u0026#34;framework\u0026#34;: \u0026#34;torch\u0026#34;, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) print(\u0026#34;Policy architecture =\\n{}\u0026#34;.format(agent.get_policy().model)) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(1): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) results = agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;Training results:\\n{}\u0026#34;.format(pretty_print(results))) # Save the agent checkpoint = agent.save() print(\u0026#34;Checkpoint saved at {}\u0026#34;.format(checkpoint)) del agent # delete trained model to demonstrate loading # Load the trained agent agent = PPO(config=config) agent.restore(checkpoint) print(\u0026#34;Agent loaded\u0026#34;) # Evaluate the trained agent (and render each timestep to the shell\u0026#39;s # output). print(\u0026#34;\\nStarting evaluation ...\\n\u0026#34;) results = agent.evaluate() print(\u0026#34;\\n... evaluation completed.\\n\u0026#34;) print(\u0026#34;Evaluation results:\\n{}\u0026#34;.format(pretty_print(results))) How to run it:\ndiambra run python saving_loading_evaluating.py Parallel Environments In addition to what seen in previous examples, this one demonstrates how to:\nRun training and evaluation using parallel environments This example runs multiple environments. In order to properly execute it, the user needs to specify the correct number of environments instances to be created via DIAMBRA CLI when running the script. In particular, in this case, 6 different instances are needed:\n2 rollout workers with 2 environments each, accounting for 4 environments 1 evaluation worker with 2 environments, accounting for the remaining 2 environments import diambra.arena from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO from ray.tune.logger import pretty_print if __name__ == \u0026#34;__main__\u0026#34;: # Settings settings = {} settings[\u0026#34;hardcore\u0026#34;] = True settings[\u0026#34;frame_shape\u0026#34;] = (84, 84, 3) config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: settings, }, \u0026#34;train_batch_size\u0026#34;: 200, # Use 2 rollout workers \u0026#34;num_workers\u0026#34;: 2, # Use a vectorized env with 2 sub-envs. \u0026#34;num_envs_per_worker\u0026#34;: 2, # Evaluate once per training iteration. \u0026#34;evaluation_interval\u0026#34;: 1, # Run evaluation on (at least) two episodes \u0026#34;evaluation_duration\u0026#34;: 2, # ... using one evaluation worker (setting this to 0 will cause # evaluation to run on the local evaluation worker, blocking # training until evaluation is done). \u0026#34;evaluation_num_workers\u0026#34;: 1, # Special evaluation config. Keys specified here will override # the same keys in the main config, but only for evaluation. \u0026#34;evaluation_config\u0026#34;: { # Render the env while evaluating. # Note that this will always only render the 1st RolloutWorker\u0026#39;s # env and only the 1st sub-env in a vectorized env. \u0026#34;render_env\u0026#34;: True, }, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(2): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) results = agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;Training results:\\n{}\u0026#34;.format(pretty_print(results))) How to run it:\ndiambra run -s=6 python parallel_envs.py Advanced The nex example make use of the complete observation space of our environments. This is of type Dict, in which different elements are organized as key-value pairs and they can be of different type.\nDictionary Observations In addition to what seen in previous examples, this one demonstrates how to:\nActivate a complete set of environment wrappers How to properly handle dictionary observations for Ray RLlib There are two main things to note in this example: how to handle observation normalization and dictionary observations. As it can be seen from the snippet below, the normalization wrapper is applied on all elements prescribing one-hot encoding to be applied on binary discrete observations too. This is usually not needed nor suggested, but it is requested by Ray RLlib to automatically handle this observation type. On the other hand, the library does not have constraints on dictionary observation spaces, being able to handle nested ones too.\nThe policy network is automatically generated, properly handling different types of inputs. Model architecture is then printed to the console output, allowing to clearly identify all the different contributions.\nimport diambra.arena from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO from ray.tune.logger import pretty_print if __name__ == \u0026#34;__main__\u0026#34;: # Settings settings = {} settings[\u0026#34;frame_shape\u0026#34;] = (84, 84, 1) settings[\u0026#34;characters\u0026#34;] = (\u0026#34;Kasumi\u0026#34;) # Wrappers Settings wrappers_settings = {} wrappers_settings[\u0026#34;reward_normalization\u0026#34;] = True wrappers_settings[\u0026#34;actions_stack\u0026#34;] = 12 wrappers_settings[\u0026#34;frame_stack\u0026#34;] = 5 wrappers_settings[\u0026#34;scale\u0026#34;] = True wrappers_settings[\u0026#34;process_discrete_binary\u0026#34;] = True config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: settings, \u0026#34;wrappers_settings\u0026#34;: wrappers_settings, }, \u0026#34;num_workers\u0026#34;: 0, \u0026#34;train_batch_size\u0026#34;: 200, \u0026#34;framework\u0026#34;: \u0026#34;torch\u0026#34;, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) print(\u0026#34;Policy architecture =\\n{}\u0026#34;.format(agent.get_policy().model)) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(1): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) results = agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;Training results:\\n{}\u0026#34;.format(pretty_print(results))) # Evaluate the trained agent (and render each timestep to the shell\u0026#39;s # output). print(\u0026#34;\\nStarting evaluation ...\\n\u0026#34;) results = agent.evaluate() print(\u0026#34;\\n... evaluation completed.\\n\u0026#34;) print(\u0026#34;Evaluation results:\\n{}\u0026#34;.format(pretty_print(results))) How to run it:\ndiambra run python dict_obs_space.py "},{"uri":"https://docs.diambra.ai/competitionplatform/submissionevaluation/","title":"Submission Evaluation","tags":[],"description":"","content":"Each time you submit an agent, it is run for one episode to be evaluated. Every submission will generate a score, used for leaderboard positioning, and will unlock achievements.\nThe score is a function of both the total cumulative reward and the submission difficulty you selected at submission time, which can be either \u0026ldquo;Easy\u0026rdquo;, \u0026ldquo;Medium\u0026rdquo; or \u0026ldquo;Hard\u0026rdquo;. Every game has a different difficulty level scale, so a specific mapping is applied and is represented by the following table:\nGame Easy Medium Hard Dead Or Alive ++ 2 3 4 Street Fighter III 4 6 8 Tekken Tag Tournament 5 7 9 Ultimate Mortal Kombat 3 3 4 5 Samurai Showdown 5 4 6 8 King of Fighters \u0026lsquo;98 4 6 8 The relation that links score with total cumulative reward and difficulty is shown in the picture below. When \u0026ldquo;Easy\u0026rdquo; is selected, the score is exactly equal to the total cumulative reward. When \u0026ldquo;Medium\u0026rdquo; (or \u0026ldquo;Hard\u0026rdquo;) is selected, the score is obtained multiplying the total cumulative reward by a weighting value that varies linearly with the total cumulative reward obtained. It is equal to 1 if you obtain the lowest possible total cumulative reward (i.e. same score as if \u0026ldquo;Easy\u0026rdquo; was selected), and it is equal to the ratio between the game difficulty level for \u0026ldquo;Medium\u0026rdquo; (or \u0026ldquo;Hard\u0026rdquo;) and the game difficulty level for \u0026ldquo;Easy\u0026rdquo; if you obtain the highest possible total cumulative reward.\nSo, for example, for Dead or Alive ++, the weighting values for \u0026ldquo;Medium\u0026rdquo; and \u0026ldquo;Hard\u0026rdquo; vary linearly between\n$$ \\begin{equation} \\begin{gathered} k_M = \\left[1.0, \\frac{3}{2} \\right] = \\left[1.0, 1.5 \\right] \\\\ k_H = \\left[1.0, \\frac{4}{2} \\right] = \\left[1.0, 2.0 \\right] \\end{gathered} \\end{equation} $$\nScoring as a function of Total Cumulative Reward and Submission Difficulty "},{"uri":"https://docs.diambra.ai/envs/","title":"Environments","tags":[],"description":"","content":"Index Overview Interaction Basics Settings General Environment Settings Game Specific Settings Action Space(s) Action Spaces in Numbers Observation Space Global Player Specific Reward Function This page describes in details all general aspects related to DIAMBRA Arena environments. For game-specific details visit Games \u0026amp; Specifics page.\nOverview DIAMBRA Arena is a software package featuring a collection of high-quality environments for Reinforcement Learning research and experimentation. It provides a standard interface to popular arcade emulated video games, offering a Python API fully compliant with OpenAI Gym format, that makes its adoption smooth and straightforward.\nIt supports all major Operating Systems (Linux, Windows and MacOS) and can be easily installed via Python PIP, as described in the installation section. It is completely free to use, the user only needs to register on the official website.\nIn addition, its GitHub repository provides a collection of examples covering main use cases of interest that can be run in just a few steps.\nMain Features All environments are episodic Reinforcement Learning tasks, with discrete actions (gamepad buttons) and observations composed by screen pixels plus additional numerical data (RAM states like characters health bars or characters stage side).\nThey all support both single player (1P) as well as two players (2P) mode, making them the perfect resource to explore all the following Reinforcement Learning subfields:\nStandard RL Competitive Multi-Agent Competitive Human-Agent Self-Play Imitation Learning Human-in-the-Loop Available Games Interfaced games have been selected among the most popular fighting retro-games. While sharing the same fundamental mechanics, they provide different challenges, with specific features such as different type and number of characters, how to perform combos, health bars recharging, etc.\nWhenever possible, games are released with all hidden/bonus characters unlocked.\nAdditional details can be found in their dedicated section.\nInteraction Basics DIAMBRA Arena Environments usage follows the standard RL interaction framework: the agent sends an action to the environment, which process it and performs a transition accordingly, from the starting state to the new state, returning the observation and the reward to the agent to close the interaction loop. The figure below shows this typical interaction scheme and data flow.\nScheme of Agent-Environment Interaction The shortest snippet for a complete basic execution of an environment consists of just a few lines of code, and is presented in the code block below:\n1 # DIAMBRA Arena module import 2 import diambra.arena 3 4 # Environment creation 5 env = diambra.arena.make(\u0026#34;doapp\u0026#34;) 6 7 # Environment reset 8 observation = env.reset() 9 10 # Agent-Environment interaction loop 11 while True: 12 # (Optional) Environment rendering 13 env.render() 14 15 # Action random sampling 16 actions = env.action_space.sample() 17 18 # Environment stepping 19 observation, reward, done, info = env.step(actions) 20 21 # Episode end (Done condition) check 22 if done: 23 observation = env.reset() 24 break 25 26 # Environment close 27 env.close() More complex and complete examples can be found in the Examples section.\nSettings General Environment Settings All environments share a numerous set of options allowing to handle many different aspects, controlled by key-value pairs in a Python dictionary passed to the environment creation method, as shown below:\nenv = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) The first argument, the only one that is mandatory, is the game_id string, it specifies the game to execute among those available (see games list and info).\nNext table summarizes and describes the general, game-independent, settings, while the game-specific ones are presented in the game dedicated pages.\nGame-specific settings that are shared among all games, are found in the table contained in the Game Specific Settings section below.\nTwo ready-to-use examples showing how environment settings are used can be found here and here.\nKey Type Default Value(s) Value Range Description player str Random 1P mode: P1 (left), P2 (right), Random (50% P1, 50% P2)\n2P mode: P1P2 Selects single player (1P) or two players (2P) mode, and to select on which side to play (left/right) step_ratio int 6 [1, 6] Defines how many steps the game (emulator) performs for every environment step frame_shape tuple of three int (H, W, C) (0, 0, 0) H, W: [0, 512]\nC: 0 or 1 If active, resizes the frame and/or converts it from RGB to grayscale.\nCombinations:\n(0, 0, 0) - Deactivated;\n(H, W, 0) - RBG frame resized to H X W;\n(0, 0, 1) - Grayscale frame;\n(H, W, 1) - Grayscale frame resized to H X W. continue_game float 0.0 [0.0, 1.0]: probability of continuing game at game over\nint(abs(-inf, -1.0]): number of continues at game over before episode to be considered done Defines if and how to allow ”Continue” when the agent is about to face the game over condition show_final bool True True / False Activates displaying of final animation when game is completed action_space str multi_discrete discrete / multi_discrete Defines the type of the action space attack_but_combination bool True True / False Activates attack buttons combinations hardcore bool False True / False Activates hardcore mode, in which the observation is only made of the game frame Game Specific Settings Environment settings depending on the specific game and shared among all of them are reported in the table below. Additional ones (if present) are reported in game-dedicated pages.\nKey Type Default Value(s) Value Range Description difficulty int 3 Game-specific min and max values allowed Specifies game difficulty (1P only) characters str or tuple of maximum three str Random Game-specific lists of characters that can be selected Specifies character(s) to use char_outfits int 1 Game-specific min and max values allowed Defines the number of outfits to draw from at character selection Example of Dead or Alive ++ available outfits for Kasumi Of these general settings, action_space, attack_but_combination, characters, and char_outfits need to be provided as tuples of two elements (the first for P1 and the second for P2) when using the environments in two players mode. The same applies to some game-specific settings, they are listed in the game-dedicated page.\nAction Space(s) Actions of the interfaced games can be grouped in two categories: move actions (Up, Left, etc.) and attack ones (Punch, Kick, etc.). DIAMBRA Arena provides four different action spaces: the main distinction is between Discrete and MultiDiscrete ones. The former is a single list composed by the union of move and attack actions (of type gym.spaces.Discrete), while the latter consists of two sets combined, for move and attack actions respectively (of type gym.spaces.MultiDiscrete).\nFor each of the two options, there is an additional differentiation available: if to use attack buttons combinations or not. This option is mainly available to reduce the action space size as much as possible, since combinations of attack buttons can be seen as additional attack buttons. The complete visual description of available action spaces is shown in the figure below, where all four choices are presented via the correspondent gamepad buttons configuration for Dead Or Alive ++.\nWhen run in 2P mode, the environment is provided with a Dictionary action space (type gym.spaces.Dict) populated with two items, identified by keys P1 and P2, whose values are either gym.spaces.Discrete or gym.spaces.MultiDiscrete as described above.\nEach game has specific action spaces since attack buttons (and their combinations) are, in general, game-dependent. For this reason, in each game-dedicated page, a table like the one found below is reported, describing all four actions spaces for the specific game.\nIn Discrete action spaces:\nThere is only one ”no-op” action, that covers both the ”no-move” and ”no-attack” actions. The total number of actions available is Nm + Na − 1 where Nm is the number of move actions (no-move included) and Na is the number of attack actions (no-attack included). Only one action, either move or attack, can be sent for each environment step. In MultiDiscrete action spaces:\nThere is only one ”no-op” action, that covers both the ”no-move” and ”no-attack” actions. The total number of actions available is Nm × Na. Both move and attack actions can be sent at the same time for each environment step. All and only meaningful actions are made available per each game: they are sufficient to cover the entire spectrum of moves and combos for all the available characters. If a specific game has Button-1 and Button-2 among its available actions, and not Button-1 + Button-2, it means that the latter has no effect in any circumstance, considering all characters in all conditions.\nSome actions (especially attack buttons combinations) may have no effect for some of the characters: in some games combos requiring attack buttons combinations are valid only for a subset of characters.\nExample of Dead Or Alive ++ Action Spaces Action Spaces in Numbers For every game, a table containing the following info is reported. It provides numerical details about action spaces sizes.\nType Attack Buttons Combination Space Size (Number of Actions) Discrete / MultiDiscrete Active / Not active Total number of actions available, divided in move and attack actions Observation Space Environment observations are composed by two main elements: a visual one (the game frame) and an aggregation of quantitative values called RAM states (stage number, health values, etc.). Both of them are exposed through an observation space of type gym.spaces.Dict. It consists of global elements and player-specific ones, they are presented and described in the tables below. To give additional context, next figure shows an example of Dead Or Alive ++ observation where some of the RAM States are highlighted, superimposed on the game frame.\nEach game specifies and extends the set presented here with its custom one, described in the game-dedicated page.\nAn example of Dead Or Alive ++ RAM states Global Global elements of the observation space are unrelated to the player and they are currently limited to those presented and described in the following table. The same table is found on each game-dedicated page reporting its specs:\nKey Type Value Range Description frame Box Game-specific min and max values for each dimension Latest game frame (RGB pixel screen) stage Box Game-specific min and max values Current stage of the game Player specific Player-specific observations can be accessed using key(s) P1 (1P and 2P Modes) and/or P2 (2P Mode only), as shown in the following snippet for the Side element:\nown_side_var = observation[\u0026#34;P1\u0026#34;][\u0026#34;ownSide\u0026#34;] Typical values that are available for each game are reported and described in the table below. The same table is found in every game-dedicated page, specifying and extending (if needed) the observation elements set.\nKey Type Value Range Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, max number of rounds] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, max number of characters - 1] Index of first character selected (for games where only one character is selected, this values is the same as \u0026ldquo;character in use\u0026rdquo;) ownChar/oppChar Discrete [0, max number of characters - 1] Index of character in use ownHealth/oppHealth Box [0, max health value] Health bar value actions+move Discrete [0, max number of move actions - 1] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, max number of attack actions - 1] Index of latest attack action performed (no-attack, hold, punch, etc.) Reward Function The default reward is defined as a function of characters health values so that, qualitatively, damage suffered by the agent corresponds to a negative reward, and damage inflicted to the opponent corresponds to a positive reward. The quantitative, general and formal reward function definition is as follows:\n$$ \\begin{equation} R_t = \\sum_i^{0,N_c}\\left(\\bar{H_i}^{t^-} - \\bar{H_i}^{t} - \\left(\\hat{H_i}^{t^-} - \\hat{H_i}^{t}\\right)\\right) \\end{equation} $$\nWhere:\n$\\bar{H}$ and $\\hat{H}$ are health values for opponent’s character(s) and agent’s one(s) respectively; $t^-$ and $t$ are used to indicate conditions at ”state-time” and at ”new state-time” (i.e. before and after environment step); $N_c$ is the number of characters taking part in a round. Usually is $N_c = 1$ but there are some games where multiple characters are used, with the additional possible option of alternating them during gameplay, like Tekken Tag Tournament where 2 characters have to be selected and two opponents are faced every round (thus $N_c = 2$); The lower and upper bounds for the episode total cumulative reward are defined in the equations (Eqs. 2) below. They consider the default reward function for game execution with Continue Game option set equal to 0.0 (Continue not allowed).\n$$ \\begin{equation} \\begin{gathered} \\min{\\sum_t^{0,T_s}R_t} = - N_c \\left( \\left(N_s-1\\right) \\left(N_r-1\\right) + N_r\\right) \\Delta H \\\\ \\max{\\sum_t^{0,T_s}R_t} = N_c N_s N_r \\Delta H \\end{gathered} \\end{equation} $$\nWhere:\n$N_r$ is the number of rounds to win (or lose) in order to win (or lose) a stage; $T_s$ is the terminal state, reached when either $N_r$ rounds are lost (for both 1P and 2P mode) or game is cleared (for 1P mode only); $t$ represents the environment step and for an episode goes from 0 to $T_s$; $N_s$ is the maximum number of stages the agent can play before the game reaches $T_s$. $\\Delta H = H_{max} - H_{min}$ is the difference between the maximum and the mimnimum health values for the given game; ususally, but not always, $H_{min} = 0$. For 1P mode $N_s$ is game-dependent, while for 2P mode $N_s=1$, meaning the episode always ends after a single stage (so after $N_r$ rounds have been won / lost be the same player, either P1 or P2).\nFor 2P mode, P1 reward is defined as $R$ in the reward Eq. 1 and P2 reward is equal to $-R$ (zero-sum games). Eq. 1 describes the default reward function. It is of course possible to tweak it at will by means of custom Reward Wrappers.\nThe minimum and maximum total cumulative reward for the round can be different than $N_c\\Delta H$ in some cases. This may happen because:\nWhen multiple characters are used at the same time, the \u0026ldquo;Round Done\u0026rdquo; condition can be different for different games (e.g. either at least one character has zero health or all characters have zero health) impacting on the amount of reward collected. For some games health bars can be recharged (e.g. the character in background in Tekken Tag Tournament, or Gill\u0026rsquo;s resurrection move in Street Fighter III), making available an extra amount of reward to be collected or lost in that round. For some games, in some stages, additional opponents may be faced (opponent $N_c$ not constant through stages), making available an extra amount of reward to be collected (e.g. the endurance stages in Ultimate Mortal Kombat 3). For some games, not all characters share the same maximum health. $H_{max}$ and $H_{min}$ are always the extremes for a given game, among all characters. Lower and upper bounds of episode total cumulative reward may, in some cases, deviate from what defined by Eqs. 2, because:\nThe absolute value of minimum / maximum total cumulative reward for the round can be different from $N_c\\Delta H$ (see above). For some games, $N_r$ is not the same for all the stages (1P mode only), for example for Tekken Tag Tournament the final stage is made of a single round while all previous ones require two wins. Please note that the maximum cumulative reward (for 1P mode) is obtained when clearing the game winning all rounds with a perfect ($\\max{\\sum_t^{0,T_s}R_t}\\Rightarrow$ game completed), but the vice versa is not true. In fact not necessarily the higher number of stages won, the higher is the total cumulative reward ($\\max{\\sum_t^{0,T_s}R_t}\\not\\propto$ stage reached, game completed $\\nRightarrow\\max{\\sum_t^{0,T_s}R_t}$). Somehow counter intuitively, in order to obtain the lowest possible total cumulative reward the agent is supposed to reach the final stage (collecting negative rewards in all previous ones) before loosing by $N_r$ perfects.\nNormalized Reward If a normalized reward is considered, the total cumulative reward equation becomes:\n$$ \\begin{equation} R_t = \\frac{\\sum_i^{0,N_c}\\left(\\bar{H_i}^{t^-} - \\bar{H_i}^{t} - \\left(\\hat{H_i}^{t^-} - \\hat{H_i}^{t}\\right)\\right)}{N_k \\Delta H} \\end{equation} $$\nWith the following additional term at the denominator:\n$N_k$ is the reward normalization factor defined through our Reward Nomralization Wrapper. The normalization term at the denominator ensures that a round won with a perfect (i.e. without losing any health), generates always the same maximum total cumulative reward (for the round) accross all games, equal to $N_c/N_k$.\n"},{"uri":"https://docs.diambra.ai/envs/games/tektagt/","title":"Tekken Tag Tournament","tags":[],"description":"","content":" Index Game Specific Info Game Specific Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID tektagt Original ROM Name tektagtac.zip\nRename the rom from tektagtac.zip to tektagt.zip SHA256 Checksum 57be777eae0ee9e1c035a64da4c0e7cb7112259ccebe64e7e97029ac7f01b168 Search Keywords TEKKEN TAG TOURNAMENT [ASIA] (CLONE), tekken-tag-tournament-asia-clone, 108661, wowroms Game Resolution\n(H X W X C) 240px X 512px X 3 Number of Moves and Attack Actions\n(Without Buttons Combination) 9, 13 (6)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-12): (No-Attack, Left Punch, Right Punch, Left Kick, Right Kick, Tag), Left Punch+Right Punch, Left Punch+Left Kick, Left Punch+Right Kick, Right Punch+Left Kick, Right Punch+Right Kick, Right Punch+Tag, Left Kick+Right Kick Max Difficulty (1P Mode) 9 Number of Characters (Selectable) 39 (38) Max Number of Outfits 2 Number of Stages (1P Mode) 8 Game Specific Settings Key Type Default Value(s) Value Range difficulty int 3 [1, 9] characters str or tuple of maximum three str (Random, Random) Xiaoyu, Yoshimitsu, Nina, Law, Hwoarang, Eddy, Paul, King, Lei, Jin, Baek, Michelle, Armorking, Gunjack, Anna, Brian, Heihachi, Ganryu, Julia, Jun, Kunimitsu, Kazuya, Bruce, Kuma, Jack-Z, Lee, Wang, P.Jack, Devil, True Ogre, Ogre, Roger, Tetsujin, Panda, Tiger, Angel, Alex, Mokujin char_outfits int 1 [1, 2] characters and char_outfits need to be provided as tuples of two elements (the first for P1 and the second for P2) when using this environment in two players mode.\nAction Spaces Type Attack Buttons\nCombination Space Size (Number of Actions) Discrete Not active 9 (moves) + 6 (attacks) - 1 (no-action counted twice) = 14 Discrete Active 9 (moves) + 13 (attacks) - 1 (no-action counted twice) = 21 MultiDiscrete Not active 9 (moves) X 6 (attacks) = 54 MultiDiscrete Active 9 (moves) X 13 (attacks) = 117 Observation Space Some examples of Tekken Tag Tournament RAM states Global Key Type Value Range Description frame Box [0, 255] X [240 X 512 X 3] Latest game frame (RGB pixel screen) stage Box [1, 8] Current stage of the game Player specific Key Type Value Range Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, 2] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, 38] Index of first character selected\n0: Xiaoyu, 1: Yoshimitsu, 2: Nina, 3: Law, 4: Hwoarang, 5: Eddy, 6: Paul, 7: King, 8: Lei, 9: Jin, 10: Baek, 11: Michelle, 12: Armorking, 13: Gunjack, 14: Anna, 15: Brian, 16: Heihachi, 17: Ganryu, 18: Julia, 19: Jun, 20: Kunimitsu, 21: Kazuya, 22: Bruce, 23: Kuma, 24: Jack-Z, 25: Lee, 26: Wang, 27: P.Jack, 28: Devil, 29: True Ogre, 30: Ogre, 31: Roger, 32: Tetsujin, 33: Panda, 34: Tiger, 35: Angel, 36: Alex, 37: Mokujin, 38: Unknown ownChar2/oppChar2 Discrete [0, 38] Index of second character selected\n0: Xiaoyu, 1: Yoshimitsu, 2: Nina, 3: Law, 4: Hwoarang, 5: Eddy, 6: Paul, 7: King, 8: Lei, 9: Jin, 10: Baek, 11: Michelle, 12: Armorking, 13: Gunjack, 14: Anna, 15: Brian, 16: Heihachi, 17: Ganryu, 18: Julia, 19: Jun, 20: Kunimitsu, 21: Kazuya, 22: Bruce, 23: Kuma, 24: Jack-Z, 25: Lee, 26: Wang, 27: P.Jack, 28: Devil, 29: True Ogre, 30: Ogre, 31: Roger, 32: Tetsujin, 33: Panda, 34: Tiger, 35: Angel, 36: Alex, 37: Mokujin, 38: Unknown ownChar/oppChar Discrete [0, 38] Index of character in use\n0: Xiaoyu, 1: Yoshimitsu, 2: Nina, 3: Law, 4: Hwoarang, 5: Eddy, 6: Paul, 7: King, 8: Lei, 9: Jin, 10: Baek, 11: Michelle, 12: Armorking, 13: Gunjack, 14: Anna, 15: Brian, 16: Heihachi, 17: Ganryu, 18: Julia, 19: Jun, 20: Kunimitsu, 21: Kazuya, 22: Bruce, 23: Kuma, 24: Jack-Z, 25: Lee, 26: Wang, 27: P.Jack, 28: Devil, 29: True Ogre, 30: Ogre, 31: Roger, 32: Tetsujin, 33: Panda, 34: Tiger, 35: Angel, 36: Alex, 37: Mokujin, 38: Unknown ownHealth1/oppHealth1 Box [0, 182] Health bar value for first character in use ownHealth2/oppHealth2 Box [0, 182] Health bar value for second character in use actions+move Discrete [0, 8] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, 12] or [0, 5] Index of latest attack action performed (no-attack, left punch, right punch, etc.) with, respectively, attack buttons combination active or not ownActiveChar/oppActiveChar Discrete (Binary) [0, 1] Index of the active character\n0: first, 1: second ownBarStatus/oppBarStatus Discrete [0, 4] Status of the background character health bar\n0: reserve health bar almost filled, 1: small amount of health lost, recharging in progress, 2: large amount of health lost, recharging in progress, 3: rage mode on, combo attack ready, 4: no background character (final boss) "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/","title":"How To Submit An Agent","tags":[],"description":"","content":"The basic process to submit an agent consists in the following steps:\nWrite a python script in which your agent interact with the environment exactly as if you were performing evaluation in your local machine Store your trained agent\u0026rsquo;s scripts and weights (if any) in a private repository and create a personal access token to the repository (in our examples we will use GitHub). Submit the agent using our Command Line Interface specifying your private repository files path, your secret token and one of the public pre-built dependencies images we provide In our DIAMBRA Agents repo we provide many examples, ranging from a trivial random agent to RL agents trained with state-of-the-art RL libraries.\nIn the subsections linked below, we guide you through this process, starting from the easiest use case and building upon it to show you how to leverage the most advanced features.\nSubmit pre-built Agents Submit your own Agent Custom dependencies image "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/submitprebuiltagents/","title":"Submit Pre-Built Agents","tags":[],"description":"","content":"To get the feeling of how an agent submission works, you can leverage our pre-built agents. In DIAMBRA Agents repo, together with different source code examples, we also provide pre-built docker images (packages) for some of them.\nFor example, here you find the pre-built docker image for the random agent correspondent to this source code. As indicated by the python script settings, this random agent will play using a \u0026ldquo;Random\u0026rdquo; character in a random game.\nUsing this pre-built docker image you can easily perform your first submission ever on DIAMBRA platform, and appear in the official online leaderboard by simply typing in your preferred shell the following command:\ndiambra agent submit diambra/agent-random-1:main If you want to specify the game on which to run the random agent, use the --gameId command line argument that our pre-built image accepts, when submitting the docker image as follows: diambra agent submit diambra/agent-random-1:main --gameId tektagt. Additional similar use cases are covered in the \u0026ldquo;Arguments and Commands\u0026rdquo; page.\nAfter running the command, you will receive a submission confirmation, its identification number as well as the url where to see the results, something similar to the following:\ndiambra agent submit diambra/agent-random-1:main 🖥️ (178) Agent submitted: https://diambra.ai/submission/178 By default, the submission will select the lowest difficulty level (Easy) of the three available (Easy, Medium, Hard). To change this, you can add the --submission.difficulty argument: diambra agent submit --submission.difficulty Medium diambra/agent-random-1:main\nAs shown here, it is possible to embed your agent files (i.e. scripts and weights) in the dependencies docker image and submit only that. Keep in mind that this image needs to be public and will be visible on the platform, so every user will be able to use it for his own submissions.\n"},{"uri":"https://docs.diambra.ai/envs/games/umk3/","title":"Ultimate Mortal Kombat 3","tags":[],"description":"","content":" Index Game Specific Info Game Specific Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID umk3 Original ROM Name umk3r10.zip\nRename the rom from umk3r10.zip to umk3.zip SHA256 Checksum f48216ad82f78cb86e9c07d2507be347f904f4b5ae354a85ae7c34d969d265af Search Keywords ULTIMATE MORTAL KOMBAT 3 (CLONE), ultimate-mortal-kombat-3-clone, 109574, wowroms Game Resolution\n(H X W X C) 254px X 500px X 3 Number of Moves and Attack Actions\n(Without Buttons Combination) 9, 7 (7)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-6): (No-Attack, High Punch, High Kick, Low Kick, Low Punch, Run, Block) Max Difficulty (1P Mode) 5 Number of Characters (Selectable) 26 (22) Max Number of Outfits 2 Number of Stages (1P Mode) 8 (Tower 1), 9 (Tower 2), 10 (Tower 3), 11 (Tower 4) Game Specific Settings Key Type Default Value(s) Value Range difficulty int 3 [1, 5] characters str or tuple of maximum three str Random Kitana, Reptile, Kano, Sektor, Kabal, Sonya, Mileena, Sindel, Sheeva, Jax, Ermac, Stryker, Shang Tsung, Nightwolf, Sub-Zero-2, Cyrax, Liu Kang, Jade, Sub-Zero, Kung Lao, Smoke, Skorpion char_outfits int 1 [1, 1] Extended Game Settings Key Type Default Value(s) Value Range Description tower int 3 [1, 4] Selects the tower to play in (1P mode only) characters and char_outfits need to be provided as tuples of two elements (the first for P1 and the second for P2) when using this environment in two players mode.\nAction Spaces Type Attack Buttons\nCombination Space Size (Number of Actions) Discrete Not active 9 (moves) + 7 (attacks) - 1 (no-action counted twice) = 15 Discrete Active 9 (moves) + 7 (attacks) - 1 (no-action counted twice) = 15 MultiDiscrete Not active 9 (moves) X 7 (attacks) = 63 MultiDiscrete Active 9 (moves) X 7 (attacks) = 63 Observation Space Some examples of Ultimate Mortal Kombat 3 RAM states Global Key Type Value Description frame Box [0, 255] X [254 X 500 X 3] Latest game frame (RGB pixel screen) stage Box [1, 11] Current stage of the game Player specific Key Type Value Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, 2] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, 25] Index of first character selected (since in this game only one character is selected, these values are the same as \u0026ldquo;character in use\u0026rdquo;)\n0: Kitana, 1: Reptile, 2: Kano, 3: Sektor, 4: Kabal, 5: Sonya, 6: Mileena, 7: Sindel, 8: Sheeva, 9: Jax, 10: Ermac, 11: Stryker, 12: Shang Tsung, 13: Nightwolf, 14: Sub-Zero-2, 15: Cyrax, 16: Liu Kang, 17: Jade, 18: Sub-Zero, 19: Kung Lao, 20: Smoke, 21: Skorpion, 22: Human Smoke, 23: Noob Saibot, 24: Motaro\u0026quot;, 25: Shao Kahn ownChar/oppChar Discrete [0, 25] Index of character in use\n0: Kitana, 1: Reptile, 2: Kano, 3: Sektor, 4: Kabal, 5: Sonya, 6: Mileena, 7: Sindel, 8: Sheeva, 9: Jax, 10: Ermac, 11: Stryker, 12: Shang Tsung, 13: Nightwolf, 14: Sub-Zero-2, 15: Cyrax, 16: Liu Kang, 17: Jade, 18: Sub-Zero, 19: Kung Lao, 20: Smoke, 21: Skorpion, 22: Human Smoke, 23: Noob Saibot, 24: Motaro\u0026quot;, 25: Shao Kahn ownHealth/oppHealth Box [0, 166] Health bar value actions+move Discrete [0, 8] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, 6] Index of latest attack action performed (no-attack, high punch, high kick, etc.) ownAggressorBar/oppAggressorBar Box [0, 48] Aggressor bar value "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/wrappersoptions/","title":"Wrappers Options","tags":[],"description":"","content":"This example focuses on:\nWrappers settings configuration Wrapped observation visualization A dedicated section describing environment wrappers settings is presented here, while more info on wrapped observation visualization utils are presented here. They both provide additional details on usage and purpose.\nModules import import diambra.arena Wrappers settings # Gym wrappers settings wrappers_settings = {} # Number of no-Op actions to be executed # at the beginning of the episode (0 by default) wrappers_settings[\u0026#34;no_op_max\u0026#34;] = 0 # Number of steps for which the same action should be sent (1 by default) wrappers_settings[\u0026#34;sticky_actions\u0026#34;] = 1 # Frame resize operation spec (deactivated by default) # WARNING: for speedup, avoid frame warping wrappers, # use environment\u0026#39;s native frame wrapping through # \u0026#34;frame_shape\u0026#34; setting (see documentation for details). wrappers_settings[\u0026#34;hwc_obs_resize\u0026#34;] = (128, 128, 1) # Wrapper option for reward normalization # When activated, the reward normalization factor can be set (default = 0.5) # The normalization is performed by dividing the reward value # by the product of the factor times the value of the full health bar # reward = reward / (C * fullHealthBarValue) wrappers_settings[\u0026#34;reward_normalization\u0026#34;] = True wrappers_settings[\u0026#34;reward_normalization_factor\u0026#34;] = 0.5 # If to clip rewards (False by default) wrappers_settings[\u0026#34;clip_rewards\u0026#34;] = False # Number of frames to be stacked together (1 by default) wrappers_settings[\u0026#34;frame_stack\u0026#34;] = 4 # Frames interval when stacking (1 by default) wrappers_settings[\u0026#34;dilation\u0026#34;] = 1 # How many past actions to stack together (1 by default) wrappers_settings[\u0026#34;actions_stack\u0026#34;] = 12 # If to scale observation numerical values (deactivated by default) # optionally exclude images from normalization (deactivated by default) # and optionally perform one-hot encoding also on discrete binary variables (deactivated by default) wrappers_settings[\u0026#34;scale\u0026#34;] = True wrappers_settings[\u0026#34;exclude_image_scaling\u0026#34;] = True wrappers_settings[\u0026#34;process_discrete_binary\u0026#34;] = True # Scaling interval (0 = [0.0, 1.0], 1 = [-1.0, 1.0]) wrappers_settings[\u0026#34;scale_mod\u0026#34;] = 0 # Flattening observation dictionary and filtering # a sub-set of the RAM states wrappers_settings[\u0026#34;flatten\u0026#34;] = True wrappers_settings[\u0026#34;filter_keys\u0026#34;] = [\u0026#34;stage\u0026#34;, \u0026#34;P1_ownSide\u0026#34;, \u0026#34;P1_oppSide\u0026#34;, \u0026#34;P1_ownHealth\u0026#34;, \u0026#34;P1_oppChar\u0026#34;, \u0026#34;P1_actions_move\u0026#34;, \u0026#34;P1_actions_attack\u0026#34;] Environment execution env = diambra.arena.make(\u0026#34;doapp\u0026#34;, {}, wrappers_settings) observation = env.reset() env.show_obs(observation) while True: actions = env.action_space.sample() print(\u0026#34;Actions: {}\u0026#34;.format(actions)) observation, reward, done, info = env.step(actions) env.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if done: observation = env.reset() env.show_obs(observation) break env.close() "},{"uri":"https://docs.diambra.ai/wrappers/","title":"Wrappers","tags":[],"description":"","content":"Index Generic Wrappers NoOp Steps After Reset Observation Wrappers Frame Warping Frame Stacking With Optional Dilation Action Stacking Scaling Flattening and Filtering Action Wrappers Actions Sticking Reward Wrappers Reward Normalization Reward Clipping DIAMBRA Arena comes with a large number of ready-to-use wrappers and examples showing how to apply them. They cover a wide spectrum of use cases, and also provide reference templates to develop custom ones. In order to activate wrappers, one has just to add an additional kwargs dictionary, here named wrappers_settings, to the environment creation method, as shown in the next code block. The dictionary has to be populated as described in the next sections.\nenv = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings, wrappers_settings) Implementation examples and templates can be found in the code repository, here.\nUse of these functionalities can be found in this and this examples.\nGeneric Wrappers NoOp Steps After Reset Key Type Default Value(s) Value Range Description no_op_max int 0 [0, 12] Performs a maximum of value No-Action steps after episode reset wrappers_settings[\u0026#34;no_op_max\u0026#34;] = 0 Observation Wrappers Frame Warping DEPRECATED: For speed, consider using the environment setting frame_shape that performs the same operation on the engine side, as described in this section.\nKey Type Default Value(s) Value Range Target Observation Element Description hwc_obs_resize tuple of three int (H, W, C) (84, 84, 0) H, W: [1, 512]\nC: 0, 1 or 3 Frame Warps the frame from original Game resolution to H X W size.\nC values:\n0 - Deactivated;\n1 - Grayscale;\n3 - RGB;\nKeeps observation element of type Box, changes its shape wrappers_settings[\u0026#34;hwc_obs_resize\u0026#34;] = (128, 128, 1) Frame Stacking With Optional Dilation Key Type Default Value(s) Value Range Target Observation Element Description frame_stack int 1 [1, 48] Frame Stacks latest value frames together along the third dimension.\nKeeps observation element of type Box, changes its shape dilation int 1 [1, 48] Frame Builds frame stacks adding one every value frames.\nKeeps observation element of type Box wrappers_settings[\u0026#34;frame_stack\u0026#34;] = 4 wrappers_settings[\u0026#34;dilation\u0026#34;] = 1 Action Stacking Key Type Default Value(s) Value Range Target Observation Element Description actions_stack int 1 [1, 48] Actions-Move,\nActions-Attack Stacks latest value actions together for both moves and attacks.\nChanges observation element type from Discrete to MultiDiscrete, having value elements, each with cardinality equal to the original Discrete one wrappers_settings[\u0026#34;actions_stack\u0026#34;] = 12 Scaling Key Type Default Value(s) Value Range Target Observation Element Description scale bool False True / False All Activates observation scaling.\nAffects observation elements as follows:\n- Box: type kept, changes bounds according to scale_mod setting.\n- Discrete (Binary): depends on process_discrete_binary (see below).\n- Discrete: changed to MultiBinary through one-hot encoding, size equal to original Discrete cardinality.\n- MultiDiscrete: changed to N-MultiBinary through N-times one-hot encoding, N equal to original MultiDiscrete size, encoding size equal to original MultiDiscrete element cardinality exclude_image_scaling bool False True / False Frame If True, prevents scaling to be applied on the game frame process_discrete_binary bool False True / False Discrete Binary Controls how Discrete (Binary) observations are treated: - False: not affected; - True: changed to MultiBinary through one-hot encoding, size equal to 2 scale_mod int 0 [0, 1] All Defines the scaling bounds: 0 - [0, 1]; 1 - [-1, 1].\nKeeps observation elements of the same type wrappers_settings[\u0026#34;scale\u0026#34;] = True wrappers_settings[\u0026#34;exclude_image_scaling\u0026#34;] = True wrappers_settings[\u0026#34;process_discrete_binary\u0026#34;] = True wrappers_settings[\u0026#34;scale_mod\u0026#34;] = 0 Flattening and Filtering Key Type Default Value(s) Value Range Target Observation Element Description flatten bool False True / False RAM States Activates RAM States dictionary flattening, removing nested dictionaries and creating new keys joining original ones using \u0026ldquo;_\u0026rdquo; across nesting levels. For example: obs[\u0026quot;P1\u0026quot;][\u0026quot;key1\u0026quot;] becomes obs[\u0026quot;P1_key1\u0026quot;] filter_keys list of str None - RAM States Defines the list of RAM states to keep in the observation space wrappers_settings[\u0026#34;flatten\u0026#34;] = True wrappers_settings[\u0026#34;filter_keys\u0026#34;] = [\u0026#34;stage\u0026#34;, \u0026#34;P1_ownHealth\u0026#34;, \u0026#34;P1_oppChar\u0026#34;] Action Wrappers Actions Sticking Key Type Default Value(s) Value Range Description sticky_actions int 1 [1, 12] Keeps repeating the same action for value environment steps In order to use this wrapper, the step_ratio generic setting must be set to equal to 1.\nwrappers_settings[\u0026#34;sticky_actions\u0026#34;] = 1 Reward Wrappers Reward Normalization Key Type Default Value(s) Value Range Description reward_normalization bool False True / False Activates reward normalization. Divides reward value by the product between the reward_normalization_factor (see next row) value and the delta between maximum and minium health bar value for the given game reward_normalization_factor float 0.5 [-inf, +inf] Defines the normalization factor multiplying the delta between maximum and minium health bar value for the given game wrappers_settings[\u0026#34;reward_normalization\u0026#34;] = True wrappers_settings[\u0026#34;reward_normalization_factor\u0026#34;] = 0.5 Reward Clipping Key Type Default Value(s) Value Range Description clip_rewards bool False True / False Activates reward clipping. Applies the sign function to the reward value wrappers_settings[\u0026#34;clip_rewards\u0026#34;] = False "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/humanexperiencerecorder/","title":"Human Experience Recorder","tags":[],"description":"","content":"This example focuses on:\nHuman experience recording settings configuration Controller interfacing (Gamepad or Keyboard) A dedicated section describing human experience recording wrapper settings is presented here and provides additional details on their usage and purpose.\nDepending on the Operating System used, specific permissions may be needed in order to read the keyboard inputs.\n- On Windows, by default no specific permissions are needed. However, if you have some third-party security software you may need to white-list Python.\n- On Linux you need to add the user the input group: sudo usermod -aG input $USER\n- On Mac, it is possible you need to use the settings application to allow your program to access the input devices (see this reference).\nOfficial inputs python package reference guide can be found at this link\nModules import import os from os.path import expanduser import diambra.arena from diambra.arena.utils.controller import get_diambra_controller Settings # Environment Settings settings = {} settings[\u0026#34;player\u0026#34;] = \u0026#34;Random\u0026#34; settings[\u0026#34;step_ratio\u0026#34;] = 1 settings[\u0026#34;frame_shape\u0026#34;] = (128, 128, 1) settings[\u0026#34;action_space\u0026#34;] = \u0026#34;multi_discrete\u0026#34; settings[\u0026#34;attack_but_combination\u0026#34;] = True # Gym wrappers settings wrappers_settings = {} wrappers_settings[\u0026#34;reward_normalization\u0026#34;] = True wrappers_settings[\u0026#34;frame_stack\u0026#34;] = 4 wrappers_settings[\u0026#34;actions_stack\u0026#34;] = 12 wrappers_settings[\u0026#34;scale\u0026#34;] = True Experience recording settings # Gym trajectory recording wrapper kwargs traj_rec_settings = {} home_dir = expanduser(\u0026#34;~\u0026#34;) # Username traj_rec_settings[\u0026#34;username\u0026#34;] = \u0026#34;Alex\u0026#34; # Path where to save recorderd trajectories game_id = \u0026#34;doapp\u0026#34; traj_rec_settings[\u0026#34;file_path\u0026#34;] = os.path.join(home_dir, \u0026#34;diambraArena/trajRecordings\u0026#34;, game_id) # If to ignore P2 trajectory (useful when collecting # only human trajectories while playing as a human against a RL agent) traj_rec_settings[\u0026#34;ignore_p2\u0026#34;] = False Envionment execution env = diambra.arena.make(game_id, settings, wrappers_settings, traj_rec_settings) # Controller initialization controller = get_diambra_controller(env.action_list) controller.start() observation = env.reset() while True: env.render() actions = controller.get_actions() observation, reward, done, info = env.step(actions) if done: observation = env.reset() break controller.stop() env.close() "},{"uri":"https://docs.diambra.ai/envs/games/samsh5sp/","title":"Samurai Showdown 5 Special","tags":[],"description":"","content":" Index Game Specific Info Game Specific Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID samsh5sp Original ROM Name samsh5sp.zip SHA256 Checksum adf33d8a02f3d900b4aa95e62fb21d9278fb920b179665b12a489bd39a6c103d Search Keywords SAMURAI SHODOWN V SPECIAL, samurai-shodown-v-special, 100347, wowroms Game Resolution\n(H X W X C) 224px X 320px X 3 Number of Moves and Attack Actions\n(Without Buttons Combination) 9, 11 (5)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-10): (No-Attack, Weak Slash, Medium Slash, Kick, Meditation), Weak Slash + Medium Slash (Strong Slash), Medium Slash + Kick (Surprise Attack), Weak Slash + Kick, Kick + Meditation, Weak Slash + Medium Slash + Kick (Rage), Medium Slash + Kick + Meditation Max Difficulty (1P Mode) 8 Number of Characters (Selectable) 28 (28) Max Number of Outfits 4 Number of Stages (1P Mode) 9 Game Specific Settings Key Type Default Value(s) Value Range difficulty int 6 [1, 8] characters str or tuple of maximum three str Random Kyoshiro, Jubei, Hanzo, Enja, Amakusa, Suija, Galford, Charlotte, Kusare, Sogetsu, Gaira, Ukyo, Yoshitora, Gaoh, Haohmaru, Genjuro, Shizumaru, Kazuki, Tamtam, Rasetsumaru, Rimururu, Mina, Zankuro, Nakoruru, Rera, Yunfei, Basara, Mizuki char_outfits int 1 [1, 4] characters and char_outfits need to be provided as tuples of two elements (the first for P1 and the second for P2) when using this environment in two players mode.\nAction Spaces Type Attack Buttons\nCombination Space Size (Number of Actions) Discrete Not active 9 (moves) + 5 (attacks) - 1 (no-action counted twice) = 13 Discrete Active 9 (moves) + 11 (attacks) - 1 (no-action counted twice) = 19 MultiDiscrete Not active 9 (moves) X 5 (attacks) = 45 MultiDiscrete Active 9 (moves) X 11 (attacks) = 99 Observation Space Some examples of Samurai Showdown 5 Special RAM states Global Key Type Value Description frame Box [0, 255] X [224 X 320 X 3] Latest game frame (RGB pixel screen) stage Box [1, 9] Current stage of the game Player specific Key Type Value Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, 2] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, 27] Index of first character selected (since in this game only one character is selected, these values are the same as \u0026ldquo;character in use\u0026rdquo;)\n0: Kyoshiro, 1: Jubei, 2: Hanzo, 3: Enja, 4: Amakusa, 5: Suija , 6: Galford, 7: Charlotte, 8: Kusare, 9: Sogetsu, 10: Gaira, 11: Ukyo, 12: Yoshitora, 13: Gaoh, 14: Haohmaru, 15: Genjuro, 16: Shizumaru, 17: Kazuki, 18: Tamtam, 19: Rasetsumaru, 20: Rimururu, 21: Mina, 22: Zankuro, 23: Nakoruru, 24: Rera, 25: Yunfei, 26: Basara, 27: Mizuki ownChar/oppChar Discrete [0, 27] Index of character in use\n0: Kyoshiro, 1: Jubei, 2: Hanzo, 3: Enja, 4: Amakusa, 5: Suija , 6: Galford, 7: Charlotte, 8: Kusare, 9: Sogetsu, 10: Gaira, 11: Ukyo, 12: Yoshitora, 13: Gaoh, 14: Haohmaru, 15: Genjuro, 16: Shizumaru, 17: Kazuki, 18: Tamtam, 19: Rasetsumaru, 20: Rimururu, 21: Mina, 22: Zankuro, 23: Nakoruru, 24: Rera, 25: Yunfei, 26: Basara, 27: Mizuki ownHealth/oppHealth Box [0, 125] Health bar value actions+move Discrete [0, 8] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, 7] or [0, 3] Index of latest attack action performed (no-attack, hold, punch, etc.) with, respectively, attack buttons combination active or not ownRageOn/oppRageOn Discrete (Binary) [0, 1] Rage on for the player\n0: False, 1: True ownRageUsed/oppRageUsed Discrete (Binary) [0, 1] Rage used by the player\n0: False, 1: True ownWeaponLost/oppWeaponLost Discrete (Binary) [0, 1] Weapon lost by the player\n0: False, 1: True ownWeaponFight/oppWeaponFight Discrete (Binary) [0, 1] Weapon fight condition triggered\n0: False, 1: True ownRageBar/oppRageBar Box [0, 164096] Rage bar value ownWeaponBar/oppWeaponBar Box [0, 120] Weapon bar value ownPowerBar/oppPowerBar Box [0, 64] Power bar value "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/submityourownagent/","title":"Submit Your Own Agent","tags":[],"description":"","content":"These are the steps to submit your own agent:\nStore your agent files (e.g. scripts and weights) in private repository, we will use GitHub as example Create your personal access token (official docs here): Go to \u0026ldquo;Settings\u0026rdquo; in the top-right corner of the GitHub website. Click \u0026ldquo;Developer settings\u0026rdquo; at the bottom-left of the page. Click \u0026ldquo;Personal access tokens\u0026rdquo; and then \u0026ldquo;Generate new token.\u0026rdquo; Give your token a name, select the necessary scopes (e.g., \u0026ldquo;repo\u0026rdquo; for accessing private repositories), and click \u0026ldquo;Generate token.\u0026rdquo; Copy the generated token and save it somewhere safe, as you won\u0026rsquo;t be able to see it again. Submit your AI agent: Choose the appropriate dependencies docker image for your submission. We provide different pre-built ones giving access to various common third party libraries Submit your agent as shown in the following examples Example 1: Command Line Interface Command Assuming you are using the arena-stable-baselines3-on3.10-bullseye dependencies image and have your agent\u0026rsquo;s files stored on GitHub:\ndiambra agent submit \\ --submission.mode AIvsCOM \\ --submission.source agent.py=https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/trained-agent/your_agent.py \\ --submission.source models/model.zip=https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/nn-weights/your_model.zip \\ --submission.secret token=your_gh_token \\ --submission.set-command \\ arena-stable-baselines3-on3.10-bullseye \\ python \u0026#34;/sources/agent.py\u0026#34; \u0026#34;/sources/models/model.zip\u0026#34; Replace your_gh_token, your_agent.py and your_model.zip with the appropriate values.\nThe --submission.source flag takes URL\u0026lt;-\u0026gt;path/in/container mappings to download files to the specified path. The {{ .Secrets.\u0026lt;something\u0026gt; }} can be used to include secrets specified in the --submission.secret flag. Combining both flags, you can create a submission that includes secrets and sources to download your weights from your private repository.\nExample 2: Using a Manifest File Alternatively, you can use a manifest file to define your submission. Assuming you are using the arena-stable-baselines3-on3.10-bullseye dependencies image and have your agent\u0026rsquo;s files stored on GitHub, create a file named submission-manifest.yaml with the following content:\nmode: AIvsCOM image: diambra/arena-stable-baselines3-on3.10-bullseye:main command: - python - \u0026#34;/sources/agent.py\u0026#34; - \u0026#34;/sources/models/model.zip\u0026#34; sources: agent.py: https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/trained-agent/your_agent.py models/model.zip: https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/nn-weights/your_model.zip Replace your_agent.py and your_model.zip with the appropriate values.\nThen, submit your agent using the manifest file:\ndiambra agent submit --submission.secret token=your_gh_token --submission.manifest submission-manifest.yaml Replace your_gh_token with the GitHub token you saved earlier.\nDo not add your tokens directly in the submission YAML file, they will be publicly visible.\n"},{"uri":"https://docs.diambra.ai/utils/","title":"Utils","tags":[],"description":"","content":"Index Available Games ROMs Check Environment Spaces Summary Observation Inspection Gym Observation Wrapped Observation Controller Interface DIAMBRA Arena comes with many different tools supporting development and debug. They provide different functionalities, all described below in the sections below where both code and output is reported.\nSource code can be found in the code repository, here.\nAvailable Games Provides a list of available games and their most important details. It is executed as shown below:\nimport diambra.arena diambra.arena.available_games(print_out=True, details=True) Output will be similar to what follows:\nTitle: Dead Or Alive ++ - GameId: doapp Difficulty levels: Min 1 - Max 4 SHA256 sum: d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Original ROM name: doapp.zip Search keywords: [\u0026#39;DEAD OR ALIVE ++ [JAPAN]\u0026#39;, \u0026#39;dead-or-alive-japan\u0026#39;, \u0026#39;80781\u0026#39;, \u0026#39;wowroms\u0026#39;] Characters list: [\u0026#39;Kasumi\u0026#39;, \u0026#39;Zack\u0026#39;, \u0026#39;Hayabusa\u0026#39;, \u0026#39;Bayman\u0026#39;, \u0026#39;Lei-Fang\u0026#39;, \u0026#39;Raidou\u0026#39;, \u0026#39;Gen-Fu\u0026#39;, \u0026#39;Tina\u0026#39;, \u0026#39;Bass\u0026#39;, \u0026#39;Jann-Lee\u0026#39;, \u0026#39;Ayane\u0026#39;] Title: Street Fighter III - GameId: sfiii3n Difficulty levels: Min 1 - Max 8 SHA256 sum: 7239b5eb005488db22ace477501c574e9420c0ab70aeeb0795dfeb474284d416 Original ROM name: sfiii3n.zip Search keywords: [\u0026#39;STREET FIGHTER III 3RD STRIKE: FIGHT FOR THE FUTUR [JAPAN] (CLONE)\u0026#39;, \u0026#39;street-fighter-iii-3rd-strike-fight-for-the-futur-japan-clone\u0026#39;, \u0026#39;106255\u0026#39;, \u0026#39;wowroms\u0026#39;] Characters list: [\u0026#39;Alex\u0026#39;, \u0026#39;Twelve\u0026#39;, \u0026#39;Hugo\u0026#39;, \u0026#39;Sean\u0026#39;, \u0026#39;Makoto\u0026#39;, \u0026#39;Elena\u0026#39;, \u0026#39;Ibuki\u0026#39;, \u0026#39;Chun-Li\u0026#39;, \u0026#39;Dudley\u0026#39;, \u0026#39;Necro\u0026#39;, \u0026#39;Q\u0026#39;, \u0026#39;Oro\u0026#39;, \u0026#39;Urien\u0026#39;, \u0026#39;Remy\u0026#39;, \u0026#39;Ryu\u0026#39;, \u0026#39;Gouki\u0026#39;, \u0026#39;Yun\u0026#39;, \u0026#39;Yang\u0026#39;, \u0026#39;Ken\u0026#39;, \u0026#39;Gill\u0026#39;] Title: Tekken Tag Tournament - GameId: tektagt Difficulty levels: Min 1 - Max 9 SHA256 sum: 57be777eae0ee9e1c035a64da4c0e7cb7112259ccebe64e7e97029ac7f01b168 Original ROM name: tektagtac.zip Search keywords: [\u0026#39;TEKKEN TAG TOURNAMENT [ASIA] (CLONE)\u0026#39;, \u0026#39;tekken-tag-tournament-asia-clone\u0026#39;, \u0026#39;108661\u0026#39;, \u0026#39;wowroms\u0026#39;] Notes: Rename the rom from tektagtac.zip to tektagt.zip Characters list: [\u0026#39;Xiaoyu\u0026#39;, \u0026#39;Yoshimitsu\u0026#39;, \u0026#39;Nina\u0026#39;, \u0026#39;Law\u0026#39;, \u0026#39;Hwoarang\u0026#39;, \u0026#39;Eddy\u0026#39;, \u0026#39;Paul\u0026#39;, \u0026#39;King\u0026#39;, \u0026#39;Lei\u0026#39;, \u0026#39;Jin\u0026#39;, \u0026#39;Baek\u0026#39;, \u0026#39;Michelle\u0026#39;, \u0026#39;Armorking\u0026#39;, \u0026#39;Gunjack\u0026#39;, \u0026#39;Anna\u0026#39;, \u0026#39;Brian\u0026#39;, \u0026#39;Heihachi\u0026#39;, \u0026#39;Ganryu\u0026#39;, \u0026#39;Julia\u0026#39;, \u0026#39;Jun\u0026#39;, \u0026#39;Kunimitsu\u0026#39;, \u0026#39;Kazuya\u0026#39;, \u0026#39;Bruce\u0026#39;, \u0026#39;Kuma\u0026#39;, \u0026#39;Jack-Z\u0026#39;, \u0026#39;Lee\u0026#39;, \u0026#39;Wang\u0026#39;, \u0026#39;P.Jack\u0026#39;, \u0026#39;Devil\u0026#39;, \u0026#39;True Ogre\u0026#39;, \u0026#39;Ogre\u0026#39;, \u0026#39;Roger\u0026#39;, \u0026#39;Tetsujin\u0026#39;, \u0026#39;Panda\u0026#39;, \u0026#39;Tiger\u0026#39;, \u0026#39;Angel\u0026#39;, \u0026#39;Alex\u0026#39;, \u0026#39;Mokujin\u0026#39;, \u0026#39;Unknown\u0026#39;] ... ROMs Check Checks ROM SHA256 checksum to validate them.\nWithout game_id specification If no game_id is specified, the ROM file provided as first argument, will be verified against all available games. It is executed as shown below:\nimport diambra.arena diambra.arena.check_game_sha_256(path=\u0026#34;path/to/specific/rom/doapp.zip\u0026#34;, game_id=None) Output will be similar to what follows:\nCorrect ROM file for Dead Or Alive ++, sha256 = d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e With game_id specification If game_id is specified, the checksum of the ROM file provided will be compared with the one of the specified game identified by game_id. It is executed as shown below:\nimport diambra.arena diambra.arena.check_game_sha_256(path=\u0026#34;path/to/specific/rom/doapp.zip\u0026#34;, game_id=\u0026#34;umk3\u0026#34;) Output will be similar to what follows:\nExpected SHA256 Checksum: f48216ad82f78cb86e9c07d2507be347f904f4b5ae354a85ae7c34d969d265af Retrieved SHA256 Checksum: d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Environment Spaces Summary Prints out a summary of Environment\u0026rsquo;s Observation and Action spaces showing nesting levels, keys, space types and low/high bounds where applicable. It is executed as shown below:\nfrom diambra.arena.gym_utils import env_spaces_summary ... env_spaces_summary(env=environment) Output will be similar to what follows:\nObservation space: observation_space[\u0026#34;P1\u0026#34;][\u0026#34;actions\u0026#34;][\u0026#34;attack\u0026#34;]: Discrete(4) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;actions\u0026#34;][\u0026#34;move\u0026#34;]: Discrete(9) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;oppChar\u0026#34;]: Discrete(11) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;oppChar1\u0026#34;]: Discrete(11) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;oppHealth\u0026#34;]: Box() Space type = int32 Space high bound = 208 Space low bound = 0 observation_space[\u0026#34;P1\u0026#34;][\u0026#34;oppSide\u0026#34;]: Discrete(2) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;oppWins\u0026#34;]: Box() Space type = int32 Space high bound = 2 Space low bound = 0 observation_space[\u0026#34;P1\u0026#34;][\u0026#34;ownChar\u0026#34;]: Discrete(11) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;ownChar1\u0026#34;]: Discrete(11) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;ownHealth\u0026#34;]: Box() Space type = int32 Space high bound = 208 Space low bound = 0 observation_space[\u0026#34;P1\u0026#34;][\u0026#34;ownSide\u0026#34;]: Discrete(2) observation_space[\u0026#34;P1\u0026#34;][\u0026#34;ownWins\u0026#34;]: Box() Space type = int32 Space high bound = 2 Space low bound = 0 observation_space[\u0026#34;frame\u0026#34;]: Box(480, 512, 3) Space type = uint8 Space high bound = [[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]] ... [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]] Space low bound = [[[0 0 0] [0 0 0] [0 0 0] ... [0 0 0] [0 0 0] [0 0 0]] ... [[0 0 0] [0 0 0] [0 0 0] ... [0 0 0] [0 0 0] [0 0 0]]] observation_space[\u0026#34;stage\u0026#34;]: Box() Space type = int8 Space high bound = 8 Space low bound = 1 Action space: action_space = Discrete(12) Space type = int64 Space n = 12 Observation Inspection Prints out a detailed description of Environment\u0026rsquo;s observation content, showing every level of it. The only element that is not printed in the terminal is the game frame that, when viz input argument is set to True, is shown is a dedicated graphical window. The wait_key parameter defines how this window behaves: when set equal to 0, it pauses waiting for the user to press a button, while if set different from zero, it waits the prescribed number of milliseconds before continuing.\nThere are two different methods, one to be used for the basic Gym Environment and the other one specific for the wrapped one.\nGym Observation Use of this functionality can be found in this and this examples.\nfrom diambra.arena.gym_utils import show_gym_obs ... show_gym_obs(observation=obs, char_list=characters_names, wait_key=1, viz=True) Output will be similar to what follows:\nFrame visualization window: Terminal printout: observation[\u0026#34;frame\u0026#34;].shape: (480, 512, 3) observation[\u0026#34;stage\u0026#34;]: 1 observation[\u0026#34;P1\u0026#34;][\u0026#34;ownChar1\u0026#34;]: Kasumi observation[\u0026#34;P1\u0026#34;][\u0026#34;oppChar1\u0026#34;]: Bayman observation[\u0026#34;P1\u0026#34;][\u0026#34;ownChar\u0026#34;]: Kasumi observation[\u0026#34;P1\u0026#34;][\u0026#34;oppChar\u0026#34;]: Bayman observation[\u0026#34;P1\u0026#34;][\u0026#34;ownHealth\u0026#34;]: 66 observation[\u0026#34;P1\u0026#34;][\u0026#34;oppHealth\u0026#34;]: 184 observation[\u0026#34;P1\u0026#34;][\u0026#34;ownSide\u0026#34;]: 0 observation[\u0026#34;P1\u0026#34;][\u0026#34;oppSide\u0026#34;]: 1 observation[\u0026#34;P1\u0026#34;][\u0026#34;ownWins\u0026#34;]: 0 observation[\u0026#34;P1\u0026#34;][\u0026#34;oppWins\u0026#34;]: 0 observation[\u0026#34;P1\u0026#34;][\u0026#34;actions\u0026#34;]: {\u0026#39;move\u0026#39;: 0, \u0026#39;attack\u0026#39;: 3} Wrapped Observation Use of this functionality can be found in this and this examples.\nThis functionality currently does not support all possible wrappers configurations but only a part of them. In particular, it assumes the observation normalization wrapper is active.\nfrom diambra.arena.gym_utils import show_wrap_obs ... show_wrap_obs(observation=obs, n_actions_stack=n_act_stack, char_list=characters_names, wait_key=1, viz=True) Output will be similar to what follows:\nFrame stack visualization windows: Terminal printout:\nobservation[\u0026#34;frame\u0026#34;].shape: (128, 128, 4) observation[\u0026#34;stage\u0026#34;]: 0.0 observation[\u0026#34;P1\u0026#34;][\u0026#34;ownChar1\u0026#34;]: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] / Bayman observation[\u0026#34;P1\u0026#34;][\u0026#34;oppChar1\u0026#34;]: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] / Kasumi observation[\u0026#34;P1\u0026#34;][\u0026#34;ownChar\u0026#34;]: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] / Bayman observation[\u0026#34;P1\u0026#34;][\u0026#34;oppChar\u0026#34;]: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] / Kasumi observation[\u0026#34;P1\u0026#34;][\u0026#34;ownHealth\u0026#34;]: 0.8173076923076923 observation[\u0026#34;P1\u0026#34;][\u0026#34;oppHealth\u0026#34;]: 0.8028846153846154 observation[\u0026#34;P1\u0026#34;][\u0026#34;ownSide\u0026#34;]: 1 observation[\u0026#34;P1\u0026#34;][\u0026#34;oppSide\u0026#34;]: 0 observation[\u0026#34;P1\u0026#34;][\u0026#34;ownWins\u0026#34;]: 0.0 observation[\u0026#34;P1\u0026#34;][\u0026#34;oppWins\u0026#34;]: 0.0 observation[\u0026#34;P1\u0026#34;][\u0026#34;actions\u0026#34;][\u0026#34;move\u0026#34;]: [[1 0 0 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1] [1 0 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 1 0 0] [0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 1]] observation[\u0026#34;P1\u0026#34;][\u0026#34;actions\u0026#34;][\u0026#34;attack\u0026#34;]: [[1 0 0 0] [1 0 0 0] [1 0 0 0] [0 0 1 0] [1 0 0 0] [1 0 0 0] [1 0 0 0] [1 0 0 0] [1 0 0 0] [1 0 0 0] [1 0 0 0] [1 0 0 0]] Controller Interface Use of this functionality can be found in this example.\nDepending on the Operating System used, specific permissions may be needed in order to read the keyboard inputs.\n- On Windows, by default no specific permissions are needed. However, if you have some third-party security software you may need to white-list Python.\n- On Linux you need to add the user the input group: sudo usermod -aG input $USER\n- On Mac, it is possible you need to use the settings application to allow your program to access the input devices (see this reference).\nOfficial inputs python package reference guide can be found at this link\nIt allows to easily interface a common USB Gamepad or the Keyboard to DIAMBRA Arena environments, to be used for experiments where human input is required, for example Human Expert Demonstration Collection or Competitive Human-Agent. The following code snippet shows a typical usage.\nimport diambra.arena from diambra.arena.utils.controller import get_diambra_controller ... # Controller initialization controller = get_diambra_controller(env.action_list) controller.start() ... actions = controller.get_actions() ... controller.stop() "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/imitationlearning/","title":"Imitation Learning","tags":[],"description":"","content":"This example focuses on:\nHuman expert demonstration loader class usage for Imitation Learning A dedicated section describing recorded experience loader is presented here and provides additional details on its usage and purpose.\nModules import import diambra.arena import os import numpy as np Imitation learning settings # Show files in folder base_path = os.path.dirname(os.path.abspath(__file__)) recorded_traj_folder = os.path.join(base_path, \u0026#34;recordedTrajectories\u0026#34;) recorded_traj_files = [os.path.join(recorded_traj_folder, f) for f in os.listdir(recorded_traj_folder) if os.path.isfile(os.path.join(recorded_traj_folder, f))] print(recorded_traj_files) # Imitation learning settings settings = {} # List of recorded trajectories files settings[\u0026#34;traj_files_list\u0026#34;] = recorded_traj_files # Number of parallel Imitation Learning environments that will be run settings[\u0026#34;total_cpus\u0026#34;] = 2 # Rank of the created environment settings[\u0026#34;rank\u0026#34;] = 0 Environment execution env = diambra.arena.ImitationLearning(**settings) observation = env.reset() env.render(mode=\u0026#34;human\u0026#34;) env.show_obs(observation) # Show trajectory summary env.traj_summary() while True: dummy_actions = 0 observation, reward, done, info = env.step(dummy_actions) env.render(mode=\u0026#34;human\u0026#34;) env.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if np.any(env.exhausted): break if done: observation = env.reset() env.render(mode=\u0026#34;human\u0026#34;) env.show_obs(observation) env.close() "},{"uri":"https://docs.diambra.ai/envs/games/kof98umh/","title":"The King of Fighers &#39;98 UMH","tags":[],"description":"","content":" Index Game Specific Info Game Specific Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID kof98umh Original ROM Name kof98umh.zip SHA256 Checksum beb7bdea87137832f5f6d731fd1abd0350c0cd6b6b2d57cab2bedbac24fe8d0a Search Keywords The King Of Fighters '98: Ultimate Match HERO, kof98umh, allmyroms Game Resolution\n(H X W X C) 240px X 320px X 3 Number of Moves and Attack Actions\n(Without Buttons Combination) 9, 9 (5)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-8): (No-Attack, Weak Punch, Weak Kick, Strong Punch, Strong Kick), Weak Punch + Weak Kick, Strong Punch + Strong Kick, Weak Punch + Weak Kick + Strong Punch + Strong Kick, Weak Punch + Weak Kick + Strong Punch Max Difficulty (1P Mode) 8 Number of Characters (Selectable) 45 (43) Max Number of Outfits 4 Number of Stages (1P Mode) 7 Game Specific Settings Key Type Default Value(s) Value Range difficulty int 6 [1, 8] characters str or tuple of maximum three str (Random, Random, Random) Kyo, Benimaru, Daimon, Terry, Andy, Joe, Ryo, Robert, Yuri, Leona, Ralf, Clark, Athena, Kensou, Chin, Chizuru, Mai, King, Kim, Chang, Choi, Yashiro, Shermie, Chris, Yamazaki, Mary, Billy, Iori, Mature, Vice, Heidern, Takuma, Saisyu, Heavy-D!, Lucky, Brian, Eiji, Kasumi, Shingo, Rugal, Geese, Krauser, Mr.Big char_outfits int 1 [1, 4] Extended game settings Key Type Default Value(s) Value Range Description fighting_style int 0 [0, 3] Selects the fighting style.\n0: Random, 1: Advanced, 2: Extra, 3: Ultimate ultimate_style tuple of three int (0, 0, 0) [0, 2] X [0, 2] X [0, 2] Selects details about ultimate fighting style for Dash, Evade and Bar features.\n0: Random, 1: Advanced, 2: Extra characters, char_outfits, fighting_style and ultimate_style need to be provided as tuples of two elements (the first for P1 and the second for P2) when using this environment in two players mode.\nAction Spaces Type Attack Buttons\nCombination Space Size (Number of Actions) Discrete Not active 9 (moves) + 5 (attacks) - 1 (no-action counted twice) = 13 Discrete Active 9 (moves) + 9 (attacks) - 1 (no-action counted twice) = 17 MultiDiscrete Not active 9 (moves) X 5 (attacks) = 45 MultiDiscrete Active 9 (moves) X 9 (attacks) = 81 Observation Space Some examples of The King of Fighters '98 Ultimate Match RAM states Global Key Type Value Description frame Box [0, 255] X [240 X 320 X 3] Latest game frame (RGB pixel screen) stage Box [1, 7] Current stage of the game Player specific Key Type Value Description ownSide/oppSide Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right ownWins/oppWins Box [0, 3] Number of rounds won by the player ownChar1/oppChar1 Discrete [0, 44] Index of first character selected\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi ownChar2/oppChar2 Discrete [0, 44] Index of second character selected\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi ownChar3/oppChar3 Discrete [0, 44] Index of third character selected\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi ownChar/oppChar Discrete [0, 44] Index of character in use\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi ownHealth/oppHealth Box [-1, 119] Health bar value actions+move Discrete [0, 8] Index of latest move action performed (no-move, left, left+up, up, etc.) actions+attack Discrete [0, 7] or [0, 3] Index of latest attack action performed (no-attack, hold, punch, etc.) with, respectively, attack buttons combination active or not ownPowerBar/oppPowerBar Box [0, 100] Power bar value ownSpecialAttacks/oppSpecialAttacks Box [0, 5] Number of special attacks available ownActiveChar/oppActiveChar Box [0, 2] Index of the active character\n0: first, 1: second, 2: third ownBarType/oppBarType Discrete [0, 7] Index of bar type\n0: Advanced / Ultimate (Dash Advanced, Evade Advanced, Bar Advanced), 1: Extra / Ultimate (Dash Extra, Evade Extra, Bar Extra), 2: Ultimate (Dash Extra, Evade Advanced, Bar Advanced), 3: Ultimate (Dash Advanced, Evade Advanced, Bar Extra), 4: Ultimate (Dash Extra, Evade Advanced, Bar Extra), 5: Ultimate (Dash Advanced, Evade Extra, Bar Advanced), 6: Ultimate (Dash Extra, Evade Extra, Bar Advanced), 7: Ultimate (Dash Advanced, Evade Extra, Bar Extra) "},{"uri":"https://docs.diambra.ai/competitionplatform/argumentsandcommands/","title":"Arguments and Commands","tags":[],"description":"","content":"In case you want to specify command line arguments and/or overriding the image entrypoint at submission time, you can leverage the command line interface. Here are the different use cases covered:\nAdd arguments to a given docker image diambra agent submit \u0026lt;docker image\u0026gt; arg1 arg2 the correspondent submission manifest would use a new args keyword as follows:\n--- image: \u0026lt;docker image\u0026gt; mode: AIvsCOM difficulty: easy args: - arg1 - arg2 Add arguments to a given submission manifest diambra agent submit --submission.manifest manifest.yaml arg1 arg2 arg3 the resulting submission manifest sent to the platform would be\n--- image: diambra/agent-random-1:main mode: AIvsCOM difficulty: easy args: - arg1 - arg2 - arg3 Override entrypoint of a given image diambra agent submit --submission.set-command \u0026lt;docker image\u0026gt; command arg1 arg2 the correspondent submission manifest would be:\n--- image: \u0026lt;docker image\u0026gt; mode: AIvsCOM difficulty: easy command: - command - arg1 - arg2 Override entrypoint of an image specified in a given submission manifest diambra agent submit --submission.set-command --submission.manifest manifest.yaml command arg1 arg2 the resulting submission manifest sent to the platform would be\n--- image: diambra/agent-random-1:main mode: AIvsCOM difficulty: easy command: - command - arg1 - arg2 "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/customdependenciesimage/","title":"Custom Dependencies Image","tags":[],"description":"","content":"Instead of using the pre-built dependencies docker images we provide, you may want/need to create custom ones. It can easily be done in just a few steps:\nCreate the Dockerfile containing the custom dependencies you need. Any mix of publicly available packages and repository, and copies of libraries you have in your local system work.\nBuild the docker image with your custom dependencies:\ndocker build -t \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; . This will create the docker image and tag it. You can use any public registry, like quay.io or dockerhub, but make sure the image is public.\nPush the image to the registry:\ndocker push \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; Once these steps are completed, you can submit the agent to the platform using your custom dependecies images. Assuming you are in the very same situation explained in the examples shown in the Submit Your Own Agent page, you would tweak them, respectively, as follows:\nExample 1: Command Line Interface Command\nUpdate the image name command line argument:\ndiambra agent submit \\ --submission.mode AIvsCOM \\ --submission.source agent.py=https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/trained-agent/your_agent.py \\ --submission.source models/model.zip=https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/nn-weights/your_model.zip \\ --submission.secret token=your_gh_token \\ --submission.set-command \\ \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; \\ python \u0026#34;/sources/agent.py\u0026#34; \u0026#34;/sources/models/model.zip\u0026#34; Example 2: Using a Manifest File\nUpdate the image name in the submission manifest:\nmode: AIvsCOM image: \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; command: - python - \u0026#34;/sources/agent.py\u0026#34; - \u0026#34;/sources/models/model.zip\u0026#34; sources: agent.py: https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/trained-agent/your_agent.py models/model.zip: https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/nn-weights/your_model.zip Please note that the dependencies docker images needs to be public and will be publicly visible on the platform. Make sure you do not include in them any file you want to keep private.\nCurrently, we can only process Docker images built for amd64 CPU architecture. So, if you are using MacOS with M1 or M2 CPUs, you need to explicitly tell Docker to do that at build time as follows:\n1. Open Docker Desktop Dashboard / Preferences (cog icon) / Turn \u0026ldquo;Experimental Features\u0026rdquo; on \u0026amp; apply\n2. Create a new builder instance with docker buildx create --use\n3. Run docker buildx build --platform linux/amd64 --push -t \u0026lt;image-tag\u0026gt; .\nNote that:\n- If you can’t see an “Experimental Features” option, sign up for the Docker developer program\n- You have to push directly to a repository instead of doing it after build\n"},{"uri":"https://docs.diambra.ai/imitationlearning/","title":"Imitation Learning","tags":[],"description":"","content":" Index Experience Recording Wrapper Recorded Experience Loader With the goal of easing the usage of Imitation Learning, DIAMBRA Arena comes with two, easy-to-use, useful features: an environment wrapper allowing to record agent experience (to be used, for example, to save human expert demonstrations), and a loader class allowing to flawlessly serve stored trajectories.\nExperience Recording Wrapper In order to activate the experience recording wrapper, one has just to add an additional kwargs dictionary, here named traj_rec_settings, to the environment creation method, as shown in the next code block. The dictionary has to be populated as described below.\nenv = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings, wrappers_settings, traj_rec_settings) Implementation examples and templates can be found in the code repository, here.\nUse of this functionality can be found in this example.\nKey Type Default Value(s) Value Range Description username str - - Provides an identifier to be associated with the recorded trajectory file_path str - - Specifies the path where to save recorded experiences ignore_p2 int - [0, 1] Specifies if to ignore P2 experience. Useful for example when recording expert demonstrations of a human player (P1) who is playing against an RL agent (P2) trajRecSettings[\u0026#34;username\u0026#34;] = \u0026#34;user\u0026#34; trajRecSettings[\u0026#34;file_path\u0026#34;] = \u0026#34;/home/user/DIAMBRA/\u0026#34; trajRecSettings[\u0026#34;ignore_p2\u0026#34;] = 0 In order to use this wrapper, the Flattening and Filtering wrapper must be disabled.\nRecorded Experience Loader DIAMBRA Arena provides a dedicated class to load and use recorded trajectories for training. It needs the settings described in the following table in the form of a kwargs python dictionary. It also supports parallel environment executions, providing interfaces to easily integrate it with third party libraries.\nUse of this functionality can be found in this example.\nKey Type Default Value(s) Value Range Description traj_files_list str - - Contains the list of recorded experience files, specified as absolute paths total_cpus int 1 [1, inf) Specifies the number of parallel environments one wants to run at the same time rank int 0 [0, settings[\u0026ldquo;total_cpus\u0026rdquo;]-1] Assigns a rank number to the environment to identify the instance number when using parallel environments settings[\u0026#34;traj_files_list\u0026#34;] = recorded_trajectories_files settings[\u0026#34;total_cpus\u0026#34;] = 1 settings[\u0026#34;rank\u0026#34;] = 0 Once settings dictionary has been properly setup, the environment is created as shown in the next code block:\nenv = diambra.arena.ImitationLearning(**settings) The interaction with the environment follows the usual methods and conventions, except for the fact that actions in the step method are obviously ignored and the recorded ones are used.\n"},{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/","title":"Hands-on Reinforcement Learning","tags":[],"description":"","content":"Index Learning RL End-To-End DeepRL What is the best path leading a passionate coder to the creation of a trained AI agent capable of effectively playing a video game? It consists in two steps: learning reinforcement learning and applying it.\nLearning RL section below deals with how to get started with RL: it presents resources that cover from the basics up to the most advanced details of the latest, best-performing algorithms.\nThen, in the End-to-end Deep Reinforcement Learning section, some of the most important tech tools are presented together with a step-by-step guide showing how to successfully train a Deep RL agent in our environments.\nLearning Reinforcement Learning Books The first suggested step is to learn the basics of Reinforcement Learning. The best option to do so is Sutton \u0026amp; Barto\u0026rsquo;s book \u0026ldquo;Reinforcement Learning: An Introduction\u0026rdquo;, that can be considered the reference text for the field. An additional option is Packt\u0026rsquo;s \u0026ldquo;The Reinforcement Learning Workshop\u0026rdquo; that covers theory but also a good amount of practice, being very hands-on and complemented by a GitHub repo with worked exercises.\nReinforcement Learning: An Introduction - Sutton \u0026 Barto • Link The Reinforcement Learning Workshop - Palmas et al. • Link Courses / Video-lectures An additional useful resource is represented by courses and/or video-lectures. The three listed in this paragraph, in particular, are extremely valuable. The first one, \u0026ldquo;DeepMind Reinforcement Learning Lectures at University College London\u0026rdquo;, is a collection of lectures dealing with RL in general, as Sutton \u0026amp; Barto\u0026rsquo;s book, providing the solid foundations of the field. The second one, \u0026ldquo;OpenAI Spinning Up with Deep RL\u0026rdquo;, is a very useful website providing a step-by-step primer focused on Deep RL, guiding the reader from the basics to understanding the most important algorithms down to the implementation details. The third one, \u0026ldquo;Berkeley Deep RL Bootcamp\u0026rdquo;, provides video and slides dealing specifically with Deep RL too, and presents a wide overview of the most important, state-of-the-art methods in the field. These are all extremely useful and available for free.\nDeepMind Reinforcement Learning Lectures at University College London • Link OpenAI Spinning Up with Deep RL • Link Berkeley Deep RL Bootcamp • Link Research Publications After having acquired solid fundamentals, as usual in the whole ML domain, one should rely on publications to keep the pace of field advancements. Conference papers, peer-reviewed journal and open access publications are all options to consider.\nA good starting point is to read the reference paper for all state-of-the-art algorithms implemented in the most important RL libraries (see next section), as found for example here (SB3) and here (RAY RLlib).\nOpen Access (Arxiv, etc.) International Conferences (ICML, NeurIPS, ICLR, etc.) Peer-reviewed Journals (ELSEVIER, Springer, etc.) More Finally, additional sources of useful information to better understand this field, and to get inspired by its great potential, are documentaries presenting notable milestones achieved by some of the best AI labs in the world. They showcase reinforcement learning masterpieces, such as AlphaGo/AlphaZero, OpenAI Five and Gran Turismo Sophy, mastering the games of Go, DOTA 2 and Gran Turismo® 7 respectively.\nDeepMind\nAlphaGo The Movie • Link OpenAI\nArtificial Gamer • Link Sony AI\nGran Turismo® Sophy • Link End-to-End Deep Reinforcement Learning Reinforcement Learning Libraries If one wants to rely on already implemented RL algorithms, focusing his efforts on higher level aspects such as policy network architecture, features selection, hyper-parameters tuning, and so on, the best choice is to leverage state-of-the-art RL libraries as the ones shown below. There are many different options, here we list those that, in our experience, are recognized as the leaders in the field, and have been proven to achieve good performances in DIAMBRA Arena environments.\nThere are multiple advantages related to the use of these libraries, to name a few: they provide high quality RL algorithms, efficiently implemented and continuously tested, they allow to natively parallelize environment execution, and in some cases they even support distributed training using multiple GPUs in a single workstation or even in cluster contexts.\nThe next section provides guidance and examples using some of the options listed down here.\nStable Baselines 3 • Link Ray RLlib • Link Intel RL Coach • Link Creating an Agent All the examples presented in these sections (plus additional code) showing how to interface DIAMBRA Arena with the major reinforcement learning libraries, can be found in our open source repository DIAMBRA Agents.\nScripted Agents The classical way to create an agent able to play a game is to hand-code the rules governing its behavior. These rules can vary from very simple heuristics to very complex behavioral trees, but they all have in common the need of an expert coder that knows the game and is able to distill the key elements of it to craft the scripted bot.\nThe following are two examples of (very simple) scripted agents interfaced with our environments, and they are available here: DIAMBRA Agents - Basic.\nNo-Action Agent This agent simply performs the \u0026ldquo;No-Action\u0026rdquo; action at every step. By convention it is the action with index 0, and it needs to be a single value for Discrete action spaces, and a tuple of 0s for MultiDiscrete ones, as shown in the snippet below.\nimport diambra.arena if __name__ == \u0026#34;__main__\u0026#34;: env = diambra.arena.make(\u0026#34;doapp\u0026#34;) observation = env.reset() while True: env.render() action = 0 if env.env_settings[\u0026#34;action_space\u0026#34;][0] == \u0026#34;discrete\u0026#34; else [0, 0] observation, reward, done, info = env.step(action) if done: observation = env.reset() break env.close() Random Agent This agent simply performs a random action at every step. In this case, the sampling method takes care of generating an action that is consistent with the environment action space.\nimport diambra.arena if __name__ == \u0026#34;__main__\u0026#34;: env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) observation = env.reset() while True: actions = env.action_space.sample() observation, reward, done, info = env.step(actions) if done: observation = env.reset() break env.close() More complex scripts can be built in similar ways, for example continuously performing user-defined combos moves, or adding some more complex choice mechanics. But this would still require to decide the tactics in advance, properly translating knowledge into code. A different approach would be to leverage reinforcement learning, so that the agent will improve leveraging its own experience.\nDeepRL Trained Agents An alternative approach to scripted agents is adopting reinforcement learning, and the following sections provide examples on how to do that with the most important libraries in the domain.\nDIAMBRA Arena natively provides interfaces to both Stable Baselines 3 and Ray RLlib, allowing to easily train models with them on our environments. Each library-dedicated page presents some basic and advanced examples.\nStable Baselines 3 Ray RLlib DIAMBRA Arena provides a working interface with Stable Baselines 2 too, but it is deprecated and will be discontinued in the near future.\n"},{"uri":"https://docs.diambra.ai/competitionplatform/","title":"Competition Platform","tags":[],"description":"","content":" Our competition platform allows you to submit your agents and compete with other coders around the globe in epic video games tournaments!\nIt features a public global leaderboard where users are ranked by the best score achieved by their agents in our different environments.\nIt also offers you the possibility to unlock cool achievements depending on the performances of your agent.\nSubmitted agents are evaluated and their episodes are streamed on our Twitch channel.\nWe aimed at making the submission process as smooth as possible, try it now! You find all the details in the sub-pages linked below.\nBasic Agent Script Submission Evaluation How to Submit an Agent Arguments and Commands Test Your Agent Locally "},{"uri":"https://docs.diambra.ai/competitionplatform/testyouragentlocally/","title":"Test Your Agent Locally","tags":[],"description":"","content":"If you want to test your agent locally before submitting it for evaluation on the platform, you can use the specific feature provided by our command line interface. The pattern of the command is the very same used for submission, except that instead of the submit option you will use test.\nIt can be used to make sure the agent behaves as expected, and to debug it in case it fails, without waiting for the online evaluation pipeline.\nIt works with both plain docker images as well as submission manifests with privately hosted files and secret tokens, using respectively, the following commands:\ndiambra agent test \u0026lt;docker image\u0026gt; or\ndiambra agent test --submission.secret token=\u0026lt;my-secret token\u0026gt; --submission.manifest submission.yaml "},{"uri":"https://docs.diambra.ai/projects/","title":"Projects","tags":[],"description":"","content":" This section contains a collection of projects that have been developed using DIAMBRA.\nIf you want to add yours, you can fork the docs repo and submit a Pull Request or get in touch on our Discord server and send us the material.\nProject List RLZ Tournament Game Painter "},{"uri":"https://docs.diambra.ai/","title":"Home","tags":[],"description":"","content":"DIAMBRA Docs DIAMBRA Arena Index Overview Installation Quickstart Competition Platform Docs Stucture Support, Feature Requests \u0026amp; Bugs Reports References Citation Terms of Use Overview DIAMBRA Arena is a software package featuring a collection of high-quality environments for Reinforcement Learning research and experimentation. It provides a standard interface to popular arcade emulated video games, offering a Python API fully compliant with OpenAI Gym format, that makes its adoption smooth and straightforward.\nIt supports all major Operating Systems (Linux, Windows and MacOS) and can be easily installed via Python PIP, as described in the installation section below. It is completely free to use, the user only needs to register on the official website.\nIn addition, its GitHub repository provides a collection of examples covering main use cases of interest that can be run in just a few steps.\nAgent-Environment Interaction Scheme Main Features All environments are episodic Reinforcement Learning tasks, with discrete actions (gamepad buttons) and observations composed by screen pixels plus additional numerical data (RAM values like characters health bars or characters stage side).\nThey all support both single player (1P) as well as two players (2P) mode, making them the perfect resource to explore all the following Reinforcement Learning subfields:\nStandard RL Competitive Multi-Agent Competitive Human-Agent Self-Play Imitation Learning Human-in-the-Loop Available Games Interfaced games have been selected among the most popular fighting retro-games. While sharing the same fundamental mechanics, they provide different challenges, with specific features such as different type and number of characters, how to perform combos, health bars recharging, etc.\nWhenever possible, games are released with all hidden/bonus characters unlocked.\nAdditional details can be found in their dedicated section.\nInstallation Register on our website, it requires just a few clicks and is 100% free\nInstall Docker Desktop (Linux | Windows | MacOS) and make sure you have permissions to run it (see here). On Linux, it\u0026rsquo;s usually enough to run sudo usermod -aG docker $USER, log out and log back in.\nInstall DIAMBRA Command Line Interface: python3 -m pip install diambra\nInstall DIAMBRA Arena: python3 -m pip install diambra-arena\nUsing a virtual environment to isolate your python packages installation is strongly suggested\nQuickstart Download Game ROM(s) and Check Validity Check available games with the following command:\ndiambra arena list-roms Output example:\n[...] Title: Dead Or Alive ++ - GameId: doapp Difficulty levels: Min 1 - Max 4 SHA256 sum: d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Original ROM name: doapp.zip Search keywords: [\u0026#39;DEAD OR ALIVE ++ [JAPAN]\u0026#39;, \u0026#39;dead-or-alive-japan\u0026#39;, \u0026#39;80781\u0026#39;, \u0026#39;wowroms\u0026#39;] Characters list: [\u0026#39;Kasumi\u0026#39;, \u0026#39;Zack\u0026#39;, \u0026#39;Hayabusa\u0026#39;, \u0026#39;Bayman\u0026#39;, \u0026#39;Lei-Fang\u0026#39;, \u0026#39;Raidou\u0026#39;, \u0026#39;Gen-Fu\u0026#39;, \u0026#39;Tina\u0026#39;, \u0026#39;Bass\u0026#39;, \u0026#39;Jann-Lee\u0026#39;, \u0026#39;Ayane\u0026#39;] [...] If you are using Windows 10 \u0026ldquo;N\u0026rdquo; editions and get this error ImportError: DLL load failed while importing cv2, you might need to install the \u0026ldquo;Media Feature Pack\u0026rdquo;.\nSearch ROMs on the web using Search Keywords provided by the game list command reported above. Pay attention, follow game-specific notes reported there, and store all ROMs in the same folder, whose absolute path will be referred in the following as your/roms/local/path.\nSpecific game ROM files are required, check validity of the downloaded ROMs as follows.\nCheck ROM(s) validity running:\ndiambra arena check-roms your/roms/local/path/romFileName.zip The output for a valid ROM file would look like the following:\nCorrect ROM file for Dead Or Alive ++, sha256 = d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Make sure to check out our Terms of Use, and in particular Section 7. By using the software, you accept the in full.\nBase script A Python script to run a complete episode with a random agent requires less than 20 lines:\nimport diambra.arena env = diambra.arena.make(\u0026#34;doapp\u0026#34;) observation = env.reset() while True: env.render() actions = env.action_space.sample() observation, reward, done, info = env.step(actions) if done: observation = env.reset() break env.close() To execute the script run:\ndiambra run -r your/roms/local/path python script.py To avoid specifying ROMs path at every run, you can define the environment variable DIAMBRAROMSPATH=your/roms/local/path, either temporarily in your current shell/prompt session, or permanently in your profile (e.g. on linux in ~/.bashrc).\nCompetition Platform Our competition platform allows you to submit your agents and compete with other coders around the globe in epic video games tournaments!\nIt features a public global leaderboard where users are ranked by the best score achieved by their agents in our different environments.\nIt also offers you the possibility to unlock cool achievements depending on the performances of your agent.\nSubmitted agents are evaluated and their episodes are streamed on our Twitch channel.\nWe aimed at making the submission process as smooth as possible, try it now!\nDocs Structure Getting Started Environments Wrappers Utils Imitation Learning Hands-on Reinforcement Learning Competition Platform Support, Feature Requests \u0026amp; Bugs Reports To receive support, use the dedicated channel in our Discord Server.\nTo request features or report bugs, use the GitHub issue tracker for the specific repository:\nDIAMBRA Arena Issue Tracker DIAMBRA Agents Issue Tracker References Website: https://diambra.ai GitHub: https://github.com/diambra/ Paper: https://arxiv.org/abs/2210.10595 Linkedin: https://www.linkedin.com/company/diambra Discord: https://discord.gg/tFDS2UN5sv Twitch: https://www.twitch.tv/diambra_ai YouTube: https://www.youtube.com/c/diambra_ai Twitter: https://twitter.com/diambra_ai Citation Paper: https://arxiv.org/abs/2210.10595\n@article{Palmas22, author = {{Palmas}, Alessandro}, title = \u0026#34;{DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation}\u0026#34;, journal = {arXiv e-prints}, keywords = {reinforcement learning, transfer learning, multi-agent, games}, year = 2022, month = oct, eid = {arXiv:2210.10595}, pages = {arXiv:2210.10595}, archivePrefix = {arXiv}, eprint = {2210.10595}, primaryClass = {cs.AI} } Terms of Use DIAMBRA Arena software package is subject to our Terms of Use. By using it, you accept them in full.\nDIAMBRA™ is a Trade Mark, © Copyright 2018-2023. All Rights Reserved. "},{"uri":"https://docs.diambra.ai/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://docs.diambra.ai/credits/","title":"Credits","tags":[],"description":"","content":"Contributors Thanks to them for making Open Source Software a better place !\n@matcornic 158 commits @matalo33 48 commits @coliff 19 commits @lierdakil 16 commits @mdavids 10 commits @ozobi 5 commits @Xipas 5 commits @Alan-Cha 4 commits @pdelaby 4 commits @helfper 4 commits @Chris-Greaves 3 commits @mreithub 3 commits @massimeddu 3 commits @LinuxSuRen 3 commits @dptelecom 3 commits @willwade 3 commits @diemol 2 commits @denisvm 2 commits @hucste 2 commits @ImgBotApp 2 commits @jamesbooker 2 commits @jice-lavocat 2 commits @wikijm 2 commits @lfalin 2 commits @JianLoong 2 commits @armsnyder 1 commits @afilini 1 commits @MrAkaki 1 commits @AmirLavasani 1 commits @afs2015 1 commits @arifpedia 1 commits @berryp 1 commits @MrMoio 1 commits @ChrisLasar 1 commits @DCsunset 1 commits @IEvangelist 1 commits @fritzmg 1 commits @bogaertg 1 commits @geoffreybauduin 1 commits @giuliov 1 commits @haitch 1 commits @zeegin 1 commits @RealOrangeOne 1 commits @jared-stehler 1 commits @JohnBlood 1 commits @JohnAllen2tgt 1 commits @kamilchm 1 commits @gwleclerc 1 commits @lloydbenson 1 commits @massimocireddu 1 commits @sykesm 1 commits @nvasudevan 1 commits @nnja 1 commits @owulveryck 1 commits @654wak654 1 commits @PierreAdam 1 commits @qiwenmin 1 commits @ripienaar 1 commits @stou 1 commits @razonyang 1 commits @HontoNoRoger 1 commits @pocc 1 commits @EnigmaCurry 1 commits @taiidani 1 commits @exKAZUu 1 commits @Oddly 1 commits @sandrogauci 1 commits @shelane 1 commits @mbbx6spp 1 commits @swenzel 1 commits @tedyoung 1 commits @Thiht 1 commits @editicalu 1 commits @fossabot 1 commits @kamar535 1 commits @mtbt03 1 commits @ngocbichdao 1 commits @nonumeros 1 commits @pgorod 1 commits @proelbtn 1 commits And a special thanks to @vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support Tooling Netlify - Continuous deployement and hosting of this documentation Hugo "},{"uri":"https://docs.diambra.ai/tags/","title":"Tags","tags":[],"description":"","content":""}]