[{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/sheeprl/","title":"SheepRL","tags":[],"description":"","content":" Index Getting Ready General Environment Settings Native Interface Agent Settings Basic Customising the Configurations Basic Example Configs Folder Define the Environment Define the Agent Define the Experiment Train and Evaluate the Agent Train and Evaluate Scripts PPO Implementation Parallel Environments Advanced Fabric Metric and Logging The source code of all examples described in this section is available in our DIAMBRA Agents repository.\nGetting Ready We highly recommend using virtual environments to isolate your python installs, especially to avoid conflicts in dependencies. In what follows we use Conda but any other tool should work too.\nCreate and activate a new dedicated virtual environment:\nconda create -n diambra-arena-sheeprl python=3.9 conda activate diambra-arena-sheeprl Install DIAMBRA Arena with SheepRL interface:\npip install diambra-arena[sheeprl] This should be enough to prepare your system to execute the following examples. You can refer to the official SheepRL documentation or reach out on our Discord server for specific needs.\nRemember that to train agents, you must have installed the diambra CLI (python3 -m pip install diambra) and set the DIAMBRAROMSPATH environment variable properly.\nAll the examples presented below are available here: DIAMBRA Agents - SheepRL. They have been created following the high level approach found on SheepRL DIAMBRA page, thus allowing to easily extend them and to understand how they interface with the different components.\nThese examples only aim at demonstrating the core functionalities and high-level aspects, they will not generate well-performing agents, even if the training time is extended to cover a large number of training steps. The user will need to build upon them, exploring aspects like policy network architecture, algorithm hyperparameter tuning, observation space tweaking, rewards wrapping, and other similar ones.\nGeneral Environment Settings SheepRL provides a lot of different environments that share a set of parameters. Moreover, SheepRL leverages Hydra for defining hierarchical configurations. Below is reported the general structure of the configuration of an environment and a table describing the arguments.\nid: ??? num_envs: 4 frame_stack: 1 sync_env: False screen_size: 64 action_repeat: 1 grayscale: False clip_rewards: False capture_video: True frame_stack_dilation: 1 max_episode_steps: null reward_as_observation: False wrapper: ??? Argument Type Default Value(s) Description id str - Game environment identifier num_envs int 4 The number of environment to initialize for training frame_stack int 1 The number of frames to stack sync_env bool False Whether to use the gymnasium.vector.SyncVectorEnv (True) or gymnasium.vector.AsyncVectorEnv (False) for handling vectorized environments screen_size int | Tuple[int, int] 64 Screen size of the frames action_repeat int 64 How many times repeat the same action grayscale bool False Whether to use grayscale frames clip_rewards bool False Whether or not to clip rewards using a tanh capture_video bool True Whether or not to capture the video of the episodes during training frame_stack_dilation int 1 The number of frames to be skipped between frames in the frame_stack max_episode_steps int | None null The maximum number of steps in a single episode reward_as_observation bool False Whether or not to add the reward to the observations wrapper Dict[str, Any] - Environment-related arguments (see here) If you have never used Hydra, before continuing, it is strongly recommended to check the Hydra official documentation and the SheepRL-related section.\nNative interface DIAMBRA Arena native interface with SheepRL covers a wide range of use cases, automating the handling of vectorized environments and monitoring wrappers. In the majority of cases, it will be sufficient for users to directly import and use it, with no need for additional customization. Below is reported its interface and a table describing its arguments.\nclass DiambraWrapper(gym.Wrapper): def __init__( self, id: str, action_space: str = \u0026#34;diambra.arena.SpaceTypes.DISCRETE\u0026#34;, screen_size: Union[int, Tuple[int, int]] = 64, grayscale: bool = False, repeat_action: int = 1, rank: int = 0, diambra_settings: Dict[str, Any] = {}, diambra_wrappers: Dict[str, Any] = {}, render_mode: str = \u0026#34;rgb_array\u0026#34;, log_level: int = 0, increase_performance: bool = True, ): Argument Type Default Value(s) Description id str - Game environment identifier action_space str DISCRETE* Which action space to use: one between DISCRETE* and MULTI_DISCRETE* screen_size int | Tuple[int, int] 64 Screen size of the frames grayscale bool False Whether to use grayscale frames rank int 0 Rank of the environment diambra_settings Dict[str, Any] {} The settings of the environment. See here to check which settings you can specify. diambra_wrappers Dict[str, Any] {} The wrappers to apply to the environment. See here to check which wrappers you can specify. render_mode str \u0026quot;rgb_array\u0026quot; Rendering mode log_level int 0 Log level increase_performance bool True Whether to modify frames on the engine side (True) or use the wrapper (False) *: DISCRETE is a placeholder for diambra.arena.SpaceTypes.DISCRETE, whereas MULTI_DISCRETE is a placeholder for diambra.arena.SpaceTypes.MULTI_DISCRETE. You must enter the full string.\nFor the interface low-level details, users can review the correspondent source code here.\nAgent Settings SheepRL provides several SOTA algorithms, both model-free and model-based. Here you can find the default configurations for these agent. Of course, one can change algorithm-related hyper-parameters for customizing his/her experiments.\nBasic As anticipated before, SheepRL provides several default configurations for all its components, which are available and can be composed to set up an experiment. Otherwise, you can customize the ones you want: the two main ones to be defined for experiments are the agent and the environment.\nRegarding the environment, there are some constraints that must be respected, for example, the dictionary observation spaces cannot be nested. For this reason, the DIAMBRA flattening wrapper is always used. For more information about the constraints of the SheepRL library, check here.\nInstead, regarding the agent, the only two constraints that are present concern the observation and action spaces that agents support. You can read the supported observation and action spaces in Table 1 and Table 2 of the README in the SheepRL GitHub repository, respectively.\nCustomising the Configurations The default configurations are available here. If you want to define your custom experiments, you just need to follow a few steps:\nYou need to create a folder (with the same structure as the SheepRL configs folder) where to place your custom configurations. You need to define the SHEEPRL_SEARCH_PATH environment variable in the .env file as follows: SHEEPRL_SEARCH_PATH=file://relative/path/to/custom/configs/folder;pkg://sheeprl.configs. You need to define the custom configurations, being careful that the filename is different from the default ones. If this is not respected, your file will overwrite the default configurations. Basic Example This example demonstrates how to:\nLeverage SheepRL to define the environment for training. Define a PPO Agent to be trained. Define custom configurations for your experiment. Train the agent. Run the trained agent in the environment for one episode. SheepRL natively supports dictionary observation spaces, the only thing you need to define is the keys of the observations you want to process. For more information about observations selection, check here.\nConfigs Folder First, it is necessary to create a folder for the configuration files. We create the configs folder under the ./sheeprl/ folder in the DIAMBRA Arena GitHub repository. Then we added the .env file in ./sheeprl/ folder, in which we need to define the SHEEPRL_SEARCH_PATH environment variable as follows:\nSHEEPRL_SEARCH_PATH=file://configs;pkg://sheeprl.configs Define the Environment Now, in the ./sheeprl/configs folder we create the env folder in which the custom_env.yaml will be placed. Below is reported a possible configuration of the environment.\ndefaults: - default - _self_ # Override from `default` config # `default` config contains the arguments shared # among all the environments in SheepRL id: doapp frame_stack: 1 sync_env: True action_repeat: 1 num_envs: 1 screen_size: 64 grayscale: False clip_rewards: False capture_video: True frame_stack_dilation: 1 max_episode_steps: null reward_as_observation: False # DOAPP-related arguments wrapper: # class to be instantiated _target_: sheeprl.envs.diambra.DiambraWrapper id: ${env.id} action_space: diambra.arena.SpaceTypes.DISCRETE # or `diambra.arena.SpaceTypes.MULTI_DISCRETE` screen_size: ${env.screen_size} grayscale: ${env.grayscale} repeat_action: ${env.action_repeat} rank: null log_level: 0 increase_performance: True diambra_settings: role: diambra.arena.Roles.P1 # or `diambra.arena.Roles.P1` or `null` step_ratio: 6 difficulty: 4 continue_game: 0.0 show_final: False outfits: 2 splash_screen: False diambra_wrappers: stack_actions: 1 no_op_max: 0 no_attack_buttons_combinations: False add_last_action: True scale: False exclude_image_scaling: False process_discrete_binary: False role_relative: True Define the Agent As for the environment, we need to create a dedicated folder to place the custom configurations of the agents: we create the algo folder in the ./sheeprl/configs folder and we place the custom_ppo_agent.yaml file. Under the default keyword, it is possible to retrieve the configurations specified in another file, in our case, since we are defining the agent, we can take the configuration from the algorithm config folder in SheepRL, in which several SOTA agents are defined.\nWhen defining an agent it is mandatory to define the name of the algorithm (it must be equal to the filename of the file in which the algorithm is defined). The value of these parameters defines which algorithm will be used for training. If you inherit the default configurations of a specific algorithm, then you do not need to define it, since it is already defined in the default configs of that algorithm.\nBelow is reported a configuration file for a PPO agent. defaults: # Take default configurations of PPO - ppo # define Adam optimizer under the `optimizer` key # from the sheeprl/configs/optim folder - override /optim@optimizer: adam - _self_ # Override default ppo arguments # `name` is a mandatory attribute, it must be equal to the filename # of the file in which the algorithm is defined. # If you inherit the default configurations of a specific algoritm, # then you do not need to define it, since it is already defined in the default configs name: ppo update_epochs: 1 normalize_advantages: True rollout_steps: 32 dense_units: 16 mlp_layers: 1 dense_act: torch.nn.Tanh max_grad_norm: 1.0 # Encoder encoder: cnn_features_dim: 128 mlp_features_dim: 32 dense_units: ${algo.dense_units} mlp_layers: ${algo.mlp_layers} dense_act: ${algo.dense_act} layer_norm: ${algo.layer_norm} # Actor actor: dense_units: ${algo.dense_units} mlp_layers: ${algo.mlp_layers} dense_act: ${algo.dense_act} layer_norm: ${algo.layer_norm} # Critic critic: dense_units: ${algo.dense_units} mlp_layers: ${algo.mlp_layers} dense_act: ${algo.dense_act} layer_norm: ${algo.layer_norm} # Single optimizer for both actor and critic optimizer: lr: 5e-3 eps: 1e-6 Define the Experiment The last thing to do is to define the experiment. You just need to define a custom_exp.yaml file in the ./sheeprl/configs/exp folder and assemble the environment, the agent, and the other components of the SheepRL framework. In particular, there are four parameters that must be defined:\nalgo.total_steps: the total number of policy steps to compute during training (for more information, check here). buffer.size: the dimension of the replay buffer. algo.cnn_keys: the keys of frames in observations that must be encoded (and eventually reconstructed by the decoder). algo.mlp_keys: the keys of vectors in observations that must be encoded (and eventually reconstructed by the decoder). Both algo.cnn_keys and algo.mlp_keys must be non-empty lists. Moreover, the user specified keys must be a subset of the environment observation keys.\nBelow is an example of an experiment config file. # @package _global_ defaults: # Selects the algorithm and the environment - override /algo: custom_ppo_agent - override /env: custom_env - _self_ # Buffer buffer: share_data: False size: ${algo.rollout_steps} checkpoint: save_last: True # Experiment algo: total_steps: 1024 per_rank_batch_size: 16 cnn_keys: encoder: [frame] mlp_keys: encoder: - own_character - own_health - own_side - own_wins - opp_character - opp_health - opp_side - opp_wins - stage - timer - action When defining the configurations of the experiment you can specify how frequently save checkpoints of the model, and if you want to save the final agent. For more information, check here.\nTrain and Evaluate the Agent To run the experiment you just need to go into the ./sheeprl folder and run the following command:\ndiambra run -s=2 python train.py exp=custom_exp You have to instantiate 2 docker containers because sheeprl automatically performs a test of the agent after training.\nAfter training, you can decide to evaluate the agent as many times as you want. You can specify only a few parameters for evaluating your agent:\nThe checkpoint of the agent that you want to evaluate (checkpoint_path, mandatory). The type of device on which you want to run the evaluation (fabric.device, default to cpu). Whether or not to capture the video of the evaluation (env.capture_video, default to True). The reason why only these three parameters need to be specified is to avoid inconsistencies, e.g. the checkpoint of one agent and the configurations of the evaluation refer to another one, or the model in the checkpoint has different dimensions from the model specified in the configurations. This implies, however, that the evaluation script expects a certain directory structure. For this reason, the structure of the log directory should not be changed: all of it can be moved, but not the checkpoint individually, otherwise the script cannot automatically retrieve the environment and agent configurations.\n# @package _global_ # specify here default training configuration defaults: - _self_ - override hydra/hydra_logging: disabled - override hydra/job_logging: disabled hydra: output_subdir: null run: dir: . fabric: accelerator: cpu env: capture_video: True checkpoint_path: ??? To evaluate the agent you just need to run the following command:\ndiambra run python evaluate.py checkpoint_path=/path/to/checkpoint.ckpt If you want to specify the device to use, for instance cuda, you have to run the following command:\ndiambra run python evaluate.py checkpoint_path=/path/to/checkpoint.ckpt fabric.device=cuda If you want to specify whether or not to capture the video, you have to run the following command:\ndiambra run python evaluate.py checkpoint_path=/path/to/checkpoint.ckpt env.capture_video=True Train and Evaluate Scripts In this section, we show the two scripts for training and evaluating agents. With regard to training, first the environment selected by the user is checked, if it is not one of diambra, then an exception is raised. Next, the run() function of SheepRL is called, which will initialize all components and start the training.\nAs far as evaluation is concerned, simply the configurations are passed directly to the evaluate() function of sheeprl. There is no need to check the environment as it has already been checked before training.\nThe train.py script: # Diambra Agents import hydra from diambra.arena.sheeprl import CONFIGS_PATH from omegaconf import DictConfig from sheeprl.cli import run def check_configs(cfg: DictConfig): if \u0026#34;diambra\u0026#34; not in cfg.env.wrapper._target_: raise ValueError( f\u0026#34;You must choose a DIAMBRA environment. \u0026#34; f\u0026#34;Got \u0026#39;{cfg.env.id}\u0026#39; provided by \u0026#39;{cfg.env.wrapper._target_.split(\u0026#39;.\u0026#39;)[-2]}\u0026#39;.\u0026#34; ) @hydra.main(version_base=\u0026#34;1.3\u0026#34;, config_path=CONFIGS_PATH, config_name=\u0026#34;config\u0026#34;) def train(cfg: DictConfig): check_configs(cfg) run(cfg) if __name__ == \u0026#34;__main__\u0026#34;: train() The evaluate.py script: # Diambra Agents import hydra from diambra.arena.sheeprl import CONFIGS_PATH from omegaconf import DictConfig from sheeprl.cli import evaluation @hydra.main(version_base=\u0026#34;1.3\u0026#34;, config_path=CONFIGS_PATH, config_name=\u0026#34;eval_config\u0026#34;) def run(cfg: DictConfig): evaluation(cfg) if __name__ == \u0026#34;__main__\u0026#34;: run() PPO Implementation In this paragraph, we quote the code of our ppo implementation (the ppo.py file in the SheepRL PPO folder), just to give more context on how SheepRL works. In the main() function, all the components needed for training are instantiated (i.e., the agent, the environments, the buffer, the logger, and so on). Then, the environment interaction is performed, and after collecting the rollout steps, the train function is called.\nThe train() function is responsible for sharing the data between processes, if more processes are launched and the buffer.share_data is set to True. Then, for each batch, the losses are computed and the agent is updated.\nfrom __future__ import annotations import copy import os import warnings from typing import Any, Dict, Union import gymnasium as gym import hydra import numpy as np import torch from lightning.fabric import Fabric from lightning.fabric.wrappers import _FabricModule from torch import nn from torch.utils.data import BatchSampler, DistributedSampler, RandomSampler from torchmetrics import SumMetric from sheeprl.algos.ppo.agent import build_agent from sheeprl.algos.ppo.loss import entropy_loss, policy_loss, value_loss from sheeprl.algos.ppo.utils import normalize_obs, test from sheeprl.data.buffers import ReplayBuffer from sheeprl.utils.env import make_env from sheeprl.utils.logger import get_log_dir, get_logger from sheeprl.utils.metric import MetricAggregator from sheeprl.utils.registry import register_algorithm from sheeprl.utils.timer import timer from sheeprl.utils.utils import gae, normalize_tensor, polynomial_decay, save_configs def train( fabric: Fabric, agent: Union[nn.Module, _FabricModule], optimizer: torch.optim.Optimizer, data: Dict[str, torch.Tensor], aggregator: MetricAggregator | None, cfg: Dict[str, Any], ): \u0026#34;\u0026#34;\u0026#34;Train the agent on the data collected from the environment.\u0026#34;\u0026#34;\u0026#34; indexes = list(range(next(iter(data.values())).shape[0])) if cfg.buffer.share_data: sampler = DistributedSampler( indexes, num_replicas=fabric.world_size, rank=fabric.global_rank, shuffle=True, seed=cfg.seed, ) else: sampler = RandomSampler(indexes) sampler = BatchSampler(sampler, batch_size=cfg.algo.per_rank_batch_size, drop_last=False) for epoch in range(cfg.algo.update_epochs): if cfg.buffer.share_data: sampler.sampler.set_epoch(epoch) for batch_idxes in sampler: batch = {k: v[batch_idxes] for k, v in data.items()} normalized_obs = normalize_obs( batch, cfg.algo.cnn_keys.encoder, cfg.algo.mlp_keys.encoder + cfg.algo.cnn_keys.encoder ) _, logprobs, entropy, new_values = agent( normalized_obs, torch.split(batch[\u0026#34;actions\u0026#34;], agent.actions_dim, dim=-1) ) if cfg.algo.normalize_advantages: batch[\u0026#34;advantages\u0026#34;] = normalize_tensor(batch[\u0026#34;advantages\u0026#34;]) # Policy loss pg_loss = policy_loss( logprobs, batch[\u0026#34;logprobs\u0026#34;], batch[\u0026#34;advantages\u0026#34;], cfg.algo.clip_coef, cfg.algo.loss_reduction, ) # Value loss v_loss = value_loss( new_values, batch[\u0026#34;values\u0026#34;], batch[\u0026#34;returns\u0026#34;], cfg.algo.clip_coef, cfg.algo.clip_vloss, cfg.algo.loss_reduction, ) # Entropy loss ent_loss = entropy_loss(entropy, cfg.algo.loss_reduction) # Equation (9) in the paper loss = pg_loss + cfg.algo.vf_coef * v_loss + cfg.algo.ent_coef * ent_loss optimizer.zero_grad(set_to_none=True) fabric.backward(loss) if cfg.algo.max_grad_norm \u0026gt; 0.0: fabric.clip_gradients(agent, optimizer, max_norm=cfg.algo.max_grad_norm) optimizer.step() # Update metrics if aggregator and not aggregator.disabled: aggregator.update(\u0026#34;Loss/policy_loss\u0026#34;, pg_loss.detach()) aggregator.update(\u0026#34;Loss/value_loss\u0026#34;, v_loss.detach()) aggregator.update(\u0026#34;Loss/entropy_loss\u0026#34;, ent_loss.detach()) @register_algorithm() def main(fabric: Fabric, cfg: Dict[str, Any]): if \u0026#34;minedojo\u0026#34; in cfg.env.wrapper._target_.lower(): raise ValueError( \u0026#34;MineDojo is not currently supported by PPO agent, since it does not take \u0026#34; \u0026#34;into consideration the action masks provided by the environment, but needed \u0026#34; \u0026#34;in order to play correctly the game. \u0026#34; \u0026#34;As an alternative you can use one of the Dreamers\u0026#39; agents.\u0026#34; ) initial_ent_coef = copy.deepcopy(cfg.algo.ent_coef) initial_clip_coef = copy.deepcopy(cfg.algo.clip_coef) # Initialize Fabric rank = fabric.global_rank world_size = fabric.world_size device = fabric.device fabric.seed_everything(cfg.seed) torch.backends.cudnn.deterministic = cfg.torch_deterministic # Resume from checkpoint if cfg.checkpoint.resume_from: state = fabric.load(cfg.checkpoint.resume_from) # Create Logger. This will create the logger only on the # rank-0 process logger = get_logger(fabric, cfg) if logger and fabric.is_global_zero: fabric._loggers = [logger] fabric.logger.log_hyperparams(cfg) log_dir = get_log_dir(fabric, cfg.root_dir, cfg.run_name) # Environment setup vectorized_env = gym.vector.SyncVectorEnv if cfg.env.sync_env else gym.vector.AsyncVectorEnv envs = vectorized_env( [ make_env( cfg, cfg.seed + rank * cfg.env.num_envs + i, rank * cfg.env.num_envs, log_dir if rank == 0 else None, \u0026#34;train\u0026#34;, vector_env_idx=i, ) for i in range(cfg.env.num_envs) ] ) observation_space = envs.single_observation_space if not isinstance(observation_space, gym.spaces.Dict): raise RuntimeError(f\u0026#34;Unexpected observation type, should be of type Dict, got: {observation_space}\u0026#34;) if cfg.algo.cnn_keys.encoder + cfg.algo.mlp_keys.encoder == []: raise RuntimeError( \u0026#34;You should specify at least one CNN keys or MLP keys from the cli: \u0026#34; \u0026#34;`cnn_keys.encoder=[rgb]` or `mlp_keys.encoder=[state]`\u0026#34; ) if cfg.metric.log_level \u0026gt; 0: fabric.print(\u0026#34;Encoder CNN keys:\u0026#34;, cfg.algo.cnn_keys.encoder) fabric.print(\u0026#34;Encoder MLP keys:\u0026#34;, cfg.algo.mlp_keys.encoder) obs_keys = cfg.algo.cnn_keys.encoder + cfg.algo.mlp_keys.encoder is_continuous = isinstance(envs.single_action_space, gym.spaces.Box) is_multidiscrete = isinstance(envs.single_action_space, gym.spaces.MultiDiscrete) actions_dim = tuple( envs.single_action_space.shape if is_continuous else (envs.single_action_space.nvec.tolist() if is_multidiscrete else [envs.single_action_space.n]) ) # Create the actor and critic models agent = build_agent( fabric, actions_dim, is_continuous, cfg, observation_space, state[\u0026#34;agent\u0026#34;] if cfg.checkpoint.resume_from else None, ) # Define the optimizer optimizer = hydra.utils.instantiate(cfg.algo.optimizer, params=agent.parameters(), _convert_=\u0026#34;all\u0026#34;) if fabric.is_global_zero: save_configs(cfg, log_dir) # Load the state from the checkpoint if cfg.checkpoint.resume_from: optimizer.load_state_dict(state[\u0026#34;optimizer\u0026#34;]) # Setup agent and optimizer with Fabric optimizer = fabric.setup_optimizers(optimizer) # Create a metric aggregator to log the metrics aggregator = None if not MetricAggregator.disabled: aggregator: MetricAggregator = hydra.utils.instantiate(cfg.metric.aggregator, _convert_=\u0026#34;all\u0026#34;).to(device) # Local data if cfg.buffer.size \u0026lt; cfg.algo.rollout_steps: raise ValueError( f\u0026#34;The size of the buffer ({cfg.buffer.size}) cannot be lower \u0026#34; f\u0026#34;than the rollout steps ({cfg.algo.rollout_steps})\u0026#34; ) rb = ReplayBuffer( cfg.buffer.size, cfg.env.num_envs, memmap=cfg.buffer.memmap, memmap_dir=os.path.join(log_dir, \u0026#34;memmap_buffer\u0026#34;, f\u0026#34;rank_{fabric.global_rank}\u0026#34;), obs_keys=obs_keys, ) # Global variables last_train = 0 train_step = 0 start_step = ( # + 1 because the checkpoint is at the end of the update step # (when resuming from a checkpoint, the update at the checkpoint # is ended and you have to start with the next one) (state[\u0026#34;update\u0026#34;] // fabric.world_size) + 1 if cfg.checkpoint.resume_from else 1 ) policy_step = state[\u0026#34;update\u0026#34;] * cfg.env.num_envs * cfg.algo.rollout_steps if cfg.checkpoint.resume_from else 0 last_log = state[\u0026#34;last_log\u0026#34;] if cfg.checkpoint.resume_from else 0 last_checkpoint = state[\u0026#34;last_checkpoint\u0026#34;] if cfg.checkpoint.resume_from else 0 policy_steps_per_update = int(cfg.env.num_envs * cfg.algo.rollout_steps * world_size) num_updates = cfg.algo.total_steps // policy_steps_per_update if not cfg.dry_run else 1 if cfg.checkpoint.resume_from: cfg.algo.per_rank_batch_size = state[\u0026#34;batch_size\u0026#34;] // fabric.world_size # Warning for log and checkpoint every if cfg.metric.log_level \u0026gt; 0 and cfg.metric.log_every % policy_steps_per_update != 0: warnings.warn( f\u0026#34;The metric.log_every parameter ({cfg.metric.log_every}) is not a multiple of the \u0026#34; f\u0026#34;policy_steps_per_update value ({policy_steps_per_update}), so \u0026#34; \u0026#34;the metrics will be logged at the nearest greater multiple of the \u0026#34; \u0026#34;policy_steps_per_update value.\u0026#34; ) if cfg.checkpoint.every % policy_steps_per_update != 0: warnings.warn( f\u0026#34;The checkpoint.every parameter ({cfg.checkpoint.every}) is not a multiple of the \u0026#34; f\u0026#34;policy_steps_per_update value ({policy_steps_per_update}), so \u0026#34; \u0026#34;the checkpoint will be saved at the nearest greater multiple of the \u0026#34; \u0026#34;policy_steps_per_update value.\u0026#34; ) # Linear learning rate scheduler if cfg.algo.anneal_lr: from torch.optim.lr_scheduler import PolynomialLR scheduler = PolynomialLR(optimizer=optimizer, total_iters=num_updates, power=1.0) if cfg.checkpoint.resume_from: scheduler.load_state_dict(state[\u0026#34;scheduler\u0026#34;]) # Get the first environment observation and start the optimization step_data = {} next_obs = envs.reset(seed=cfg.seed)[0] # [N_envs, N_obs] for k in obs_keys: if k in cfg.algo.cnn_keys.encoder: next_obs[k] = next_obs[k].reshape(cfg.env.num_envs, -1, *next_obs[k].shape[-2:]) step_data[k] = next_obs[k][np.newaxis] for update in range(start_step, num_updates + 1): for _ in range(0, cfg.algo.rollout_steps): policy_step += cfg.env.num_envs * world_size # Measure environment interaction time: this considers both the model forward # to get the action given the observation and the time taken into the environment with timer(\u0026#34;Time/env_interaction_time\u0026#34;, SumMetric(sync_on_compute=False)): with torch.no_grad(): # Sample an action given the observation received by the environment normalized_obs = normalize_obs(next_obs, cfg.algo.cnn_keys.encoder, obs_keys) torch_obs = { k: torch.as_tensor(normalized_obs[k], dtype=torch.float32, device=device) for k in obs_keys } actions, logprobs, _, values = agent.module(torch_obs) if is_continuous: real_actions = torch.cat(actions, -1).cpu().numpy() else: real_actions = torch.cat([act.argmax(dim=-1) for act in actions], dim=-1).cpu().numpy() actions = torch.cat(actions, -1).cpu().numpy() # Single environment step obs, rewards, dones, truncated, info = envs.step(real_actions.reshape(envs.action_space.shape)) truncated_envs = np.nonzero(truncated)[0] if len(truncated_envs) \u0026gt; 0: real_next_obs = { k: torch.empty( len(truncated_envs), *observation_space[k].shape, dtype=torch.float32, device=device, ) for k in obs_keys } for i, truncated_env in enumerate(truncated_envs): for k, v in info[\u0026#34;final_observation\u0026#34;][truncated_env].items(): torch_v = torch.as_tensor(v, dtype=torch.float32, device=device) if k in cfg.algo.cnn_keys.encoder: torch_v = torch_v.view(cfg.env.num_envs, -1, *v.shape[-2:]) torch_v = torch_v / 255.0 - 0.5 real_next_obs[k][i] = torch_v with torch.no_grad(): vals = agent.module.get_value(real_next_obs).cpu().numpy() rewards[truncated_envs] += vals.reshape(rewards[truncated_envs].shape) dones = np.logical_or(dones, truncated).reshape(cfg.env.num_envs, -1).astype(np.uint8) rewards = rewards.reshape(cfg.env.num_envs, -1) # Update the step data step_data[\u0026#34;dones\u0026#34;] = dones[np.newaxis] step_data[\u0026#34;values\u0026#34;] = values.cpu().numpy()[np.newaxis] step_data[\u0026#34;actions\u0026#34;] = actions[np.newaxis] step_data[\u0026#34;logprobs\u0026#34;] = logprobs.cpu().numpy()[np.newaxis] step_data[\u0026#34;rewards\u0026#34;] = rewards[np.newaxis] if cfg.buffer.memmap: step_data[\u0026#34;returns\u0026#34;] = np.zeros_like(rewards, shape=(1, *rewards.shape)) step_data[\u0026#34;advantages\u0026#34;] = np.zeros_like(rewards, shape=(1, *rewards.shape)) # Append data to buffer rb.add(step_data, validate_args=cfg.buffer.validate_args) # Update the observation and dones next_obs = {} for k in obs_keys: _obs = obs[k] if k in cfg.algo.cnn_keys.encoder: _obs = _obs.reshape(cfg.env.num_envs, -1, *_obs.shape[-2:]) step_data[k] = _obs[np.newaxis] next_obs[k] = _obs if cfg.metric.log_level \u0026gt; 0 and \u0026#34;final_info\u0026#34; in info: for i, agent_ep_info in enumerate(info[\u0026#34;final_info\u0026#34;]): if agent_ep_info is not None: ep_rew = agent_ep_info[\u0026#34;episode\u0026#34;][\u0026#34;r\u0026#34;] ep_len = agent_ep_info[\u0026#34;episode\u0026#34;][\u0026#34;l\u0026#34;] if aggregator and \u0026#34;Rewards/rew_avg\u0026#34; in aggregator: aggregator.update(\u0026#34;Rewards/rew_avg\u0026#34;, ep_rew) if aggregator and \u0026#34;Game/ep_len_avg\u0026#34; in aggregator: aggregator.update(\u0026#34;Game/ep_len_avg\u0026#34;, ep_len) fabric.print(f\u0026#34;Rank-0: policy_step={policy_step}, reward_env_{i}={ep_rew[-1]}\u0026#34;) # Transform the data into PyTorch Tensors local_data = rb.to_tensor(dtype=None, device=device, from_numpy=cfg.buffer.from_numpy) # Estimate returns with GAE (https://arxiv.org/abs/1506.02438) with torch.no_grad(): normalized_obs = normalize_obs(next_obs, cfg.algo.cnn_keys.encoder, obs_keys) torch_obs = {k: torch.as_tensor(normalized_obs[k], dtype=torch.float32, device=device) for k in obs_keys} next_values = agent.module.get_value(torch_obs) returns, advantages = gae( local_data[\u0026#34;rewards\u0026#34;].to(torch.float64), local_data[\u0026#34;values\u0026#34;], local_data[\u0026#34;dones\u0026#34;], next_values, cfg.algo.rollout_steps, cfg.algo.gamma, cfg.algo.gae_lambda, ) # Add returns and advantages to the buffer local_data[\u0026#34;returns\u0026#34;] = returns.float() local_data[\u0026#34;advantages\u0026#34;] = advantages.float() if cfg.buffer.share_data and fabric.world_size \u0026gt; 1: # Gather all the tensors from all the world and reshape them gathered_data: Dict[str, torch.Tensor] = fabric.all_gather(local_data) # Flatten the first three dimensions: [World_Size, Buffer_Size, Num_Envs] gathered_data = {k: v.flatten(start_dim=0, end_dim=2).float() for k, v in gathered_data.items()} else: # Flatten the first two dimensions: [Buffer_Size, Num_Envs] gathered_data = {k: v.flatten(start_dim=0, end_dim=1).float() for k, v in local_data.items()} with timer(\u0026#34;Time/train_time\u0026#34;, SumMetric(sync_on_compute=cfg.metric.sync_on_compute)): train(fabric, agent, optimizer, gathered_data, aggregator, cfg) train_step += world_size if cfg.metric.log_level \u0026gt; 0: # Log lr and coefficients if cfg.algo.anneal_lr: fabric.log(\u0026#34;Info/learning_rate\u0026#34;, scheduler.get_last_lr()[0], policy_step) else: fabric.log(\u0026#34;Info/learning_rate\u0026#34;, cfg.algo.optimizer.lr, policy_step) fabric.log(\u0026#34;Info/clip_coef\u0026#34;, cfg.algo.clip_coef, policy_step) fabric.log(\u0026#34;Info/ent_coef\u0026#34;, cfg.algo.ent_coef, policy_step) # Log metrics if cfg.metric.log_level \u0026gt; 0 and (policy_step - last_log \u0026gt;= cfg.metric.log_every or update == num_updates): # Sync distributed metrics if aggregator and not aggregator.disabled: metrics_dict = aggregator.compute() fabric.log_dict(metrics_dict, policy_step) aggregator.reset() # Sync distributed timers if not timer.disabled: timer_metrics = timer.compute() if \u0026#34;Time/train_time\u0026#34; in timer_metrics: fabric.log( \u0026#34;Time/sps_train\u0026#34;, (train_step - last_train) / timer_metrics[\u0026#34;Time/train_time\u0026#34;], policy_step, ) if \u0026#34;Time/env_interaction_time\u0026#34; in timer_metrics: fabric.log( \u0026#34;Time/sps_env_interaction\u0026#34;, ((policy_step - last_log) / world_size * cfg.env.action_repeat) / timer_metrics[\u0026#34;Time/env_interaction_time\u0026#34;], policy_step, ) timer.reset() # Reset counters last_log = policy_step last_train = train_step # Update lr and coefficients if cfg.algo.anneal_lr: scheduler.step() if cfg.algo.anneal_clip_coef: cfg.algo.clip_coef = polynomial_decay( update, initial=initial_clip_coef, final=0.0, max_decay_steps=num_updates, power=1.0 ) if cfg.algo.anneal_ent_coef: cfg.algo.ent_coef = polynomial_decay( update, initial=initial_ent_coef, final=0.0, max_decay_steps=num_updates, power=1.0 ) # Checkpoint model if (cfg.checkpoint.every \u0026gt; 0 and policy_step - last_checkpoint \u0026gt;= cfg.checkpoint.every) or ( update == num_updates and cfg.checkpoint.save_last ): last_checkpoint = policy_step state = { \u0026#34;agent\u0026#34;: agent.state_dict(), \u0026#34;optimizer\u0026#34;: optimizer.state_dict(), \u0026#34;scheduler\u0026#34;: scheduler.state_dict() if cfg.algo.anneal_lr else None, \u0026#34;update\u0026#34;: update * world_size, \u0026#34;batch_size\u0026#34;: cfg.algo.per_rank_batch_size * fabric.world_size, \u0026#34;last_log\u0026#34;: last_log, \u0026#34;last_checkpoint\u0026#34;: last_checkpoint, } ckpt_path = os.path.join(log_dir, f\u0026#34;checkpoint/ckpt_{policy_step}_{fabric.global_rank}.ckpt\u0026#34;) fabric.call(\u0026#34;on_checkpoint_coupled\u0026#34;, fabric=fabric, ckpt_path=ckpt_path, state=state) envs.close() if fabric.is_global_zero: test(agent.module, fabric, cfg, log_dir) if not cfg.model_manager.disabled and fabric.is_global_zero: from sheeprl.algos.ppo.utils import log_models from sheeprl.utils.mlflow import register_model models_to_log = {\u0026#34;agent\u0026#34;: agent} register_model(fabric, log_models, cfg, models_to_log) Parallel Environments In addition to what is seen in previous examples, this one demonstrates how to run training using parallel environments. In this example, the same PPO algorithm is used as before. To train the agent with multiple parallel environments, you need to define properly a few environment parameters and then run the script instantiating the correct number of docker containers.\nYou can create a custom_parallel_env.yaml config file that inherits the configurations from the custom_env.yaml file: defaults: # Inherit evironment configurations from custom_env.yaml - custom_env - _self_ # Override parameters sync_env: False # True if you want to use the gymnasium.vector.SyncVectorEnv num_envs: 4 If you set the env.sync_env to False, then you must instantiate one more docker container because the gymnasium.vector.AsyncVectorEnv instantiates a dummy env when defined.\nThen, you have to create a new file for the experiment (custom_parallel_env_exp.yaml), this file inherits the configurations of the custom_exp file and overrides the environment with the newly defined configurations (custom_parallel_env): # @package _global_ defaults: # Inherit configs from custom_exp - custom_exp # Override the environment configurations - override /env: custom_parallel_env - _self_ How to run it:\n# s=6 comes from: 4 for the envs, 1 for testing, 1 for `gymnasium.vector.AsyncVectorEnv` diambra run -s=6 python train.py exp=custom_parallel_env_exp Advanced Fabric SheepRL allows training to be distributed thanks to Lightning Fabric.\nThe default Fabric configuration is the following: _target_: lightning.fabric.Fabric devices: 1 num_nodes: 1 strategy: \u0026#34;auto\u0026#34; accelerator: \u0026#34;cpu\u0026#34; precision: \u0026#34;32-true\u0026#34; callbacks: - _target_: sheeprl.utils.callback.CheckpointCallback keep_last: \u0026#34;${checkpoint.keep_last}\u0026#34; The sheeprl.utils.callback.CheckpointCallback is used for saving the checkpoint during training and for saving the trained agent.\nTo modify the Fabric configs, you can add a fabric field in the experiment file, as shown below. In this case, we selected 2 devices, the accelerator is \u0026quot;cuda\u0026quot; and the training is performed in 16 bits. As before, it inherits the configurations from the custom_exp and then sets the Fabric parameters. # @package _global_ defaults: # Inherit configs from custom_exp - custom_exp - _self_ # Set Fabric parameters fabric: devices: 2 accelerator: cuda precision: bf16 How to run it:\n# Remember to set properly the number of containers to create # - Each process has 1 environment # - There are 2 processes # - Only the zero-rank process will perform the evaluation after the training diambra run -s=3 python train.py exp=custom_fabric_exp To run the fabric experiment, make sure you have a cuda GPU in your device, otherwise, change the device from cuda to cpu (or to another device).\nMetric and Logging Finally, SheepRL allows you to visualize and monitor training using Tensorboard.\nWe strongly recommend to read the SheepRL logging documentation to know about how to enable/disable logging.\nBelow is reported the default logging configuration and a table describing the arguments.\ndefaults: - _self_ - /logger@logger: tensorboard log_every: 5000 disable_timer: False # Level of Logging: # 0: No log # 1: Log everything log_level: 1 # Metric related parameters. Please have a look at # https://torchmetrics.readthedocs.io/en/stable/references/metric.html#torchmetrics.Metric # for more information sync_on_compute: False aggregator: _target_: sheeprl.utils.metric.MetricAggregator raise_on_missing: False metrics: Rewards/rew_avg: _target_: torchmetrics.MeanMetric sync_on_compute: ${metric.sync_on_compute} Game/ep_len_avg: _target_: torchmetrics.MeanMetric sync_on_compute: ${metric.sync_on_compute} Argument Type Default Value(s) Description log_every int 5000 Number of steps between one log and the next disable_timer bool False Whether or not to disable timer information (training and environment interaction) log_level int 1 The level of logging (0: disabled, 1: log everything) sync_on_compute bool False Whether to synchronize the metrics between processes aggregator Dict[str, Any] - Configurations of the aggregator to be instantiated, containing the metrics to log You can modify the default metric configurations by adding in the custom_exp file the custom configuration you want under the metric key, as shown below. In this example, we do not log the timer information and we want to synchronize the metrics between the 2 processes. Moreover, we add 3 metrics to log to the aggregator (in addition to reward and episode length): the value loss, the policy loss, and the entropy loss. # @package _global_ defaults: # Inherit configs from custom_fabric_exp - custom_fabric_exp - _self_ # Set Metric parameters metric: disable_timer: True sync_on_compute: True aggregator: metrics: Loss/value_loss: _target_: torchmetrics.MeanMetric sync_on_compute: ${metric.sync_on_compute} Loss/policy_loss: _target_: torchmetrics.MeanMetric sync_on_compute: ${metric.sync_on_compute} Loss/entropy_loss: _target_: torchmetrics.MeanMetric sync_on_compute: ${metric.sync_on_compute} How to run it:\n# s=3 since `custom_metric_exp` extends from the fabric experiments diambra run -s=3 python train.py exp=custom_metric_exp The logs are stored in the ./logs/runs/\u0026lt;algo_name\u0026gt;/\u0026lt;env_id\u0026gt;/\u0026lt;datetime_experiment\u0026gt;/ folder, and to visualize the plots, you just need to run the following command:\ntensorboard --logdir /path/to/logging/directory open your browser and go to http://localhost:6006/. You can eventually modify the port of the process, for instance, you can use port 6010 by running the following command:\ntensorboard --logdir /path/to/logging/directory --port 6010 "},{"uri":"https://docs.diambra.ai/envs/games/doapp/","title":"Dead Or Alive ++","tags":[],"description":"","content":" Index Game Specific Info Specific Episode Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID doapp Original ROM Name doapp.zip SHA256 Checksum d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Search Keywords DEAD OR ALIVE ++ [JAPAN], dead-or-alive-japan, 80781, wowroms Game Resolution\n(H X W X C) 480px X 512px X 3 Number of Moves and Attack Actions 9, 8 (4)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-7): No-Attack, Hold, Punch, Kick, Hold+Punch, Hold+Kick, Punch+Kick, Hold+Punch+Kick Max Difficulty (1P Mode) 4 Number of Characters (Selectable) 11 (11) Max Number of Outfits 4 Number of Stages (1P Mode) 8 Specific Episode Settings Game Settings Name Type Default Value(s) Value Range difficulty None U int None [1, 4] Player Settings Name Type Default Value(s) Value Range characters* None U str U tuple of maximum three str None Kasumi, Zack, Hayabusa, Bayman, Lei-Fang, Raidou, Gen-Fu, Tina, Bass, Jann-Lee, Ayane outfits* int 1 [1, 4] *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nAction Spaces Type Space Size (Number of Actions) Discrete 9 (moves) + 8 (attacks) - 1 (no-op counted twice) = 16 MultiDiscrete 9 (moves) X 8 (attacks) = 72 Observation Space Some examples of Dead Or Alive ++ RAM states Global Key Type Value Range Description frame Box [0, 255] X [480 X 512 X 3] Latest game frame (RGB pixel screen) stage Box [1, 8] Current stage of the game timer Box [0, 40] Round time remaining Player specific Key Type Value Range Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, 2] Number of rounds won by the player character Discrete [0, 10] Index of character in use\n0: Kasumi, 1: Zack, 2: Hayabusa, 3: Bayman, 4: Lei-Fang, 5: Raidou, 6: Gen-Fu, 7: Tina, 8: Bass, 9: Jann-Lee, 10: Ayane health Box [0, 208] Health bar value "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/singleplayerenv/","title":"Single Player Environment","tags":[],"description":"","content":"This example focuses on:\nBasic environment usage (Standard RL, Single Player Mode) Environment settings configuration Agent - Environment interaction loop Gym observation visualization A dedicated section describing environment settings is presented here.\n#!/usr/bin/env python3 import diambra.arena from diambra.arena import SpaceTypes, Roles, EnvironmentSettings def main(): # Settings settings = EnvironmentSettings() # Single agent environment # --- Environment settings --- # Number of steps performed by the game # for every environment step, bounds: [1, 6] settings.step_ratio = 6 # Native frame resize operation settings.frame_shape = (128, 128, 0) # RBG with 128x128 size # settings.frame_shape = (0, 0, 1) # Grayscale with original size # settings.frame_shape = (0, 0, 0) # Deactivated (Original size RBG) # If to use discrete or multi_discrete action space settings.action_space = SpaceTypes.MULTI_DISCRETE # --- Episode settings --- # Player role selection: P1 (left), P2 (right), None (50% P1, 50% P2) settings.role = Roles.P1 # Game continue logic (0.0 by default): # - [0.0, 1.0]: probability of continuing game at game over # - int((-inf, -1.0]): number of continues at game over # before episode to be considered done settings.continue_game = 0.0 # If to show game final when game is completed settings.show_final = False # Game-specific options (see documentation for details) # Game difficulty level settings.difficulty = 4 # Character to be used, automatically extended with None for games # requiring to select more than one character (e.g. Tekken Tag Tournament) settings.characters = \u0026#34;Kasumi\u0026#34; # Character outfit settings.outfits = 2 env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings, render_mode=\u0026#34;human\u0026#34;) observation, info = env.reset(seed=42) env.show_obs(observation) while True: actions = env.action_space.sample() print(\u0026#34;Actions: {}\u0026#34;.format(actions)) observation, reward, terminated, truncated, info = env.step(actions) done = terminated or truncated env.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if done: # Optionally, change episode settings here options = {} options[\u0026#34;role\u0026#34;] = Roles.P2 options[\u0026#34;continue_game\u0026#34;] = 0.0 options[\u0026#34;difficulty\u0026#34;] = None options[\u0026#34;characters\u0026#34;] = None options[\u0026#34;outfits\u0026#34;] = 4 observation, info = env.reset(options=options) env.show_obs(observation) break env.close() # Return success return 0 if __name__ == \u0026#39;__main__\u0026#39;: main() "},{"uri":"https://docs.diambra.ai/competitionplatform/basicagentscript/","title":"Basic Agent Script","tags":[],"description":"","content":"The central element of a submission is the agent python script. Its structure is always composed by two main parts: the preparation step, where the agent and the environment setup is completed, and the interaction loop, where the classical agent-environment interaction happens.\nAfter a first call to the reset method, you start iterating alternating action selection and environment stepping, resetting the environment when the episode is done. That\u0026rsquo;s it, we take care of the rest.\nThere is one thing that is worth noticing: since we want your submission to be the same no matter how many episodes are needed to evaluate it, you need to implement the while loop in a way that it keeps iterating indefinitely (while True:) and only exits (the break statement) when the value info[\u0026quot;env_done\u0026quot;] is true. This value is set by us and used to let the agent know that the evaluation has been completed. In this way, the same script can be used to run evaluations made of 3, 5, 10 or whatever number of episodes you want, without changing a single line.\n#!/usr/bin/env python3 import diambra.arena from diambra.arena import SpaceTypes, Roles, EnvironmentSettings from diambra.arena.utils.gym_utils import available_games import random import argparse def main(game_id=\u0026#34;random\u0026#34;, test=False): game_dict = available_games(False) if game_id == \u0026#34;random\u0026#34;: game_id = random.sample(game_dict.keys(),1)[0] else: game_id = opt.gameId if opt.gameId in game_dict.keys() else random.sample(game_dict.keys(),1)[0] # Settings settings = EnvironmentSettings() settings.step_ratio = 6 settings.frame_shape = (128, 128, 1) settings.role = Roles.P2 settings.difficulty = 4 settings.action_space = SpaceTypes.MULTI_DISCRETE env = diambra.arena.make(game_id, settings) observation, info = env.reset() while True: action = env.get_no_op_action() observation, reward, terminated, truncated, info = env.step(action) if terminated or truncated: observation, info = env.reset() if info[\u0026#34;env_done\u0026#34;] or test is True: break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--gameId\u0026#39;, type=str, default=\u0026#34;random\u0026#34;, help=\u0026#39;Game ID\u0026#39;) parser.add_argument(\u0026#39;--test\u0026#39;, type=int, default=0, help=\u0026#39;Test mode\u0026#39;) opt = parser.parse_args() print(opt) main(opt.gameId, bool(opt.test)) "},{"uri":"https://docs.diambra.ai/projects/rlztournament/","title":"RLZ Tournament","tags":[],"description":"","content":" Project summary In 2021, we organized the very first AI Tournament leveraging DIAMBRA. It has been organized in collaboration with Reinforcement Learning Zurich (RLZ), a community of researchers, data scientists and software engineers interested in applications of Reinforcement Learning and AI.\nParticipants trained an AI algorithm to effectively play Dead Or Alive ++. The three best algorithms participated in the final event and competed for the 1400 CHF prize pool!\nAuthor(s) Dr. Claus Horn - RLZ (linkedin) Mark Rowan - RLZ (linkedin) Alessandro Palmas - DIAMBRA (linkedin) References Tournament Website: https://diambra.gitlab.io/website/aitournament/ RLZ Linkedin Page: https://www.linkedin.com/company/reinforcement-learning-zurich/ RLZ Meetup Page: https://www.meetup.com/Reinforcement-Learning-Zurich "},{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/stablebaselines3/","title":"Stable Baselines 3","tags":[],"description":"","content":" Index Getting Ready Native Interface Basic Basic Example Saving, Loading and Evaluating Parallel Environments Advanced Complete Training Script Agent Script for Competition The source code of all examples described in this section is available in our DIAMBRA Agents repository.\nGetting Ready We highly recommend using virtual environments to isolate your python installs, especially to avoid conflicts in dependencies. In what follows we use Conda but any other tool should work too.\nCreate and activate a new dedicated virtual environment:\nconda create -n diambra-arena-sb3 python=3.8 conda activate diambra-arena-sb3 Install DIAMBRA Arena with Stable Baselines 3 interface:\npip install diambra-arena[stable-baselines3] This should be enough to prepare your system to execute the following examples. You can refer to the official Stable Baselines 3 documentation or reach out on our Discord server for specific needs.\nAll the examples presented below are available here: DIAMBRA Agents - Stable Baselines 3. They have been created following the high level approach found on Stable Baselines 3 examples page, thus allowing to easily extend them and to understand how they interface with the different components.\nThese examples only aims at demonstrating the core functionalities and high level aspects, they will not generate well performing agents, even if the training time is extended to cover a large number of training steps. The user will need to build upon them, exploring aspects like: policy network architecture, algorithm hyperparameter tuning, observation space tweaking, rewards wrapping and other similar ones.\nNative interface DIAMBRA Arena native interface with Stable Baselines 3 covers a wide range of use cases, automating handling of vectorized environments and monitoring wrappers. In the majority of cases it will be sufficient for users to directly import and use it, with no need for additional customization. Below is reported its interface and a table describing its arguments.\ndef make_sb3_env(game_id: str, env_settings: EnvironmentSettings=EnvironmentSettings(), wrappers_settings: WrappersSettings=WrappersSettings(), episode_recording_settings: RecordingSettings=RecordingSettings(), render_mode: str=\u0026#34;rgb_array\u0026#34;, seed: int=None, start_index: int=0, allow_early_resets: bool=True, start_method: str=None, no_vec: bool=False, use_subprocess: bool=True, log_dir_base: str=\u0026#34;/tmp/DIAMBRALog/\u0026#34;): Argument Type Default Value(s) Description game_id str - Game environment identifier env_settings EnvironmentSettings EnvironmentSettings() Environment settings (see more) wrappers_settings WrappersSettings WrappersSettings() Wrappers settings (see more) episode_recording_settings RecordingSettings RecordingSettings() Episode recording settings\n(see more) render_mode str \u0026quot;rgb_array\u0026quot; Rendering mode seed int None Random number generator seed start_index int 0 Starting process rank index allow_early_resets bool True Monitor wrapper argument to allow environment reset before it is done start_method str None Method to spawn subprocesses when active (see more) no_vec bool False If True avoids using vectorized environments (valid only when using a single instance) use_subprocess bool True If to use subprocesses for multi-threaded parallelization log_dir_base str \u0026quot;/tmp/DIAMBRALog/\u0026quot; Folder where to save execution logs For the interface low level details, users can review the correspondent source code here.\nBasic For all the examples there are two main things to note about the observation space.\nFirst, the normalization wrapper is applied on all elements but the image frame, as Stable Baselines 3 automatically normalizes images and expects their pixels to be in the range [0 - 255].\nSecond, the library also has a specific constraint on dictionary observation spaces: they cannot be nested. For this reason we provide a flattening wrapper that creates a shallow, not nested, dictionary from the original observation space, allowing in addition to filter it by keys.\nStable Baselines 3 automatically defines the network architecture, properly matching the input type. In some of the examples the architecture is printed to the console output, allowing to clearly identify all the different contributions.\nBasic Example This example demonstrates how to:\nLeverage DIAMBRA Arena native Stable Baselines 3 interface to create the environment Interface the environment with one of Stable Baselines 3\u0026rsquo;s algorithms Train the algorithm Run the trained agent in the environment for one episode It uses the A2C algorithm, with a MultiInputPolicy policy network to properly process the dictionary observation space as input. For demonstration purposes, the algorithm is trained for only 200 steps, so the resulting agent will be far from optimal.\nfrom diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env, EnvironmentSettings, WrappersSettings from stable_baselines3 import A2C def main(): # Settings settings = EnvironmentSettings() settings.frame_shape = (128, 128, 1) settings.characters = (\u0026#34;Kasumi\u0026#34;) # Wrappers Settings wrappers_settings = WrappersSettings() wrappers_settings.normalize_reward = True wrappers_settings.stack_frames = 5 wrappers_settings.add_last_action = True wrappers_settings.stack_actions = 12 wrappers_settings.scale = True wrappers_settings.exclude_image_scaling = True wrappers_settings.role_relative = True wrappers_settings.flatten = True wrappers_settings.filter_keys = [\u0026#34;action\u0026#34;, \u0026#34;own_health\u0026#34;, \u0026#34;opp_health\u0026#34;, \u0026#34;own_side\u0026#34;, \u0026#34;opp_side\u0026#34;, \u0026#34;opp_character\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;timer\u0026#34;] # Create environment env, num_envs = make_sb3_env(\u0026#34;doapp\u0026#34;, settings, wrappers_settings) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) print(\u0026#34;\\nStarting training ...\\n\u0026#34;) agent = A2C(\u0026#34;MultiInputPolicy\u0026#34;, env, verbose=1) agent.learn(total_timesteps=200) print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;\\nStarting trained agent execution ...\\n\u0026#34;) observation = env.reset() while True: env.render() action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) if done: observation = env.reset() break print(\u0026#34;\\n... trained agent execution completed.\\n\u0026#34;) # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run python basic.py Saving, loading and evaluating In addition to what seen in the previous example, this one demonstrates how to:\nSave a trained agent Load a saved agent Evaluate an agent on a given number of episodes The same conditions of the previous example for algorithm, policy and training steps are used in this one too.\nfrom diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env, EnvironmentSettings, WrappersSettings from stable_baselines3 import A2C from stable_baselines3.common.evaluation import evaluate_policy def main(): # Settings settings = EnvironmentSettings() settings.frame_shape = (128, 128, 1) settings.characters = (\u0026#34;Kasumi\u0026#34;) # Wrappers Settings wrappers_settings = WrappersSettings() wrappers_settings.normalize_reward = True wrappers_settings.stack_frames = 5 wrappers_settings.add_last_action = True wrappers_settings.stack_actions = 12 wrappers_settings.scale = True wrappers_settings.exclude_image_scaling = True wrappers_settings.role_relative = True wrappers_settings.flatten = True wrappers_settings.filter_keys = [\u0026#34;action\u0026#34;, \u0026#34;own_health\u0026#34;, \u0026#34;opp_health\u0026#34;, \u0026#34;own_side\u0026#34;, \u0026#34;opp_side\u0026#34;, \u0026#34;opp_character\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;timer\u0026#34;] # Create environment env, num_envs = make_sb3_env(\u0026#34;doapp\u0026#34;, settings, wrappers_settings) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) # Instantiate the agent agent = A2C(\u0026#34;MultiInputPolicy\u0026#34;, env, verbose=1) # Train the agent agent.learn(total_timesteps=200) # Save the agent agent.save(\u0026#34;a2c_doapp\u0026#34;) del agent # delete trained agent to demonstrate loading # Load the trained agent # NOTE: if you have loading issue, you can pass `print_system_info=True` # to compare the system on which the agent was trained vs the current one # agent = A2C.load(\u0026#34;a2c_doapp\u0026#34;, env=env, print_system_info=True) agent = A2C.load(\u0026#34;a2c_doapp\u0026#34;, env=env) # Evaluate the agent # NOTE: If you use wrappers with your environment that modify rewards, # this will be reflected here. To evaluate with original rewards, # wrap environment in a \u0026#34;Monitor\u0026#34; wrapper before other wrappers. mean_reward, std_reward = evaluate_policy(agent, agent.get_env(), n_eval_episodes=3) print(\u0026#34;Reward: {} (avg) ± {} (std)\u0026#34;.format(mean_reward, std_reward)) # Run trained agent observation = env.reset() cumulative_reward = 0 while True: env.render() action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) cumulative_reward += reward if (reward != 0): print(\u0026#34;Cumulative reward =\u0026#34;, cumulative_reward) if done: observation = env.reset() break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run python saving_loading_evaluating.py Parallel Environments In addition to what seen in previous examples, this one demonstrates how to:\nRun training using parallel environments Print out the policy network architecture In this example, the PPO algorithm is used, with the same MultiInputPolicy seen before. The policy architecture is also printed to the console output, allowing to visualize how inputs are processed and \u0026ldquo;translated\u0026rdquo; to actions probabilities.\nThis example also runs multiple environments, automatically detecting the number of instances created by DIAMBRA CLI when running the script.\nfrom diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env, EnvironmentSettings, WrappersSettings from stable_baselines3 import PPO def main(): # Settings settings = EnvironmentSettings() settings.frame_shape = (128, 128, 1) settings.characters = (\u0026#34;Kasumi\u0026#34;) # Wrappers Settings wrappers_settings = WrappersSettings() wrappers_settings.normalize_reward = True wrappers_settings.stack_frames = 5 wrappers_settings.add_last_action = True wrappers_settings.stack_actions = 12 wrappers_settings.scale = True wrappers_settings.exclude_image_scaling = True wrappers_settings.role_relative = True wrappers_settings.flatten = True wrappers_settings.filter_keys = [\u0026#34;action\u0026#34;, \u0026#34;own_health\u0026#34;, \u0026#34;opp_health\u0026#34;, \u0026#34;own_side\u0026#34;, \u0026#34;opp_side\u0026#34;, \u0026#34;opp_character\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;timer\u0026#34;] # Create environment env, num_envs = make_sb3_env(\u0026#34;doapp\u0026#34;, settings, wrappers_settings) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) # Instantiate the agent agent = PPO(\u0026#34;MultiInputPolicy\u0026#34;, env, verbose=1) # Print policy network architecture print(\u0026#34;Policy architecture:\u0026#34;) print(agent.policy) # Train the agent agent.learn(total_timesteps=200) # Run trained agent observation = env.reset() cumulative_reward = [0.0 for _ in range(num_envs)] while True: action, _state = agent.predict(observation, deterministic=True) observation, reward, done, info = env.step(action) cumulative_reward += reward if any(x != 0 for x in reward): print(\u0026#34;Cumulative reward(s) =\u0026#34;, cumulative_reward) if done.any(): observation = env.reset() break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run -s=2 python parallel_envs.py Advanced Complete Training Script In addition to what seen in previous examples, this one demonstrates how to:\nBuild a complete training script to be used with Stable Baselines via a config file How to properly handle hyper-parameters scheduling via callbacks How to use callbacks for auto-saving How to control some policy network models and optimizer parameters This example show exactly how we trained our own models on these environments. It should be considered a starting point from where to explore and experiment, the following are just a few options among the most obvious ones:\nTweak hyper-parameters for the chosen algorithm Evolve the policy network architecture Test different algorithms, both on and off-policy Try to leverage behavioral cloning / imitation learning Modify the reward function to guide learning in other directions import os import yaml import json import argparse from diambra.arena import load_settings_flat_dict, SpaceTypes from diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env, EnvironmentSettings, WrappersSettings from diambra.arena.stable_baselines3.sb3_utils import linear_schedule, AutoSave from stable_baselines3 import PPO # diambra run -s 8 python stable_baselines3/training.py --cfgFile $PWD/stable_baselines3/cfg_files/sfiii3n/sr6_128x4_das_nc.yaml def main(cfg_file): # Read the cfg file yaml_file = open(cfg_file) params = yaml.load(yaml_file, Loader=yaml.FullLoader) print(\u0026#34;Config parameters = \u0026#34;, json.dumps(params, sort_keys=True, indent=4)) yaml_file.close() base_path = os.path.dirname(os.path.abspath(__file__)) model_folder = os.path.join(base_path, params[\u0026#34;folders\u0026#34;][\u0026#34;parent_dir\u0026#34;], params[\u0026#34;settings\u0026#34;][\u0026#34;game_id\u0026#34;], params[\u0026#34;folders\u0026#34;][\u0026#34;model_name\u0026#34;], \u0026#34;model\u0026#34;) tensor_board_folder = os.path.join(base_path, params[\u0026#34;folders\u0026#34;][\u0026#34;parent_dir\u0026#34;], params[\u0026#34;settings\u0026#34;][\u0026#34;game_id\u0026#34;], params[\u0026#34;folders\u0026#34;][\u0026#34;model_name\u0026#34;], \u0026#34;tb\u0026#34;) os.makedirs(model_folder, exist_ok=True) # Settings params[\u0026#34;settings\u0026#34;][\u0026#34;action_space\u0026#34;] = SpaceTypes.DISCRETE if params[\u0026#34;settings\u0026#34;][\u0026#34;action_space\u0026#34;] == \u0026#34;discrete\u0026#34; else SpaceTypes.MULTI_DISCRETE settings = load_settings_flat_dict(EnvironmentSettings, params[\u0026#34;settings\u0026#34;]) # Wrappers Settings wrappers_settings = load_settings_flat_dict(WrappersSettings, params[\u0026#34;wrappers_settings\u0026#34;]) # Create environment env, num_envs = make_sb3_env(settings.game_id, settings, wrappers_settings) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) # Policy param policy_kwargs = params[\u0026#34;policy_kwargs\u0026#34;] # PPO settings ppo_settings = params[\u0026#34;ppo_settings\u0026#34;] gamma = ppo_settings[\u0026#34;gamma\u0026#34;] model_checkpoint = ppo_settings[\u0026#34;model_checkpoint\u0026#34;] learning_rate = linear_schedule(ppo_settings[\u0026#34;learning_rate\u0026#34;][0], ppo_settings[\u0026#34;learning_rate\u0026#34;][1]) clip_range = linear_schedule(ppo_settings[\u0026#34;clip_range\u0026#34;][0], ppo_settings[\u0026#34;clip_range\u0026#34;][1]) clip_range_vf = clip_range batch_size = ppo_settings[\u0026#34;batch_size\u0026#34;] n_epochs = ppo_settings[\u0026#34;n_epochs\u0026#34;] n_steps = ppo_settings[\u0026#34;n_steps\u0026#34;] if model_checkpoint == \u0026#34;0\u0026#34;: # Initialize the agent agent = PPO(\u0026#34;MultiInputPolicy\u0026#34;, env, verbose=1, gamma=gamma, batch_size=batch_size, n_epochs=n_epochs, n_steps=n_steps, learning_rate=learning_rate, clip_range=clip_range, clip_range_vf=clip_range_vf, policy_kwargs=policy_kwargs, tensorboard_log=tensor_board_folder) else: # Load the trained agent agent = PPO.load(os.path.join(model_folder, model_checkpoint), env=env, gamma=gamma, learning_rate=learning_rate, clip_range=clip_range, clip_range_vf=clip_range_vf, policy_kwargs=policy_kwargs, tensorboard_log=tensor_board_folder) # Print policy network architecture print(\u0026#34;Policy architecture:\u0026#34;) print(agent.policy) # Create the callback: autosave every USER DEF steps autosave_freq = ppo_settings[\u0026#34;autosave_freq\u0026#34;] auto_save_callback = AutoSave(check_freq=autosave_freq, num_envs=num_envs, save_path=model_folder, filename_prefix=model_checkpoint + \u0026#34;_\u0026#34;) # Train the agent time_steps = ppo_settings[\u0026#34;time_steps\u0026#34;] agent.learn(total_timesteps=time_steps, callback=auto_save_callback) # Save the agent new_model_checkpoint = str(int(model_checkpoint) + time_steps) model_path = os.path.join(model_folder, new_model_checkpoint) agent.save(model_path) # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--cfgFile\u0026#34;, type=str, required=True, help=\u0026#34;Configuration file\u0026#34;) opt = parser.parse_args() print(opt) main(opt.cfgFile) How to run it:\ndiambra run python training.py --cfgFile /absolute/path/to/config.yaml and the configuration file to be used with this training script is reported below:\nfolders: parent_dir: \u0026#34;./results/\u0026#34; model_name: \u0026#34;sr6_128x4_das_nc\u0026#34; settings: game_id: \u0026#34;doapp\u0026#34; step_ratio: 6 frame_shape: !!python/tuple [128, 128, 1] continue_game: 0.0 action_space: \u0026#34;multi_discrete\u0026#34; characters: \u0026#34;Kasumi\u0026#34; difficulty: 3 outfits: 2 wrappers_settings: normalize_reward: true no_attack_buttons_combinations: true stack_frames: 4 dilation: 1 add_last_action: true stack_actions: 12 scale: true exclude_image_scaling: true role_relative: true flatten: true filter_keys: [\u0026#34;action\u0026#34;, \u0026#34;own_health\u0026#34;, \u0026#34;opp_health\u0026#34;, \u0026#34;own_side\u0026#34;, \u0026#34;opp_side\u0026#34;, \u0026#34;opp_character\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;timer\u0026#34;] policy_kwargs: #net_arch: [{ pi: [64, 64], vf: [32, 32] }] net_arch: [64, 64] ppo_settings: gamma: 0.94 model_checkpoint: \u0026#34;0\u0026#34; learning_rate: [2.5e-4, 2.5e-6] # To start clip_range: [0.15, 0.025] # To start #learning_rate: [5.0e-5, 2.5e-6] # Fine Tuning #clip_range: [0.075, 0.025] # Fine Tuning batch_size: 256 #8 #nminibatches gave different batch size depending on the number of environments: batch_size = (n_steps * n_envs) // nminibatches n_epochs: 4 n_steps: 128 autosave_freq: 256 time_steps: 512 Agent Script for Competition Finally, after the agent training is completed, besides running it locally in your own machine, you may want to submit it to our Competition Platform! To do so, you can use the following script that provides a ready to use, flexible example that can accommodate different models, games and settings.\nTo submit your trained agent to our platform, compete for the first leaderboard positions, and unlock our achievements, follow the simple steps described in the \u0026ldquo;How to Submit an Agent\u0026rdquo; section.\nimport os import yaml import json import argparse from diambra.arena import Roles, SpaceTypes, load_settings_flat_dict from diambra.arena.stable_baselines3.make_sb3_env import make_sb3_env, EnvironmentSettings, WrappersSettings from stable_baselines3 import PPO \u0026#34;\u0026#34;\u0026#34;This is an example agent based on stable baselines 3. Usage: diambra run python stable_baselines3/agent.py --cfgFile $PWD/stable_baselines3/cfg_files/doapp/sr6_128x4_das_nc.yaml --trainedModel \u0026#34;model_name\u0026#34; \u0026#34;\u0026#34;\u0026#34; def main(cfg_file, trained_model, test=False): # Read the cfg file yaml_file = open(cfg_file) params = yaml.load(yaml_file, Loader=yaml.FullLoader) print(\u0026#34;Config parameters = \u0026#34;, json.dumps(params, sort_keys=True, indent=4)) yaml_file.close() base_path = os.path.dirname(os.path.abspath(__file__)) model_folder = os.path.join(base_path, params[\u0026#34;folders\u0026#34;][\u0026#34;parent_dir\u0026#34;], params[\u0026#34;settings\u0026#34;][\u0026#34;game_id\u0026#34;], params[\u0026#34;folders\u0026#34;][\u0026#34;model_name\u0026#34;], \u0026#34;model\u0026#34;) # Settings params[\u0026#34;settings\u0026#34;][\u0026#34;action_space\u0026#34;] = SpaceTypes.DISCRETE if params[\u0026#34;settings\u0026#34;][\u0026#34;action_space\u0026#34;] == \u0026#34;discrete\u0026#34; else SpaceTypes.MULTI_DISCRETE settings = load_settings_flat_dict(EnvironmentSettings, params[\u0026#34;settings\u0026#34;]) settings.role = Roles.P1 # Wrappers Settings wrappers_settings = load_settings_flat_dict(WrappersSettings, params[\u0026#34;wrappers_settings\u0026#34;]) wrappers_settings.normalize_reward = False # Create environment env, num_envs = make_sb3_env(settings.game_id, settings, wrappers_settings, no_vec=True) print(\u0026#34;Activated {} environment(s)\u0026#34;.format(num_envs)) # Load the trained agent model_path = os.path.join(model_folder, trained_model) agent = PPO.load(model_path) # Print policy network architecture print(\u0026#34;Policy architecture:\u0026#34;) print(agent.policy) obs, info = env.reset() while True: action, _ = agent.predict(obs, deterministic=False) obs, reward, terminated, truncated, info = env.step(action.tolist()) if terminated or truncated: obs, info = env.reset() if info[\u0026#34;env_done\u0026#34;] or test is True: break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--cfgFile\u0026#34;, type=str, required=True, help=\u0026#34;Configuration file\u0026#34;) parser.add_argument(\u0026#34;--trainedModel\u0026#34;, type=str, default=\u0026#34;model\u0026#34;, help=\u0026#34;Model checkpoint\u0026#34;) parser.add_argument(\u0026#34;--test\u0026#34;, type=int, default=0, help=\u0026#34;Test mode\u0026#34;) opt = parser.parse_args() print(opt) main(opt.cfgFile, opt.trainedModel, bool(opt.test)) How to run it locally:\ndiambra run python agent.py --cfgFile /absolute/path/to/config.yaml --trainedModel \u0026#34;model_name\u0026#34; and the configuration file to be used is the same that was used for training it, like the one reported in the previous paragraph.\n"},{"uri":"https://docs.diambra.ai/gettingstarted/examples/","title":"Examples","tags":[],"description":"","content":"This section presents a detailed description of the examples that are provided with DIAMBRA Arena repository. They cover the most important use-cases, and can be used as templates and starting points to explore all the features of the software package.\nThese examples show how to leverage both single and two players modes, how to set up environment wrappers with all their options, how to record human expert demonstrations and how to load them to apply imitation learning.\nEvery example has a dedicated page that can be reached via the sidebar menu or the list below.\nSource code for examples described in what follows can be found in the code repository, here.\nExample Scripts Single Player Environment Multi Player Environment Wrappers Options Episode Recorder Dataset Loader "},{"uri":"https://docs.diambra.ai/envs/games/","title":"Games &amp; Specifics","tags":[],"description":"","content":" Dead or Alive ++ Street Fighter III 3rd Strike Tekken Tag Tournament Ultimate Mortal Kombat 3 Samurai Showdown 5 Sp The King of Fighers '98 UMH ... many more to come soon.\nTitle FPS* Game Id Dead Or Alive ++ 92 doapp Street Fighter III 3rd Strike 164 sfiii3n Tekken Tag Tournament 72 tektagt Ultimate Mortal Kombat 3 158 umk3 Samurai Showdown 5 Special 168 samsh5sp The King of Fighters \u0026lsquo;98: Ultimate Match Hero 92 kof98umh *Measured on Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz, using step_ratio = 1 and frame_shape = (128, 128, 1)\nGame Specific Info Game specific details provide useful information about each title. They are reported in every game-dedicated page, and summarized in the table below.\nParameter Description Game ID String identifying the game Original ROM Name Name of the original game ROM to be downloaded (if renaming is needed, it is indicated) SHA256 Checksum ROM file checksum used to validate it Search Keywords List of keywords that can be used to find the correct ROM file Game Resolution (H X W X C) Game frame native resolution Number of Moves and Attack Actions Number of moves and attack actions and their description Max Difficulty Maximum difficulty level available Number of Characters (Selectable) Number of characters featured in the game, and those that can actually be selected Max Number of Outfits Maximum number of different outfits available per each character Max Stage Maximum number of stages for the single player mode Whenever possible, games are released with all hidden/bonus characters unlocked.\nFor every released title, extensive testing has been carried out, making sure the 1P mode is playable with no bugs up until game end.\n"},{"uri":"https://docs.diambra.ai/gettingstarted/","title":"Getting Started","tags":[],"description":"","content":"Index Prerequisites Running the Environment Basic Script DIAMBRA Command Line Interface (CLI) Script Execution Advanced Usage DIAMBRA CLI Advanced Options Running Multiple Environments in Parallel Run DIAMBRA Engine without CLI Environment Native Rendering Prerequisites Having completed and tested the installation as described in Installation and Quickstart sections in homepage Having downloaded the ROMs and placed them all in the same folder, whose absolute path will be referred in the following as /absolute/path/to/roms/folder/ To avoid specifying ROMs path at every run, you can define the environment variable DIAMBRAROMSPATH=/absolute/path/to/roms/folder/, either temporarily in your current shell/prompt session, or permanently in your profile (e.g. on linux in ~/.bashrc).\nRunning the Environment Basic Script The most straightforward and simple script to use DIAMBRA Arena is reported below. It features a random agent playing Dead Or Alive ++, and it represents the general interaction schema to be used for every game and context of DIAMBRA Arena.\n#!/usr/bin/env python3 import diambra.arena def main(): # Environment creation env = diambra.arena.make(\u0026#34;doapp\u0026#34;, render_mode=\u0026#34;human\u0026#34;) # Environment reset observation, info = env.reset(seed=42) # Agent-Environment interaction loop while True: # (Optional) Environment rendering env.render() # Action random sampling actions = env.action_space.sample() # Environment stepping observation, reward, terminated, truncated, info = env.step(actions) # Episode end (Done condition) check if terminated or truncated: observation, info = env.reset() break # Environment shutdown env.close() # Return success return 0 if __name__ == \u0026#39;__main__\u0026#39;: main() More complex and complete examples can be found in the Examples section.\nDIAMBRA Command Line Interface (CLI) DIAMBRA Arena comes with a very handy tool: the DIAMBRA Command Line Interface (DIAMBRA CLI). It provides different useful commands, with related options, that contribute to make running DIAMBRA Arena environments super easy.\nThe main use of the CLI is running a command after brining up DIAMBRA Arena containerized environment(s). It sets the DIAMBRA_ENVS environment variable to list the endpoints of all running environments.\nUsage:\ndiambra run [flags] \u0026lt;command-to-execute\u0026gt; The only flag needed for simple executions is listed below. Advanced usage and options can be found in the CLI Advanced Options section below.\nFlag Type Description -r, --path.roms str Path to ROMs (default to DIAMBRAROMSPATH env var if set) Script Execution To run a python script using the CLI, one can just execute the following command:\ndiambra run -r /absolute/path/to/roms/folder/ python diambra_arena_gist.py This will start a new container with the environment, load in the DIAMBRA_ENVS environment variable the port on which the environment accepts connections, and run the script where the DIAMBRA Arena python module is imported and used to instantiate a new environment, that will automatically retrieve the port and connect to it.\nAdvanced Usage In what follows, we will omit the -r flag for the CLI, assuming the user has set the DIAMBRAROMSPATH environment variable in his system.\nDIAMBRA CLI Advanced Options Run Command Usage:\ndiambra run [flags] \u0026lt;command-to-execute\u0026gt; It runs a command after brining up DIAMBRA Arena containerized environment(s). It sets the DIAMBRA_ENVS environment variable to list the endpoints of all running environments.\nThe next snippet shows the help message for this command, where all available options are reported:\nRun runs the given command after diambraEngine is brought up. It will set the DIAMBRA_ENVS environment variable to list the endpoints of all running environments. The DIAMBRA arena python package will automatically be configured by this. The flag --agent-image can be used to run the commands in the given image. Usage: diambra run [flags] command [args...] Flags: -a, --agent.image string Run given agent command in container -l, --engine.lockfps Lock FPS -g, --engine.render Render graphics server side --engine.sound Enable sound -x, --env.autoremove Remove containers on exit (default true) --env.containerip Use \u0026lt;containerIP\u0026gt;:\u0026lt;containerPort\u0026gt; instead of \u0026lt;env.host/localhost\u0026gt;:\u0026lt;hostPort\u0026gt; --env.host string Host to bind ports on (default \u0026#34;127.0.0.1\u0026#34;) --env.image string Env image to use, omit to detect from diambra-arena version --env.mount strings Host mounts for env container (/host/path:/container/path) --env.preallocateport Preallocate port for env container. Workaround for port conflicts on Windows -s, --env.scale int Number of environments to run (default 1) --env.seccomp string Path to seccomp profile to use for env (may slow down environment). Set to \u0026#34;\u0026#34; for runtime\u0026#39;s default profile. (default \u0026#34;unconfined\u0026#34;) -h, --help help for run -n, --images.no-pull Do not try to pull image before running --init.image string Init image to use (default \u0026#34;ghcr.io/diambra/init:main\u0026#34;) -i, --interactive Open stdin for interactions with arena and agent (default true) --path.credentials string Path to credentials file (default \u0026#34;/home/alexpalms/.diambra/credentials\u0026#34;) -r, --path.roms string Path to ROMs (default to DIAMBRAROMSPATH env var if set) (default \u0026#34;/home/alexpalms/work/diambra/roms\u0026#34;) Global Flags: -d, --log.debug Enable debug logging --log.format string Set logging output format (logfmt, json, fancy) (default \u0026#34;fancy\u0026#34;) Currently, the -g, --engine.render option to render graphics server side, is only available for Linux systems. For additional info and for Windows/MacOS alternatives, see Environment Native Rendering section below.\nArena Command Usage:\ndiambra arena [flags] [command] It brings up DIAMBRA Arena containerized environment(s) and returns to the terminal output the endpoints of all running environments.\nThe snippet below lists all available commands for this mode.\nFlags reported for the Run command above apply also to this mode.\nThese are the arena related commands Usage: diambra arena [command] Available Commands: check-roms check roms down Stop DIAMBRA Arena list-roms list roms status Show status of DIAMBRA arena up Start DIAMBRA arena version version Flags: -h, --help help for arena Global Flags: -d, --log.debug Enable debug logging --log.format string Set logging output format (logfmt, json, fancy) (default \u0026#34;fancy\u0026#34;) Use \u0026#34;diambra arena [command] --help\u0026#34; for more information about a command. Running Multiple Environments in Parallel It can be useful to run multiple environment instances in parallel, for example for Deep RL training. The CLI provides a flag to control this, it can be used both by the run and the arena commands. The former will load a string in the DIAMBRA_ENVS environment variable where connection addresses are listed and separated by a space, while the latter will print out in the terminal the same string. These values can then be used properly to setup multiple parallel connections.\nRunning a script after having started 16 containers:\ndiambra run -s=16 python training_script.py Starting 4 containers and printing their addresses in the terminal:\ndiambra arena -s=4 up Server listening on 0.0.0.0:50051 127.0.0.1:49154 127.0.0.1:49155 127.0.0.1:49156 127.0.0.1:49157 Run DIAMBRA Engine without CLI Agents connect via network using gRPC to DIAMBRA Engine running in a Docker container. The diambra CLI\u0026rsquo;s run command starts the DIAMBRA Engine in a Docker container and sets up the environment to make it easy to connect to the Engine. For troubleshooting it might be useful to run the Engine manually, using host networking.\nStart Engine Linux/MacOS Win (cmd) Win (PowerShell) mkdir ~/.diambra touch ~/.diambra/credentials docker run -d --rm --name engine \\ -v $HOME/.diambra/credentials:/tmp/.diambra/credentials \\ -v /absolute/path/to/roms/folder/:/opt/diambraArena/roms \\ --net=host docker.io/diambra/engine:latest mkdir %userprofile%/.diambra echo \u0026gt; %userprofile%/.diambra/credentials docker run --rm -ti --name engine ^ -v %userprofile%/.diambra/credentials:/tmp/.diambra/credentials ^ -v %userprofile%/.diambra/roms:/opt/diambraArena/roms ^ --net=host docker.io/diambra/engine:latest mkdir $Env:userprofile/.diambra echo \u0026#34;\u0026#34; \u0026gt; $Env:userprofile/.diambra/credentials docker run --rm -ti --name engine ` -v $Env:userprofile/.diambra/credentials:/tmp/.diambra/credentials ` -v $Env:userprofile/.diambra/roms:/opt/diambraArena/roms ` --net=host docker.io/diambra/engine:latest Creating the ~/.diambra and ~/.diambra/credentials is only needed when you never ran the diambra CLI before. Otherwise this step can be skipped.\nConnect to Engine Now you can run the script that uses DIAMBRA Arena by opening a new terminal and setting DIAMBRA_ENVS environment variable followed by the python command:\nDIAMBRA_ENVS=localhost:50051 python ./script.py Environment Native Rendering It is possible to activate emulator native rendering while running environments (i.e. bringing up the emulator graphics window). The CLI provides a specific flag for this purpose, but currently this is supported only on Linux, while Windows and MacOS users have to configure a Xserver and link it to the environment container. The next tabs provide hints for each context.\nLinux Win MacOS On Linux, the CLI allows to render emulator natively on the host, the user only needs to add the -g flag to the run command, as follows:\ndiambra run -g python diambra_arena_gist.py Activating emulator native rendering will open a GUI where the game executes. Currently, this feature is affected by a problem: the mouse cursor disappears and remains constrained inside such window. To re-aquire control of the OS Xserver, one can circle through the active windows using the key combination ALT+TAB and highlight a different one.\nTo run environments with native emulator GUI support on Windows, currently requires the user to setup a virtual XServer and connect it to the container. We cannot provide support for this use case at the moment, but we plan to implement this feature in the near future.\nA virtual XServer that in our experience proved to be effective is VcXsrv Windows X Server.\nTo run environments with native emulator GUI support on MacOS, currently requires the user to setup a virtual XServer and connect it to the container. We cannot provide support for this use case at the moment, but we plan to implement this feature in the near future.\nA virtual XServer that in our experience proved to be effective is XQuartz 2.7.8 coupled to socat that can be installed via brew install socat.\n"},{"uri":"https://docs.diambra.ai/gettingstarted/examples/multiplayerenv/","title":"Multi Player Environment","tags":[],"description":"","content":"This example focuses on:\nTwo players mode environment usage (Competitive Multi-Agent, SelfPlay, Competitive Human-Agent) Environment settings configuration Agent - Environment interaction loop Gym observation visualization A dedicated section describing environment settings is presented here.\n#!/usr/bin/env python3 import diambra.arena from diambra.arena import SpaceTypes, EnvironmentSettingsMultiAgent def main(): # Environment Settings settings = EnvironmentSettingsMultiAgent() # Multi Agents environment # --- Environment settings --- # If to use discrete or multi_discrete action space settings.action_space = (SpaceTypes.DISCRETE, SpaceTypes.DISCRETE) # --- Episode settings --- # Characters to be used, automatically extended with None for games # requiring to select more than one character (e.g. Tekken Tag Tournament) settings.characters = (\u0026#34;Ryu\u0026#34;, \u0026#34;Ken\u0026#34;) # Characters outfit settings.outfits = (2, 2) env = diambra.arena.make(\u0026#34;sfiii3n\u0026#34;, settings, render_mode=\u0026#34;human\u0026#34;) observation, info = env.reset(seed=42) env.show_obs(observation) while True: actions = env.action_space.sample() print(\u0026#34;Actions: {}\u0026#34;.format(actions)) observation, reward, terminated, truncated, info = env.step(actions) done = terminated or truncated env.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if done: # Optionally, change episode settings here options = {} options[\u0026#34;characters\u0026#34;] = (None, None) options[\u0026#34;char_outfits\u0026#34;] = (5, 5) observation, info = env.reset(options=options) env.show_obs(observation) break env.close() # Return success return 0 if __name__ == \u0026#39;__main__\u0026#39;: main() "},{"uri":"https://docs.diambra.ai/envs/games/sfiii3n/","title":"Street Fighter III 3rd Strike","tags":[],"description":"","content":" Index Game Specific Info Specific Episode Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID sfiii3n Original ROM Name sfiii3n.zip SHA256 Checksum 7239b5eb005488db22ace477501c574e9420c0ab70aeeb0795dfeb474284d416 Search Keywords STREET FIGHTER III 3RD STRIKE: FIGHT FOR THE FUTUR [JAPAN] (CLONE), street-fighter-iii-3rd-strike-fight-for-the-futur-japan-clone, 106255, wowroms Game Resolution\n(H X W X C) 224px X 384px X 3 Number of Moves and Attack Actions 9, 10 (7)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-9): No-Attack, Low Punch, Medium Punch, High Punch, Low Kick, Medium Kick, High Kick, Low Punch+Low Kick, Medium Punch+Medium Kick, High Punch+High Kick Max Difficulty (1P Mode) 8 Number of Characters (Selectable) 20 (19) Max Number of Outfits 7 Number of Stages (1P Mode) 10 Specific Episode Settings Game Settings Key Type Default Value(s) Value Range difficulty None U int None [1, 8] Player Settings Key Type Default Value(s) Value Range characters* None U str U tuple of maximum three str None Alex, Twelve, Hugo, Sean, Makoto, Elena, Ibuki, Chun-Li, Dudley, Necro, Q, Oro, Urien, Remy, Ryu, Gouki, Yun, Yang, Ken outfits* int 1 [1, 7] Additional Player Settings Key Type Default Value(s) Value Range Description super_art* None U int None [1, 3] Selects the type of super move.\n1-2-3: Super move 1-2-3 *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nAction Spaces Type Space Size (Number of Actions) Discrete 9 (moves) + 10 (attacks) - 1 (no-op counted twice) = 18 MultiDiscrete 9 (moves) X 10 (attacks) = 90 Observation Space Some examples of Street Fighter III RAM states Global Key Type Value Range Description frame Box [0, 255] X [224 X 384 X 3] Latest game frame (RGB pixel screen) stage Box [1, 10] Current stage of the game timer Box [0, 100] Round time remaining Player specific Key Type Value Range Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, 2] Number of rounds won by the player character Discrete [0, 19] Index of character in use\n0: Alex, 1: Twelve, 2: Hugo, 3: Sean, 4: Makoto, 5: Elena, 6: Ibuki, 7: Chun-Li, 8: Dudley, 9: Necro, 10: Q, 11: Oro, 12: Urien, 13: Remy, 14: Ryu, 15: Gouki, 16: Yun, 17: Yang, 18: Ken, 19: Gill health Box [-1, 160] Health bar value stun_bar Box [0, 72] Stun bar value stunned Discrete (Binary) [0, 1] Stunned flag super_bar Box [0, 128] Super bar value super_type Discrete [0, 2] Selected type of super move\n0-1-2: Super Type 1-2-3 super_count Box [0, 3] Count of activated super moves super_max Box [1, 3] Maximum number of activated super moves "},{"uri":"https://docs.diambra.ai/projects/gamepainter/","title":"Game Painter","tags":[],"description":"","content":" Project summary This project is an experiment that applies in real-time the style of famous paintings to popular fighting retro games, which are provided as Reinforcement Learning environments by DIAMBRA.\nAuthor(s) Alessandro Palmas - DIAMBRA (linkedin) References GitHub Repo: https://github.com/alexpalms/diambra-game-painter Reference Paper: Perceptual Losses for Real-Time Style Transfer and Super-Resolution Paper Implementation GitHub Repo: https://github.com/1627180283/real-time-Style-Transfer "},{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/rayrllib/","title":"Ray RLlib","tags":[],"description":"","content":" Index Getting Ready Native Interface Basic Basic Example Saving, Loading and Evaluating Parallel Environments Advanced Dictionary Observations Agent Script for Competition The source code of all examples described in this section is available in our DIAMBRA Agents repository.\nGetting Ready We highly recommend using virtual environments to isolate your python installs, especially to avoid conflicts in dependencies. In what follows we use Conda but any other tool should work too.\nCreate and activate a new dedicated virtual environment:\nconda create -n diambra-arena-ray python=3.8 conda activate diambra-arena-ray Install DIAMBRA Arena with Ray RLlib interface:\npip install diambra-arena[ray-rllib] This should be enough to prepare your system to execute the following examples. You can refer to the official Ray RLlib documentation or reach out on our Discord server for specific needs.\nAll the examples presented below are available here: DIAMBRA Agents - Ray RLlib. They have been created following the high level approach found on Ray RLlib examples page and their related repository collection, thus allowing to easily extend them and to understand how they interface with the different components.\nThese examples only aims at demonstrating the core functionalities and high level aspects, they will not generate well performing agents, even if the training time is extended to cover a large number of training steps. The user will need to build upon them, exploring aspects like: policy network architecture, algorithm hyperparameter tuning, observation space tweaking, rewards wrapping and other similar ones.\nNative interface DIAMBRA Arena native interface with Ray RLlib covers a wide range of use cases, automating handling of key things like parallelization. In the majority of cases it will be sufficient for users to directly import and use it, with no need for additional customization.\nFor the interface low level details, users can review the correspondent source code here.\nBasic Basic Example This example demonstrates how to:\nBuild the config dictionary for Ray RLlib Interface one of Ray RLlib\u0026rsquo;s algorithms with DIAMBRA Arena using the native interface Train the algorithm Run the trained agent in the environment for one episode It uses the PPO algorithm and, for demonstration purposes, the algorithm is trained for only 200 steps, so the resulting agent will be far from optimal.\nimport diambra.arena from diambra.arena import SpaceTypes, EnvironmentSettings import gymnasium as gym from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO, PPOConfig from ray.tune.logger import pretty_print def main(): # Environment Settings env_settings = EnvironmentSettings() env_settings.frame_shape = (84, 84, 1) env_settings.action_space = SpaceTypes.DISCRETE # env_config env_config = { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: env_settings, } config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: env_config, \u0026#34;num_workers\u0026#34;: 0, \u0026#34;train_batch_size\u0026#34;: 200, } # Update config file config = preprocess_ray_config(config) # Instantiating the agent agent = PPO(config=config) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(1): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) result = agent.train() print(pretty_print(result)) print(\u0026#34;\\n .. training completed.\u0026#34;) # Run the trained agent (and render each timestep output). print(\u0026#34;\\nStarting trained agent execution ...\\n\u0026#34;) env = diambra.arena.make(\u0026#34;doapp\u0026#34;, env_settings, render_mode=\u0026#34;human\u0026#34;) observation, info = env.reset() while True: env.render() action = agent.compute_single_action(observation) observation, reward, terminated, truncated, info = env.step(action) if terminated or truncated: observation, info = env.reset() break print(\u0026#34;\\n... trained agent execution completed.\\n\u0026#34;) # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run python basic.py Saving, loading and evaluating In addition to what seen in the previous example, this one demonstrates how to:\nPrint out the policy network architecture Save a trained agent Load a saved agent Evaluate an agent on a given number of episodes Print training and evaluation results The same conditions of the previous example for algorithm, policy and training steps are used in this one too.\nfrom diambra.arena import SpaceTypes, EnvironmentSettings from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO from ray.tune.logger import pretty_print def main(): # Settings env_settings = EnvironmentSettings() env_settings.frame_shape = (84, 84, 1) env_settings.action_space = SpaceTypes.DISCRETE config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: env_settings, }, \u0026#34;num_workers\u0026#34;: 0, \u0026#34;train_batch_size\u0026#34;: 200, \u0026#34;framework\u0026#34;: \u0026#34;torch\u0026#34;, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) print(\u0026#34;Policy architecture =\\n{}\u0026#34;.format(agent.get_policy().model)) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(1): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) results = agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;Training results:\\n{}\u0026#34;.format(pretty_print(results))) # Save the agent checkpoint = agent.save().checkpoint.path print(\u0026#34;Checkpoint saved at {}\u0026#34;.format(checkpoint)) del agent # delete trained model to demonstrate loading # Load the trained agent agent = PPO(config=config) agent.restore(checkpoint) print(\u0026#34;Agent loaded\u0026#34;) # Evaluate the trained agent (and render each timestep to the shell\u0026#39;s # output). print(\u0026#34;\\nStarting evaluation ...\\n\u0026#34;) results = agent.evaluate() print(\u0026#34;\\n... evaluation completed.\\n\u0026#34;) print(\u0026#34;Evaluation results:\\n{}\u0026#34;.format(pretty_print(results))) # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run python saving_loading_evaluating.py Parallel Environments In addition to what seen in previous examples, this one demonstrates how to:\nRun training and evaluation using parallel environments This example runs multiple environments. In order to properly execute it, the user needs to specify the correct number of environments instances to be created via DIAMBRA CLI when running the script. In particular, in this case, 6 different instances are needed:\n2 rollout workers with 2 environments each, accounting for 4 environments 1 evaluation worker with 2 environments, accounting for the remaining 2 environments from diambra.arena import SpaceTypes, EnvironmentSettings from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO from ray.tune.logger import pretty_print def main(): # Settings env_settings = EnvironmentSettings() env_settings.frame_shape = (84, 84, 1) env_settings.action_space = SpaceTypes.DISCRETE config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: env_settings, }, \u0026#34;train_batch_size\u0026#34;: 200, # Use 2 rollout workers \u0026#34;num_workers\u0026#34;: 2, # Use a vectorized env with 2 sub-envs. \u0026#34;num_envs_per_worker\u0026#34;: 2, # Evaluate once per training iteration. \u0026#34;evaluation_interval\u0026#34;: 1, # Run evaluation on (at least) two episodes \u0026#34;evaluation_duration\u0026#34;: 2, # ... using one evaluation worker (setting this to 0 will cause # evaluation to run on the local evaluation worker, blocking # training until evaluation is done). \u0026#34;evaluation_num_workers\u0026#34;: 1, # Special evaluation config. Keys specified here will override # the same keys in the main config, but only for evaluation. \u0026#34;evaluation_config\u0026#34;: { # Render the env while evaluating. # Note that this will always only render the 1st RolloutWorker\u0026#39;s # env and only the 1st sub-env in a vectorized env. \u0026#34;render_env\u0026#34;: True, }, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(2): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) results = agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;Training results:\\n{}\u0026#34;.format(pretty_print(results))) # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run -s=6 python parallel_envs.py Advanced Dictionary Observations In addition to what seen in previous examples, this one demonstrates how to:\nActivate a complete set of environment wrappers How to properly handle dictionary observations for Ray RLlib The main thing to note in this example is that the library does not have constraints on dictionary observation spaces, being able to handle nested ones too.\nThe policy network is automatically generated, properly handling different types of inputs. Model architecture is then printed to the console output, allowing to clearly identify all the different contributions.\nfrom diambra.arena import SpaceTypes, EnvironmentSettings, WrappersSettings from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO from ray.tune.logger import pretty_print def main(): # Settings env_settings = EnvironmentSettings() env_settings.frame_shape = (84, 84, 1) env_settings.characters = (\u0026#34;Kasumi\u0026#34;) env_settings.action_space = SpaceTypes.DISCRETE # Wrappers Settings wrappers_settings = WrappersSettings() wrappers_settings.normalize_reward = True wrappers_settings.add_last_action = True wrappers_settings.stack_actions = 12 wrappers_settings.stack_frames = 5 wrappers_settings.scale = True wrappers_settings.role_relative = True config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: env_settings, \u0026#34;wrappers_settings\u0026#34;: wrappers_settings, }, \u0026#34;num_workers\u0026#34;: 0, \u0026#34;train_batch_size\u0026#34;: 200, \u0026#34;framework\u0026#34;: \u0026#34;torch\u0026#34;, } # Update config file config = preprocess_ray_config(config) # Create the RLlib Agent. agent = PPO(config=config) print(\u0026#34;Policy architecture =\\n{}\u0026#34;.format(agent.get_policy().model)) # Run it for n training iterations print(\u0026#34;\\nStarting training ...\\n\u0026#34;) for idx in range(1): print(\u0026#34;Training iteration:\u0026#34;, idx + 1) results = agent.train() print(\u0026#34;\\n .. training completed.\u0026#34;) print(\u0026#34;Training results:\\n{}\u0026#34;.format(pretty_print(results))) # Evaluate the trained agent (and render each timestep to the shell\u0026#39;s # output). print(\u0026#34;\\nStarting evaluation ...\\n\u0026#34;) results = agent.evaluate() print(\u0026#34;\\n... evaluation completed.\\n\u0026#34;) print(\u0026#34;Evaluation results:\\n{}\u0026#34;.format(pretty_print(results))) # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() How to run it:\ndiambra run python dict_obs_space.py Agent Script for Competition Finally, after the agent training is completed, besides running it locally in your own machine, you may want to submit it to our Competition Platform! To do so, you can use the following script that provides a ready to use, flexible example that can accommodate different models, games and settings.\nTo submit your trained agent to our platform, compete for the first leaderboard positions, and unlock our achievements, follow the simple steps described in the \u0026ldquo;How to Submit an Agent\u0026rdquo; section.\nimport argparse import diambra.arena from diambra.arena import SpaceTypes, EnvironmentSettings, WrappersSettings from diambra.arena.ray_rllib.make_ray_env import DiambraArena, preprocess_ray_config from ray.rllib.algorithms.ppo import PPO # Reference: https://github.com/ray-project/ray/blob/ray-2.0.0/rllib/examples/inference_and_serving/policy_inference_after_training.py \u0026#34;\u0026#34;\u0026#34;This is an example agent based on RL Lib. Usage: diambra run python agent.py --trainedModel /absolute/path/to/checkpoint/ --envSpaces /absolute/path/to/environment/spaces/descriptor/ \u0026#34;\u0026#34;\u0026#34; def main(trained_model, env_spaces, test=False): # Settings env_settings = EnvironmentSettings() env_settings.frame_shape = (84, 84, 1) env_settings.characters = (\u0026#34;Kasumi\u0026#34;) env_settings.action_space = SpaceTypes.DISCRETE # Wrappers Settings wrappers_settings = WrappersSettings() wrappers_settings.normalize_reward = True wrappers_settings.add_last_action = True wrappers_settings.stack_actions = 12 wrappers_settings.stack_frames = 5 wrappers_settings.scale = True wrappers_settings.role_relative = True config = { # Define and configure the environment \u0026#34;env\u0026#34;: DiambraArena, \u0026#34;env_config\u0026#34;: { \u0026#34;game_id\u0026#34;: \u0026#34;doapp\u0026#34;, \u0026#34;settings\u0026#34;: env_settings, \u0026#34;wrappers_settings\u0026#34;: wrappers_settings, \u0026#34;load_spaces_from_file\u0026#34;: True, \u0026#34;env_spaces_file_name\u0026#34;: env_spaces, }, \u0026#34;num_workers\u0026#34;: 0, } # Update config file config = preprocess_ray_config(config) # Load the trained agent agent = PPO(config=config) agent.restore(trained_model) print(\u0026#34;Agent loaded\u0026#34;) # Print the agent policy architecture print(\u0026#34;Policy architecture =\\n{}\u0026#34;.format(agent.get_policy().model)) env = diambra.arena.make(\u0026#34;doapp\u0026#34;, env_settings, wrappers_settings, render_mode=\u0026#34;human\u0026#34;) obs, info = env.reset() while True: env.render() action = agent.compute_single_action(observation=obs, explore=True, policy_id=\u0026#34;default_policy\u0026#34;) obs, reward, terminated, truncated, info = env.step(action) if terminated or truncated: obs, info = env.reset() if info[\u0026#34;env_done\u0026#34;] or test is True: break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--trainedModel\u0026#34;, type=str, required=True, help=\u0026#34;Model path\u0026#34;) parser.add_argument(\u0026#34;--envSpaces\u0026#34;, type=str, required=True, help=\u0026#34;Environment spaces descriptor file path\u0026#34;) parser.add_argument(\u0026#34;--test\u0026#34;, type=int, default=0, help=\u0026#34;Test mode\u0026#34;) opt = parser.parse_args() print(opt) main(opt.trainedModel, opt.envSpaces, bool(opt.test)) How to run it locally:\ndiambra run python agent.py --trainedModel /absolute/path/to/checkpoint/ --envSpaces /absolute/path/to/environment/spaces/descriptor/ "},{"uri":"https://docs.diambra.ai/competitionplatform/submissionevaluation/","title":"Submission Evaluation","tags":[],"description":"","content":"Each time you submit an agent, it is run for one episode to be evaluated. Every submission will generate a score, used for leaderboard positioning, and will unlock achievements.\nThe score is a function of both the total cumulative reward and the submission difficulty you selected at submission time, which can be either \u0026ldquo;Easy\u0026rdquo;, \u0026ldquo;Medium\u0026rdquo; or \u0026ldquo;Hard\u0026rdquo;. Every game has a different difficulty level scale, so a specific mapping is applied and is represented by the following table:\nGame Easy Medium Hard Dead Or Alive ++ 2 3 4 Street Fighter III 4 6 8 Tekken Tag Tournament 5 7 9 Ultimate Mortal Kombat 3 3 4 5 Samurai Showdown 5 4 6 8 King of Fighters \u0026lsquo;98 4 6 8 The relation that links score with total cumulative reward and difficulty is shown in the picture below. When \u0026ldquo;Easy\u0026rdquo; is selected, the score is exactly equal to the total cumulative reward. When \u0026ldquo;Medium\u0026rdquo; (or \u0026ldquo;Hard\u0026rdquo;) is selected, the score is obtained multiplying the total cumulative reward by a weighting value that varies linearly with the total cumulative reward obtained. It is equal to 1 if you obtain the lowest possible total cumulative reward (i.e. same score as if \u0026ldquo;Easy\u0026rdquo; was selected), and it is equal to the ratio between the game difficulty level for \u0026ldquo;Medium\u0026rdquo; (or \u0026ldquo;Hard\u0026rdquo;) and the game difficulty level for \u0026ldquo;Easy\u0026rdquo; if you obtain the highest possible total cumulative reward.\nSo, for example, for Dead or Alive ++, the weighting values for \u0026ldquo;Medium\u0026rdquo; and \u0026ldquo;Hard\u0026rdquo; vary linearly between\n$$ \\begin{equation} \\begin{gathered} k_M = \\left[1.0, \\frac{3}{2} \\right] = \\left[1.0, 1.5 \\right] \\\\ k_H = \\left[1.0, \\frac{4}{2} \\right] = \\left[1.0, 2.0 \\right] \\end{gathered} \\end{equation} $$\nScoring as a function of Total Cumulative Reward and Submission Difficulty "},{"uri":"https://docs.diambra.ai/envs/","title":"Environments","tags":[],"description":"","content":"Index Overview Interaction Basics Settings Environment Settings Episode Settings Game Settings Player Settings Action Space(s) Observation Space Global Player Specific Reward Function This page describes in details all general aspects related to DIAMBRA Arena environments. For game-specific details visit Games \u0026amp; Specifics page.\nOverview DIAMBRA Arena is a software package featuring a collection of high-quality environments for Reinforcement Learning research and experimentation. It provides a standard interface to popular arcade emulated video games, offering a Python API fully compliant with OpenAI Gym/Gymnasium format, that makes its adoption smooth and straightforward.\nIt supports all major Operating Systems (Linux, Windows and MacOS) and can be easily installed via Python PIP, as described in the installation section. It is completely free to use, the user only needs to register on the official website.\nIn addition, its GitHub repository provides a collection of examples covering main use cases of interest that can be run in just a few steps.\nMain Features All environments are episodic Reinforcement Learning tasks, with discrete actions (gamepad buttons) and observations composed by screen pixels plus additional numerical data (RAM states like characters health bars or characters stage side).\nThey all support both single player (1P) as well as two players (2P) mode, making them the perfect resource to explore all the following Reinforcement Learning subfields:\nStandard RL Competitive Multi-Agent Competitive Human-Agent Self-Play Imitation Learning Human-in-the-Loop Available Games Interfaced games have been selected among the most popular fighting retro-games. While sharing the same fundamental mechanics, they provide different challenges, with specific features such as different type and number of characters, how to perform combos, health bars recharging, etc.\nWhenever possible, games are released with all hidden/bonus characters unlocked.\nAdditional details can be found in their dedicated section.\nInteraction Basics DIAMBRA Arena Environments usage follows the standard RL interaction framework: the agent sends an action to the environment, which process it and performs a transition accordingly, from the starting state to the new state, returning the observation and the reward to the agent to close the interaction loop. The figure below shows this typical interaction scheme and data flow.\nScheme of Agent-Environment Interaction The shortest snippet for a complete basic execution of an environment consists of just a few lines of code, and is presented in the code block below:\n#!/usr/bin/env python3 import diambra.arena def main(): # Environment creation env = diambra.arena.make(\u0026#34;doapp\u0026#34;, render_mode=\u0026#34;human\u0026#34;) # Environment reset observation, info = env.reset(seed=42) # Agent-Environment interaction loop while True: # (Optional) Environment rendering env.render() # Action random sampling actions = env.action_space.sample() # Environment stepping observation, reward, terminated, truncated, info = env.step(actions) # Episode end (Done condition) check if terminated or truncated: observation, info = env.reset() break # Environment shutdown env.close() # Return success return 0 if __name__ == \u0026#39;__main__\u0026#39;: main() More complex and complete examples can be found in the Examples section.\nSettings All environments have different options that can be specified using a dedicated EnvironmentSettings class. They are nested as follows:\nEnvironment settings: defined only when the environment is instantiated, they never change throughout the agent-environment interaction (e.g. the action space or the frame size) Episode settings: defined first when the environment is instantiated, they can be changed each time a new episode starts, i.e. at every environment reset call, passing a dictionary containing the key-value pairs for the settings of interest through the options keyword argument. These settings are further divided in: Game settings: they specify features of the game (e.g. difficulty level) Player settings: they specify player-related aspects (e.g. selected character and its outfits) Settings specifications when instantiating the environment is done by passing the EnvironmentSettings class, properly filled, to the environment make call as follows:\nfrom diambra.arena import EnvironmentSettings # Settings specification settings = EnvironmentSettings() settings.setting_1 = value_1 settings.setting_2 = value_2 env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) The first argument is the game_id string, it specifies the game to execute among those available (see games list for details).\nEpisode settings specification at reset is done by passing the episode_settings dictionary to the environment reset call as follows:\n# Episode settings episode_settings = { setting_1: value_1, setting_2: value_2, } env.reset(options=episode_settings) Some of them are shared among all environments and are presented here below, while others are specific to the selected game and can be found in the game-specific pages listed here.\nThe tables in the next sections lists all the attributes of the setting classes.\nUse dictionaries to specify settings As for previous versions of the library, it is also possible to specify environment settings through a dictionary and use a dedicated function to load it into the EnvironmentSettings class:\nfrom diambra.arena import EnvironmentSettings, load_settings_flat_dict # Settings specification settings = { setting_1: value_1, setting_2: value_2, } settings = load_settings_flat_dict(EnvironmentSettings, settings) env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings) When using multi-agents (two players) environments, the settings class to be passed to the environment make method is the EnvironmentSettingsMultiAgent instead of EnvironmentSettings, while all the attributes will remain the same.\nFollowing OpenAI Gym/Gymnasium standard, also the seed can be specified at reset using env.reset(seed=seed, options=episode_settings), but please note that:\n• It can be directly passed through the settings dictionary when the environment is instantiated and the environment will take care of setting it at the first reset call\n• When explicitly passed to the reset keyword argument, it should only be passed to the very first reset method call and never after it\nTwo ready-to-use examples showing how environment settings are used can be found here and here.\nEnvironment Settings Name Type Default Value(s) Value Range Description frame_shape tuple of three int (H, W, C) (0, 0, 0) H, W: [0, 512]\nC: 0 or 1 If active, resizes the frame and/or converts it from RGB to grayscale.\nCombinations:\n(0, 0, 0) - Deactivated;\n(H, W, 0) - RBG frame resized to H X W;\n(0, 0, 1) - Grayscale frame;\n(H, W, 1) - Grayscale frame resized to H X W. action_space* SpaceTypes MULTI_DISCRETE DISCRETE / MULTI_DISCRETE Defines the type of the action space n_players int 1 [1, 2] Selects single player or two players mode step_ratio int 6 [1, 6] Defines how many steps the game (emulator) performs for every environment step splash_screen bool True True / False Activates displaying the splash screen when launching the environment *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nEpisode Settings Game Settings Name Type Default Value(s) Value Range Description difficulty None U int None Game-specific min and max values allowed Specifies game difficulty (1P only) continue_game float 0.0 [0.0, 1.0]: probability of continuing game at game over\nint(abs(-inf, -1.0]): number of continues at game over before episode to be considered done Defines if and how to allow ”Continue” when the agent is about to face the game over condition show_final bool False True / False Activates displaying of final animation when game is completed Other variable game settings are found in the game-specific pages where applicable.\nPlayer Settings Environment settings depending on the specific game and shared among all of them are reported in the table below. Additional ones (if present) are reported in game-dedicated pages.\nName Type Default Value(s) Value Range Description role* None U Roles None P1 (left), P2 (right), None (50% P1, 50% P2) Selects role for the player, which also affects the side positioning at round starts characters* None U str U tuple of maximum three str None Game-specific lists of characters that can be selected Specifies character(s) to use outfits* int 1 Game-specific min and max values allowed Defines the number of outfits to draw from at character selection Example of Dead or Alive ++ available outfits for Kasumi Other variable player settings are found in the game-specific pages where applicable.\n*: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nNone values specify a Random behavior for the correspondent parameter.\nAction Space(s) Actions of the interfaced games can be grouped in two categories: move actions (Up, Left, etc.) and attack ones (Punch, Kick, etc.). DIAMBRA Arena provides two different action spaces: Discrete and MultiDiscrete. The former is a single list composed by the union of move and attack actions (of type gymnasium.spaces.Discrete), while the latter consists of two sets combined, for move and attack actions respectively (of type gymnasium.spaces.MultiDiscrete). The complete visual description of available action spaces is shown in the figure below, where both choices are presented via the correspondent gamepad buttons configuration for Dead Or Alive ++.\nWhen run in 2P mode, the environment is provided with a Dictionary action space (type gymnasium.spaces.Dict) populated with two items, identified by keys agent_0 and agent_1, whose values are either gymnasium.spaces.Discrete or gymnasium.spaces.MultiDiscrete as described above.\nEach game has specific action spaces since attack buttons (and their combinations) are, in general, game-dependent. For this reason, in each game-dedicated page, a table like the one found below is reported, describing both actions spaces for the specific game.\nIn Discrete action spaces:\nThere is only one ”no-op” action, that covers both the ”no-move” and ”no-attack” actions. The total number of actions available is Nm + Na − 1 where Nm is the number of move actions (no-move included) and Na is the number of attack actions (no-attack included). Only one action, either move or attack, can be sent for each environment step. In MultiDiscrete action spaces:\nThere is only one ”no-op” action, that covers both the ”no-move” and ”no-attack” actions. The total number of actions available is Nm × Na. Both move and attack actions can be sent at the same time for each environment step. All and only meaningful actions are made available per each game: they are sufficient to cover the entire spectrum of moves and combos for all the available characters. If a specific game has Button-1 and Button-2 among its available actions, and not Button-1 + Button-2, it means that the latter has no effect in any circumstance, considering all characters in all conditions.\nSome actions (especially attack buttons combinations) may have no effect for some of the characters: in some games combos requiring attack buttons combinations are valid only for a subset of characters.\nExample of Dead Or Alive ++ Action Spaces For every game, a table containing the following info is reported. It provides numerical details about action spaces sizes.\nType Space Size (Number of Actions) Discrete / MultiDiscrete Total number of actions available, divided in move and attack actions Observation Space Environment observations are composed by two main elements: a visual one (the game frame) and an aggregation of quantitative values called RAM states (stage number, health values, etc.). Both of them are exposed through an observation space of type gym.spaces.Dict. It consists of global elements and player-specific ones, they are presented and described in the tables below. To give additional context, next figure shows an example of Dead Or Alive ++ observation where some of the RAM States are highlighted, superimposed on the game frame.\nEach game specifies and extends the set presented here with its custom one, described in the game-dedicated page.\nAn example of Dead Or Alive ++ RAM states Global Global elements of the observation space are unrelated to the player and they are currently limited to those presented and described in the following table. The same table is found on each game-dedicated page reporting its specs:\nKey Type Value Range Description frame Box Game-specific min and max values for each dimension Latest game frame (RGB pixel screen) stage Box Game-specific min and max values Current stage of the game timer Box [0, round duration] Round time remaining Player specific Player-specific observations are nested under the key indicating the player they are referred to (i.e. \u0026quot;P1\u0026quot; and \u0026quot;P2\u0026quot;). A code example is shown below for the side RAM state of the two players:\n# P1 side P1_side = observation[\u0026#34;P1\u0026#34;][\u0026#34;side\u0026#34;] # P2 side P2_side = observation[\u0026#34;P2\u0026#34;][\u0026#34;side\u0026#34;] Typical values that are available for each game are reported and described in the table below. The same table is found in every game-dedicated page, specifying and extending (if needed) the observation elements set.\nKey Type Value Range Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, max number of rounds] Number of rounds won by the player for the current stage character Discrete [0, max number of characters - 1] Index of character in use health Box [0, max health value] Health bar value Additional observations, both new and derived from specific processing of the above ones, can be obtained via the wide choice of Environment Wrappers, see the dedicated page for details.\nReward Function The default reward is defined as a function of characters health values so that, qualitatively, damage suffered by the agent corresponds to a negative reward, and damage inflicted to the opponent corresponds to a positive reward. The quantitative, general and formal reward function definition is as follows:\n$$ \\begin{equation} R_t = \\sum_i^{0,N_c}\\left(\\bar{H_i}^{t^-} - \\bar{H_i}^{t} - \\left(\\hat{H_i}^{t^-} - \\hat{H_i}^{t}\\right)\\right) \\end{equation} $$\nWhere:\n$\\bar{H}$ and $\\hat{H}$ are health values for opponent’s character(s) and agent’s one(s) respectively; $t^-$ and $t$ are used to indicate conditions at ”state-time” and at ”new state-time” (i.e. before and after environment step); $N_c$ is the number of characters taking part in a round. Usually is $N_c = 1$ but there are some games where multiple characters are used, with the additional possible option of alternating them during gameplay, like Tekken Tag Tournament where 2 characters have to be selected and two opponents are faced every round (thus $N_c = 2$); The lower and upper bounds for the episode total cumulative reward are defined in the equations (Eqs. 2) below. They consider the default reward function for game execution with Continue Game option set equal to 0.0 (Continue not allowed).\n$$ \\begin{equation} \\begin{gathered} \\min{\\sum_t^{0,T_s}R_t} = - N_c \\left( \\left(N_s-1\\right) \\left(N_r-1\\right) + N_r\\right) \\Delta H \\\\ \\max{\\sum_t^{0,T_s}R_t} = N_c N_s N_r \\Delta H \\end{gathered} \\end{equation} $$\nWhere:\n$N_r$ is the number of rounds to win (or lose) in order to win (or lose) a stage; $T_s$ is the terminal state, reached when either $N_r$ rounds are lost (for both 1P and 2P mode) or game is cleared (for 1P mode only); $t$ represents the environment step and for an episode goes from 0 to $T_s$; $N_s$ is the maximum number of stages the agent can play before the game reaches $T_s$. $\\Delta H = H_{max} - H_{min}$ is the difference between the maximum and the mimnimum health values for the given game; ususally, but not always, $H_{min} = 0$. For 1P mode $N_s$ is game-dependent, while for 2P mode $N_s=1$, meaning the episode always ends after a single stage (so after $N_r$ rounds have been won / lost be the same player, either agent_0 or agent_1).\nFor 2P mode, agent_0 reward is defined as $R$ in the reward Eq. 1 and agent_1 reward is equal to $-R$ (zero-sum games). Eq. 1 describes the default reward function. It is of course possible to tweak it at will by means of custom Reward Wrappers.\nThe minimum and maximum total cumulative reward for the round can be different than $N_c\\Delta H$ in some cases. This may happen because:\nWhen multiple characters are used at the same time, the round_done condition can be different for different games (e.g. either at least one character has zero health or all characters have zero health) impacting on the amount of reward collected. For some games, health bars can be recharged (e.g. the character in background in Tekken Tag Tournament, or Gill\u0026rsquo;s resurrection move in Street Fighter III), making available an extra amount of reward to be collected or lost in that round. For some games, in some stages, additional opponents may be faced (opponent $N_c$ not constant through stages), making available an extra amount of reward to be collected (e.g. the endurance stages in Ultimate Mortal Kombat 3). For some games, not all characters share the same maximum health. $H_{max}$ and $H_{min}$ are always the extremes for a given game, among all characters. Lower and upper bounds of episode total cumulative reward may, in some cases, deviate from what defined by Eqs. 2, because:\nThe absolute value of minimum / maximum total cumulative reward for the round can be different from $N_c\\Delta H$ (see above). For some games, $N_r$ is not the same for all the stages (1P mode only), for example for Tekken Tag Tournament the final stage is made of a single round while all previous ones require two wins. Please note that the maximum cumulative reward (for 1P mode) is obtained when clearing the game winning all rounds with a perfect ($\\max{\\sum_t^{0,T_s}R_t}\\Rightarrow$ game completed), but the vice versa is not true. In fact, not necessarily the higher number of stages won, the higher is the total cumulative reward ($\\max{\\sum_t^{0,T_s}R_t}\\not\\propto$ stage reached, game completed $\\nRightarrow\\max{\\sum_t^{0,T_s}R_t}$). Somehow counter intuitively, in order to obtain the lowest possible total cumulative reward the agent is supposed to reach the final stage (collecting negative rewards in all previous ones) before loosing by $N_r$ perfects.\nNormalized Reward If a normalized reward is considered, the total cumulative reward equation becomes:\n$$ \\begin{equation} R_t = \\frac{\\sum_i^{0,N_c}\\left(\\bar{H_i}^{t^-} - \\bar{H_i}^{t} - \\left(\\hat{H_i}^{t^-} - \\hat{H_i}^{t}\\right)\\right)}{N_k \\Delta H} \\end{equation} $$\nWith the following additional term at the denominator:\n$N_k$ is the reward normalization factor defined through our reward nomralization wrapper. The normalization term at the denominator ensures that a round won with a perfect (i.e. without losing any health), generates always the same maximum total cumulative reward (for the round) accross all games, equal to $N_c/N_k$.\n"},{"uri":"https://docs.diambra.ai/envs/games/tektagt/","title":"Tekken Tag Tournament","tags":[],"description":"","content":" Index Game Specific Info Specific Episode Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID tektagt Original ROM Name tektagtac.zip\nRename the rom from tektagtac.zip to tektagt.zip SHA256 Checksum 57be777eae0ee9e1c035a64da4c0e7cb7112259ccebe64e7e97029ac7f01b168 Search Keywords TEKKEN TAG TOURNAMENT [ASIA] (CLONE), tekken-tag-tournament-asia-clone, 108661, wowroms Game Resolution\n(H X W X C) 240px X 512px X 3 Number of Moves and Attack Actions 9, 13 (6)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-12): No-Attack, Left Punch, Right Punch, Left Kick, Right Kick, Tag, Left Punch+Right Punch, Left Punch+Left Kick, Left Punch+Right Kick, Right Punch+Left Kick, Right Punch+Right Kick, Right Punch+Tag, Left Kick+Right Kick Max Difficulty (1P Mode) 9 Number of Characters (Selectable) 39 (38) Max Number of Outfits 5 (Note that some characters have a lower number of available outfits) Number of Stages (1P Mode) 8 Specific Episode Settings Game Settings Key Type Default Value(s) Value Range difficulty None U int None [1, 9] Player Settings Key Type Default Value(s) Value Range characters* None U str U tuple of maximum three str None Xiaoyu, Yoshimitsu, Nina, Law, Hwoarang, Eddy, Paul, King, Lei, Jin, Baek, Michelle, Armorking, Gunjack, Anna, Brian, Heihachi, Ganryu, Julia, Jun, Kunimitsu, Kazuya, Bruce, Kuma, Jack-Z, Lee, Wang, P.Jack, Devil, True Ogre, Ogre, Roger, Tetsujin, Panda, Tiger, Angel, Alex, Mokujin outfits* int 1 [1, 2] *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nAction Spaces Type Space Size (Number of Actions) Discrete 9 (moves) + 13 (attacks) - 1 (no-op counted twice) = 21 MultiDiscrete 9 (moves) X 13 (attacks) = 117 Observation Space Some examples of Tekken Tag Tournament RAM states Global Key Type Value Range Description frame Box [0, 255] X [240 X 512 X 3] Latest game frame (RGB pixel screen) stage Box [1, 8] Current stage of the game timer Box [0, 60] Round time remaining Player specific Key Type Value Range Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, 2] Number of rounds won by the player char_1 Discrete [0, 38] Index of first character slot\n0: Xiaoyu, 1: Yoshimitsu, 2: Nina, 3: Law, 4: Hwoarang, 5: Eddy, 6: Paul, 7: King, 8: Lei, 9: Jin, 10: Baek, 11: Michelle, 12: Armorking, 13: Gunjack, 14: Anna, 15: Brian, 16: Heihachi, 17: Ganryu, 18: Julia, 19: Jun, 20: Kunimitsu, 21: Kazuya, 22: Bruce, 23: Kuma, 24: Jack-Z, 25: Lee, 26: Wang, 27: P.Jack, 28: Devil, 29: True Ogre, 30: Ogre, 31: Roger, 32: Tetsujin, 33: Panda, 34: Tiger, 35: Angel, 36: Alex, 37: Mokujin, 38: Unknown char_2 Discrete [0, 38] Index of second character slot\n0: Xiaoyu, 1: Yoshimitsu, 2: Nina, 3: Law, 4: Hwoarang, 5: Eddy, 6: Paul, 7: King, 8: Lei, 9: Jin, 10: Baek, 11: Michelle, 12: Armorking, 13: Gunjack, 14: Anna, 15: Brian, 16: Heihachi, 17: Ganryu, 18: Julia, 19: Jun, 20: Kunimitsu, 21: Kazuya, 22: Bruce, 23: Kuma, 24: Jack-Z, 25: Lee, 26: Wang, 27: P.Jack, 28: Devil, 29: True Ogre, 30: Ogre, 31: Roger, 32: Tetsujin, 33: Panda, 34: Tiger, 35: Angel, 36: Alex, 37: Mokujin, 38: Unknown char Discrete [0, 38] Index of character in use\n0: Xiaoyu, 1: Yoshimitsu, 2: Nina, 3: Law, 4: Hwoarang, 5: Eddy, 6: Paul, 7: King, 8: Lei, 9: Jin, 10: Baek, 11: Michelle, 12: Armorking, 13: Gunjack, 14: Anna, 15: Brian, 16: Heihachi, 17: Ganryu, 18: Julia, 19: Jun, 20: Kunimitsu, 21: Kazuya, 22: Bruce, 23: Kuma, 24: Jack-Z, 25: Lee, 26: Wang, 27: P.Jack, 28: Devil, 29: True Ogre, 30: Ogre, 31: Roger, 32: Tetsujin, 33: Panda, 34: Tiger, 35: Angel, 36: Alex, 37: Mokujin, 38: Unknown health_1 Box [0, 227] Health bar value for first character in use health_2 Box [0, 227] Health bar value for second character in use active_char Discrete (Binary) [0, 1] Index of the active character\n0: first, 1: second bar_status Discrete [0, 4] Status of the background character health bar\n0: reserve health bar almost filled, 1: small amount of health lost, recharging in progress, 2: large amount of health lost, recharging in progress, 3: rage mode on, combo attack ready, 4: no background character (final boss) "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/","title":"How To Submit An Agent","tags":[],"description":"","content":"The basic process to submit an agent consists in the following steps:\nWrite a python script in which your agent interact with the environment exactly as if you were performing evaluation in your local machine Store your trained agent\u0026rsquo;s scripts and weights (if any) in a private repository and create a personal access token to the repository (in our examples we will use GitHub). Submit the agent using our Command Line Interface specifying your private repository files path, your secret token and one of the public pre-built dependencies images we provide In our DIAMBRA Agents repo we provide many examples, ranging from a trivial random agent to RL agents trained with state-of-the-art RL libraries.\nIn the subsections linked below, we guide you through this process, starting from the easiest use case and building upon it to show you how to leverage the most advanced features.\nSubmit pre-built Agents Submit your own Agent Custom dependencies image "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/submitprebuiltagents/","title":"Submit Pre-Built Agents","tags":[],"description":"","content":"To get the feeling of how an agent submission works, you can leverage our pre-built agents. In DIAMBRA Agents repo, together with different source code examples, we also provide pre-built docker images (packages) for some of them.\nFor example, here you find the pre-built docker image for the random agent correspondent to this source code. As indicated by the python script settings, this random agent will play using a \u0026ldquo;Random\u0026rdquo; character in a random game.\nUsing this pre-built docker image you can easily perform your first submission ever on DIAMBRA platform, and appear in the official online leaderboard by simply typing in your preferred shell the following command:\ndiambra agent submit diambra/agent-random-1:main If you want to specify the game on which to run the random agent, use the --gameId command line argument that our pre-built image accepts, when submitting the docker image as follows: diambra agent submit diambra/agent-random-1:main --gameId tektagt. Additional similar use cases are covered in the \u0026ldquo;Arguments and Commands\u0026rdquo; page.\nAfter running the command, you will receive a submission confirmation, its identification number as well as the url where to see the results, something similar to the following:\ndiambra agent submit diambra/agent-random-1:main 🖥️ (178) Agent submitted: https://diambra.ai/submission/178 By default, the submission will select the lowest difficulty level (Easy) of the three available (Easy, Medium, Hard). To change this, you can add the --submission.difficulty argument: diambra agent submit --submission.difficulty Medium diambra/agent-random-1:main\nAs shown here, it is possible to embed your agent files (i.e. scripts and weights) in the dependencies docker image and submit only that. Keep in mind that this image needs to be public and will be visible on the platform, so every user will be able to use it for his own submissions.\n"},{"uri":"https://docs.diambra.ai/envs/games/umk3/","title":"Ultimate Mortal Kombat 3","tags":[],"description":"","content":" Index Game Specific Info Specific Episode Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID umk3 Original ROM Name umk3r10.zip\nRename the rom from umk3r10.zip to umk3.zip SHA256 Checksum f48216ad82f78cb86e9c07d2507be347f904f4b5ae354a85ae7c34d969d265af Search Keywords ULTIMATE MORTAL KOMBAT 3 (CLONE), ultimate-mortal-kombat-3-clone, 109574, wowroms Game Resolution\n(H X W X C) 254px X 500px X 3 Number of Moves and Attack Actions 9, 7 (7)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-6): (No-Attack, High Punch, High Kick, Low Kick, Low Punch, Run, Block) Max Difficulty (1P Mode) 5 Number of Characters (Selectable) 26 (22) Max Number of Outfits 1 Number of Stages (1P Mode) 8 (Tower 1), 9 (Tower 2), 10 (Tower 3), 11 (Tower 4) Specific Episode Settings Game Settings Key Type Default Value(s) Value Range difficulty None U int None [1, 5] Additional Game Settings Key Type Default Value(s) Value Range Description tower int 3 [1, 4] Selects the tower to play in (1P mode only) Player Settings Key Type Default Value(s) Value Range characters* None U str U tuple of maximum three str None Kitana, Reptile, Kano, Sektor, Kabal, Sonya, Mileena, Sindel, Sheeva, Jax, Ermac, Stryker, Shang Tsung, Nightwolf, Sub-Zero-2, Cyrax, Liu Kang, Jade, Sub-Zero, Kung Lao, Smoke, Skorpion outfits* int 1 [1, 1] *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nAction Spaces Type Space Size (Number of Actions) Discrete 9 (moves) + 7 (attacks) - 1 (no-op counted twice) = 15 MultiDiscrete 9 (moves) X 7 (attacks) = 63 Observation Space Some examples of Ultimate Mortal Kombat 3 RAM states Global Key Type Value Description frame Box [0, 255] X [254 X 500 X 3] Latest game frame (RGB pixel screen) stage Box [1, 11] Current stage of the game timer Box [0, 100] Round time remaining Player specific Key Type Value Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, 2] Number of rounds won by the player char Discrete [0, 25] Index of character in use\n0: Kitana, 1: Reptile, 2: Kano, 3: Sektor, 4: Kabal, 5: Sonya, 6: Mileena, 7: Sindel, 8: Sheeva, 9: Jax, 10: Ermac, 11: Stryker, 12: Shang Tsung, 13: Nightwolf, 14: Sub-Zero-2, 15: Cyrax, 16: Liu Kang, 17: Jade, 18: Sub-Zero, 19: Kung Lao, 20: Smoke, 21: Skorpion, 22: Human Smoke, 23: Noob Saibot, 24: Motaro\u0026quot;, 25: Shao Kahn health Box [0, 166] Health bar value aggressor_bar Box [0, 48] Aggressor bar value "},{"uri":"https://docs.diambra.ai/gettingstarted/examples/wrappersoptions/","title":"Wrappers Options","tags":[],"description":"","content":"This example focuses on:\nWrappers settings configuration Wrapped observation visualization A dedicated section describing environment wrappers settings is presented here.\nimport diambra.arena from diambra.arena import SpaceTypes, EnvironmentSettings, WrappersSettings def main(): # Environment settings settings = EnvironmentSettings() settings.action_space = SpaceTypes.MULTI_DISCRETE # Gym wrappers settings wrappers_settings = WrappersSettings() ### Generic wrappers # Number of no-Op actions to be executed # at the beginning of the episode (0 by default) wrappers_settings.no_op_max = 0 # Number of steps for which the same action should be sent (1 by default) wrappers_settings.repeat_action = 1 ### Reward wrappers # Wrapper option for reward normalization # When activated, the reward normalization factor can be set (default = 0.5) # The normalization is performed by dividing the reward value # by the product of the factor times the value of the full health bar # reward = reward / (C * fullHealthBarValue) wrappers_settings.normalize_reward = True wrappers_settings.normalization_factor = 0.5 # If to clip rewards (False by default) wrappers_settings.clip_reward = False ### Action space wrapper(s) # Limit the action space to single attack buttons, removing attack buttons combinations (False by default) wrappers_settings.no_attack_buttons_combinations = False ### Observation space wrapper(s) # Frame resize operation spec (deactivated by default) # WARNING: for speedup, avoid frame warping wrappers, # use environment\u0026#39;s native frame wrapping through # \u0026#34;frame_shape\u0026#34; setting (see documentation for details). wrappers_settings.frame_shape = (128, 128, 1) # Number of frames to be stacked together (1 by default) wrappers_settings.stack_frames = 4 # Frames interval when stacking (1 by default) wrappers_settings.dilation = 1 # Add last action to observation (False by default) wrappers_settings.add_last_action = True # How many past actions to stack together (1 by default) # NOTE: needs \u0026#34;add_last_action_to_observation\u0026#34; wrapper to be active wrappers_settings.stack_actions = 6 # If to scale observation numerical values (False by default) # optionally exclude images from normalization (False by default) # and optionally perform one-hot encoding also on discrete binary variables (False by default) wrappers_settings.scale = True wrappers_settings.exclude_image_scaling = True wrappers_settings.process_discrete_binary = False # If to make the observation relative to the agent as a function to its role (P1 or P2) (deactivate by default) # i.e.: # - In 1P environments, if the agent is P1 then the observation \u0026#34;P1\u0026#34; nesting level becomes \u0026#34;own\u0026#34; and \u0026#34;P2\u0026#34; becomes \u0026#34;opp\u0026#34; # - In 2P environments, if \u0026#34;agent_0\u0026#34; role is P1 and \u0026#34;agent_1\u0026#34; role is P2, then the player specific nesting levels observation (P1 - P2) # are grouped under \u0026#34;agent_0\u0026#34; and \u0026#34;agent_1\u0026#34;, and: # - Under \u0026#34;agent_0\u0026#34;, \u0026#34;P1\u0026#34; nesting level becomes \u0026#34;own\u0026#34; and \u0026#34;P2\u0026#34; becomes \u0026#34;opp\u0026#34; # - Under \u0026#34;agent_1\u0026#34;, \u0026#34;P1\u0026#34; nesting level becomes \u0026#34;opp\u0026#34; and \u0026#34;P2\u0026#34; becomes \u0026#34;own\u0026#34; wrappers_settings.role_relative = True # Flattening observation dictionary and filtering # a sub-set of the RAM states wrappers_settings.flatten = True wrappers_settings.filter_keys = [\u0026#34;stage\u0026#34;, \u0026#34;timer\u0026#34;, \u0026#34;action\u0026#34;, \u0026#34;own_side\u0026#34;, \u0026#34;opp_side\u0026#34;, \u0026#34;own_health\u0026#34;, \u0026#34;opp_health\u0026#34;, \u0026#34;opp_character\u0026#34;] env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings, wrappers_settings, render_mode=\u0026#34;human\u0026#34;) observation, info = env.reset(seed=42) env.unwrapped.show_obs(observation) while True: actions = env.action_space.sample() print(\u0026#34;Actions: {}\u0026#34;.format(actions)) observation, reward, terminated, truncated, info = env.step(actions) done = terminated or truncated env.unwrapped.show_obs(observation) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Done: {}\u0026#34;.format(done)) print(\u0026#34;Info: {}\u0026#34;.format(info)) if done: observation, info = env.reset() env.unwrapped.show_obs(observation) break env.close() # Return success return 0 if __name__ == \u0026#39;__main__\u0026#39;: main() "},{"uri":"https://docs.diambra.ai/wrappers/","title":"Wrappers","tags":[],"description":"","content":"Index Generic Wrappers No Attack Buttons Combinations NoOp Steps After Reset Observation Wrappers Warp Frame Stack Frames With Optional Dilation Add Last Action to Observation Stack Actions Scale Observation Role-relative Observation Flatten and Filter Observation Action Wrappers Repeat Action Reward Wrappers Normalize Reward Clip Reward Custom Wrappers DIAMBRA Arena comes with a large number of ready-to-use wrappers and examples showing how to apply them. They cover a wide spectrum of use cases, and also provide reference templates to develop custom ones. In order to activate wrappers, one needs to properly set the WrapperSettings class attributes and provide them as input to the environment creation method, as shown in the next code block. The class attributes are described in the next sections below.\nfrom diambra.arena import EnvironmentSettings, WrapperSettings # Settings specification settings = EnvironmentSettings() # Wrapper settings specification wrapper_settings = WrapperSettings() wrapper_settings.setting_1 = value_1 wrapper_settings.setting_2 = value_2 env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings, wrapper_settings) Implementation examples and templates can be found in the code repository, here.\nUse of these functionalities can be found in this and this examples.\nGeneric Wrappers No Attack Buttons Combinations Name Type Default Value(s) Value Range Description no_attack_buttons_combinations bool False True / False Restricts the number of attack actions to single buttons, removing attack buttons combinations wrappers_settings.no_attack_buttons_combinations = True NoOp Steps After Reset Name Type Default Value(s) Value Range Description no_op_max int 0 [0, 12] Performs a maximum of N No-Action steps after episode reset wrappers_settings.no_op_max = 6 Observation Wrappers Warp Frame DEPRECATED: For speed, consider using the environment setting frame_shape that performs the same operation on the engine side, as described in this section.\nName Type Default Value(s) Value Range Target Observation Element Description frame_shape tuple of three int (H, W, C) (0, 0, 0) H, W: [0, 512]\nC: 0 or 1 frame If active, resizes the frame and/or converts it from RGB to grayscale.\nCombinations:\n(0, 0, 0) - Deactivated;\n(H, W, 0) - RBG frame resized to H X W;\n(0, 0, 1) - Grayscale frame;\n(H, W, 1) - Grayscale frame resized to H X W;\nKeeps observation element of type Box, changes its shape wrappers_settings.frame_shape = (128, 128, 1) Stack Frames With Optional Dilation Name Type Default Value(s) Value Range Target Observation Element Description stack_frames int 1 [1, 48] frame Stacks latest N frames together along the third dimension.\nKeeps observation element of type Box, changes its shape dilation int 1 [1, 48] frame Builds frame stacks adding one every N frames.\nKeeps observation element of type Box wrappers_settings.stack_frame = 4 wrappers_settings.dilation = 1 Add Last Action to Observation Name Type Default Value(s) Value Range Target Observation Element Description add_last_action bool False True / False action Adds latest action to the observation space dictionary, under the key action.\nAdds an observation element of type Discrete or MultiDiscrete depending on the action space wrappers_settings.add_last_action = True Stack Actions Name Type Default Value(s) Value Range Target Observation Element Description stack_actions int 1 [1, 48] action Stacks latest N actions together.\nChanges observation element type from Discrete or MultiDiscrete (depending on the action space) to MultiDiscrete, having N elements, each with cardinality equal to the original set(s) of the action space wrappers_settings.stack_actions = 12 For the Stack Actions wrapper to to be applied, the Add Last Action one must be activated.\nScale Observation Name Type Default Value(s) Value Range Target Observation Element Description scale bool False True / False All Activates observation scaling.\nAffects observation elements as follows:\n- Box: type kept, changes bounds to be between 0 and 1.\n- Discrete (Binary): depends on process_discrete_binary (see below).\n- Discrete: changed to MultiBinary through one-hot encoding, size equal to original Discrete cardinality.\n- MultiDiscrete: changed to N-MultiBinary through N-times one-hot encoding, N equal to original MultiDiscrete size, encoding size equal to original MultiDiscrete element cardinality exclude_image_scaling bool False True / False Frame If True, prevents scaling to be applied on the game frame process_discrete_binary bool False True / False Discrete Binary Controls how Discrete (Binary) observations are treated: - False: not affected; - True: changed to MultiBinary through one-hot encoding, size equal to 2 wrappers_settings.scale = True wrappers_settings.exclude_image_scaling = True wrappers_settings.process_discrete_binary = True Role-relative Observation Name Type Default Value(s) Value Range Target Observation Element Description role_relative bool False True / False Player specific observation Renames observation depending on the role played by the agent. For example: if the agent is playing with P1 role, P1 key becomes own and P2 key becomes opp, thus obs[\u0026quot;P1\u0026quot;][\u0026quot;key_1\u0026quot;] becomes obs[\u0026quot;own\u0026quot;][\u0026quot;key_1\u0026quot;] and obs[\u0026quot;P2\u0026quot;][\u0026quot;key_1\u0026quot;] becomes obs[\u0026quot;opp\u0026quot;][\u0026quot;key_1\u0026quot;] wrappers_settings.role_relative = True Flatten and Filter Observation Name Type Default Value(s) Value Range Target Observation Element Description flatten bool False True / False Observation Activates observation dictionary flattening, removing nested dictionaries and creating new keys joining original ones using \u0026ldquo;_\u0026rdquo; across nesting levels. For example: obs[\u0026quot;P1\u0026quot;][\u0026quot;key_1\u0026quot;] becomes obs[\u0026quot;P1_key_1\u0026quot;] filter_keys list of str [] - Observation except frame Defines the list of RAM states to keep in the observation space wrappers_settings.flatten = True wrappers_settings.filter_keys = [\u0026#34;stage\u0026#34;, \u0026#34;P1_health\u0026#34;, \u0026#34;P2_char\u0026#34;] The two operations are applied simultaneously, thus for filtering to be applied, flattening must be activated.\nAction Wrappers Repeat Action Name Type Default Value(s) Value Range Description repeat_action int 1 [1, 12] Keeps repeating the same action for N environment steps In order to use this wrapper, the step_ratio setting must be set to equal to 1.\nwrappers_settings.repeat_action = 4 Reward Wrappers Normalize Reward Name Type Default Value(s) Value Range Description normalize_reward bool False True / False Activates reward normalization. Divides reward value by the product between the normalization_factor value (see next row) and the delta between maximum and minium health bar value for the given game normalization_factor float 0.5 [-inf, +inf] Defines the normalization factor multiplying the delta between maximum and minium health bar value for the given game wrappers_settings.normalize_reward = True wrappers_settings.normalization_factor = 0.5 Clip Reward Name Type Default Value(s) Value Range Description clip_reward bool False True / False Activates reward clipping. Applies the sign function to the reward value wrappers_settings.clip_reward = True Custom Wrappers Name Type Default Value(s) Value Range Description wrappers list of pairs (list) of [WrapperClass, kwargs] [] - Applies in sequence the list of wrappers classes with their correspondend settings wrappers_settings.wrappers = \\ [[CustomWrapper1, {\u0026#34;setting1_1\u0026#34;: value1_1,\u0026#34;setting1_2\u0026#34;: value1_2}], \\ [CustomWrapper2, {\u0026#34;setting2_1\u0026#34;: value2_1,\u0026#34;setting2_2\u0026#34;: value2_2}]] The custom wrappers are applied after all the activated built-in ones, if any.\n"},{"uri":"https://docs.diambra.ai/gettingstarted/examples/episoderecorder/","title":"Episode Recorder","tags":[],"description":"","content":"This example focuses on:\nEpisode recording settings configuration Controller interfacing (Gamepad or Keyboard) Even if this example shows how to record episode generated by a human player, the same wrapper can be used with any kind of agent, including trained bots. It can thus be leveraged to collect a dataset for offline training.\nA dedicated section describing the episode recording wrapper settings is presented here and provides additional details on their usage and purpose.\nDepending on the Operating System used, specific permissions may be needed in order to read the keyboard inputs.\n- On Windows, by default no specific permissions are needed. However, if you have some third-party security software you may need to white-list Python.\n- On Linux you need to add the user the input group: sudo usermod -aG input $USER\n- On Mac, it is possible you need to use the settings application to allow your program to access the input devices (see this reference).\nOfficial inputs python package reference guide can be found at this link\nimport os from os.path import expanduser import diambra.arena from diambra.arena import SpaceTypes, EnvironmentSettings, RecordingSettings from diambra.arena.utils.controller import get_diambra_controller import argparse def main(use_controller): # Environment Settings settings = EnvironmentSettings() settings.step_ratio = 1 settings.frame_shape = (256, 256, 1) settings.action_space = SpaceTypes.MULTI_DISCRETE # Recording settings home_dir = expanduser(\u0026#34;~\u0026#34;) game_id = \u0026#34;doapp\u0026#34; recording_settings = RecordingSettings() recording_settings.dataset_path = os.path.join(home_dir, \u0026#34;DIAMBRA/episode_recording\u0026#34;, game_id if use_controller else \u0026#34;mock\u0026#34;) recording_settings.username = \u0026#34;alexpalms\u0026#34; env = diambra.arena.make(game_id, settings, episode_recording_settings=recording_settings, render_mode=\u0026#34;human\u0026#34;) if use_controller is True: # Controller initialization controller = get_diambra_controller(env.get_actions_tuples()) controller.start() observation, info = env.reset(seed=42) while True: env.render() if use_controller is True: actions = controller.get_actions() else: actions = env.action_space.sample() observation, reward, terminated, truncated, info = env.step(actions) done = terminated or truncated if done: observation, info = env.reset() break if use_controller is True: controller.stop() env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--use_controller\u0026#39;, type=int, default=1, help=\u0026#39;Flag to activate use of controller\u0026#39;) opt = parser.parse_args() print(opt) main(bool(opt.use_controller)) "},{"uri":"https://docs.diambra.ai/envs/games/samsh5sp/","title":"Samurai Showdown 5 Special","tags":[],"description":"","content":" Index Game Specific Info Specific Episode Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID samsh5sp Original ROM Name samsh5sp.zip SHA256 Checksum adf33d8a02f3d900b4aa95e62fb21d9278fb920b179665b12a489bd39a6c103d Search Keywords SAMURAI SHODOWN V SPECIAL, samurai-shodown-v-special, 100347, wowroms Game Resolution\n(H X W X C) 224px X 320px X 3 Number of Moves and Attack Actions 9, 11 (5)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-10): No-Attack, Weak Slash, Medium Slash, Kick, Meditation, Weak Slash + Medium Slash (Strong Slash), Medium Slash + Kick (Surprise Attack), Weak Slash + Kick, Kick + Meditation, Weak Slash + Medium Slash + Kick (Rage), Medium Slash + Kick + Meditation Max Difficulty (1P Mode) 8 Number of Characters (Selectable) 28 (28) Max Number of Outfits 4 Number of Stages (1P Mode) 7 Specific Episode Settings Game Settings Key Type Default Value(s) Value Range difficulty None U int None [1, 8] Player Settings Key Type Default Value(s) Value Range characters* None U str U tuple of maximum three str None Kyoshiro, Jubei, Hanzo, Enja, Amakusa, Suija, Galford, Charlotte, Kusare, Sogetsu, Gaira, Ukyo, Yoshitora, Gaoh, Haohmaru, Genjuro, Shizumaru, Kazuki, Tamtam, Rasetsumaru, Rimururu, Mina, Zankuro, Nakoruru, Rera, Yunfei, Basara, Mizuki outfits* int 1 [1, 4] *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nAction Spaces Type Space Size (Number of Actions) Discrete 9 (moves) + 11 (attacks) - 1 (no-op counted twice) = 19 MultiDiscrete 9 (moves) X 11 (attacks) = 99 Observation Space Some examples of Samurai Showdown 5 Special RAM states Global Key Type Value Description frame Box [0, 255] X [224 X 320 X 3] Latest game frame (RGB pixel screen) stage Box [1, 7] Current stage of the game timer Box [0, 60] Round time remaining Player specific Key Type Value Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, 3] Number of rounds won by the player char Discrete [0, 27] Index of character in use\n0: Kyoshiro, 1: Jubei, 2: Hanzo, 3: Enja, 4: Amakusa, 5: Suija , 6: Galford, 7: Charlotte, 8: Kusare, 9: Sogetsu, 10: Gaira, 11: Ukyo, 12: Yoshitora, 13: Gaoh, 14: Haohmaru, 15: Genjuro, 16: Shizumaru, 17: Kazuki, 18: Tamtam, 19: Rasetsumaru, 20: Rimururu, 21: Mina, 22: Zankuro, 23: Nakoruru, 24: Rera, 25: Yunfei, 26: Basara, 27: Mizuki health Box [0, 125] Health bar value rage_on Discrete (Binary) [0, 1] Rage on for the player\n0: False, 1: True rage_used \u0026lt;a href=\u0026ldquo;ht action_move Discrete action_attack Discrete [0, 7] Index of latest attack action performed (no-attack, hold, punch, etc.) weapon_lost Discrete (Binary) [0, 1] Weapon lost by the player\n0: False, 1: True weapon_fight Discrete (Binary) [0, 1] Weapon fight condition triggered\n0: False, 1: True rage_bar Box [0, 100] Rage bar value weapon_bar Box [0, 120] Weapon bar value power_bar Box [0, 64] Power bar value "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/submityourownagent/","title":"Submit Your Own Agent","tags":[],"description":"","content":"These are the steps to submit your own agent:\nStore your agent files (e.g. scripts and weights) in private repository, we will use GitHub as example Create your personal access token (official docs here): Go to \u0026ldquo;Settings\u0026rdquo; in the top-right corner of the GitHub website. Click \u0026ldquo;Developer settings\u0026rdquo; at the bottom-left of the page. Click \u0026ldquo;Personal access tokens\u0026rdquo;, \u0026ldquo;Tokens (classic)\u0026rdquo; and then \u0026ldquo;Generate new token\u0026rdquo;. Give your token a name, select the necessary scopes (e.g., \u0026ldquo;repo\u0026rdquo; for accessing private repositories), and click \u0026ldquo;Generate token.\u0026rdquo; Copy the generated token and save it somewhere safe, as you won\u0026rsquo;t be able to see it again. Submit your AI agent: Choose the appropriate dependencies docker image for your submission. We provide different pre-built ones giving access to various common third party libraries Submit your agent as shown in the following examples To favor an easy start, we provide example agents files (scripts and weights) that work out-of-the-box (but are only minimally trained) in our DIAMBRA Agents repository, for both Stable Baselines 3 and Ray RLlib.\nDo not add your tokens directly in the submission YAML file, they will be publicly visible.\nExample 1: Using a Manifest File (Recommended) Assuming you are using the arena-stable-baselines3-on3.10-bullseye dependencies image, and have your agent\u0026rsquo;s files stored on GitHub, create a file named submission-manifest.yaml with the following content:\nmode: AIvsCOM image: diambra/arena-stable-baselines3-on3.10-bullseye:main command: - python - \u0026#34;/sources/agent.py\u0026#34; - \u0026#34;/sources/models/model.zip\u0026#34; sources: .: git+https://username:{{.Secrets.token}}@github.com/username/repository_name.git#ref=branch_name Replace username and repository_name.git#ref=branch_name with the appropriate values, and change image and command fields according to your specific use case.\nThen, submit your agent using the manifest file:\ndiambra agent submit --submission.secret token=your_gh_token --submission.manifest submission-manifest.yaml Replace your_gh_token with the GitHub token you saved earlier.\nNote that this will clone your entire repository (including Git LFS files) and put its content inside the /sources/ folder directly.\nSpecify Sources Explicitly Explicit sources specification will not work with Git LFS files, to submit them, the only option is to use the automatic git clone mechanism described above.\nIn case you don\u0026rsquo;t want to clone all your repository, you can explicitly specify the source files you want to download as follows:\n--- mode: AIvsCOM image: diambra/arena-stable-baselines3-on3.10-bullseye:main command: - python - \u0026#34;/sources/agent.py\u0026#34; - \u0026#34;/sources/models/model.zip\u0026#34; sources: agent.py: https://{{.Secrets.token}}@raw.githubusercontent.com/username/repo_name/path/to/trained-agent/your_agent.py models/model.zip: https://{{.Secrets.token}}@raw.githubusercontent.com/username/repo_name/path/to/nn-weights/your_model.zip In case you have multiple source files you need to use, and you want to avoid to list them all, the best way to proceed is to leverage our automatic git clone feature described above. But if you really do not want to use it, you can add the source files to a zip archive, store it in your repository and leverage our automatic unzip feature as follows:\nmode: AIvsCOM image: diambra/arena-stable-baselines3-on3.10-bullseye:main command: - python - \u0026#34;/sources/data/agent.py\u0026#34; - \u0026#34;/sources/data/models/model.zip\u0026#34; sources: data: https+unzip://{{.Secrets.token}}@raw.githubusercontent.com/username/repo_name/path/to/data/data.zip Note that in the url of the zip file to be downloaded there is an additional +unzip string following https.\nExample 2: Command Line Interface Only If you want to avoid using submission files, you can use the command line to directly submit your agent. Assuming you are using the arena-stable-baselines3-on3.10-bullseye dependencies image and have your agent\u0026rsquo;s files stored on GitHub:\ndiambra agent submit \\ --submission.mode AIvsCOM \\ --submission.source .=git+https://username:{{.Secrets.token}}@github.com/username/repository_name.git#ref=branch_name \\ --submission.secret token=your_gh_token \\ --submission.set-command \\ arena-stable-baselines3-on3.10-bullseye \\ python \u0026#34;/sources/agent.py\u0026#34; \u0026#34;/sources/models/model.zip\u0026#34; Replace username and repository_name.git#ref=branch_name with the appropriate values and your_gh_token with the GitHub token you saved earlier.\nNote that, in this case, the dependencies image and command fields we discussed above are merged together and provided as values to the last argument --submission.set-command. Use the same order and change their values according to your specific use case.\n"},{"uri":"https://docs.diambra.ai/utils/","title":"Utils","tags":[],"description":"","content":"Index Available Games ROMs Check Environment Spaces Summary Observation Inspection Base Environment Wrapped Observation Controller Interface DIAMBRA Arena comes with many different tools supporting development and debug. They provide different functionalities, all described below in the sections below where both code and output is reported.\nSource code can be found in the code repository, here.\nAvailable Games Provides a list of available games and their most important details. It is executed as shown below:\nimport diambra.arena diambra.arena.available_games(print_out=True, details=True) Output will be similar to what follows:\nTitle: Dead Or Alive ++ - GameId: doapp Difficulty levels: Min 1 - Max 4 SHA256 sum: d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Original ROM name: doapp.zip Search keywords: [\u0026#39;DEAD OR ALIVE ++ [JAPAN]\u0026#39;, \u0026#39;dead-or-alive-japan\u0026#39;, \u0026#39;80781\u0026#39;, \u0026#39;wowroms\u0026#39;] Characters list: [\u0026#39;Kasumi\u0026#39;, \u0026#39;Zack\u0026#39;, \u0026#39;Hayabusa\u0026#39;, \u0026#39;Bayman\u0026#39;, \u0026#39;Lei-Fang\u0026#39;, \u0026#39;Raidou\u0026#39;, \u0026#39;Gen-Fu\u0026#39;, \u0026#39;Tina\u0026#39;, \u0026#39;Bass\u0026#39;, \u0026#39;Jann-Lee\u0026#39;, \u0026#39;Ayane\u0026#39;] Title: Street Fighter III - GameId: sfiii3n Difficulty levels: Min 1 - Max 8 SHA256 sum: 7239b5eb005488db22ace477501c574e9420c0ab70aeeb0795dfeb474284d416 Original ROM name: sfiii3n.zip Search keywords: [\u0026#39;STREET FIGHTER III 3RD STRIKE: FIGHT FOR THE FUTUR [JAPAN] (CLONE)\u0026#39;, \u0026#39;street-fighter-iii-3rd-strike-fight-for-the-futur-japan-clone\u0026#39;, \u0026#39;106255\u0026#39;, \u0026#39;wowroms\u0026#39;] Characters list: [\u0026#39;Alex\u0026#39;, \u0026#39;Twelve\u0026#39;, \u0026#39;Hugo\u0026#39;, \u0026#39;Sean\u0026#39;, \u0026#39;Makoto\u0026#39;, \u0026#39;Elena\u0026#39;, \u0026#39;Ibuki\u0026#39;, \u0026#39;Chun-Li\u0026#39;, \u0026#39;Dudley\u0026#39;, \u0026#39;Necro\u0026#39;, \u0026#39;Q\u0026#39;, \u0026#39;Oro\u0026#39;, \u0026#39;Urien\u0026#39;, \u0026#39;Remy\u0026#39;, \u0026#39;Ryu\u0026#39;, \u0026#39;Gouki\u0026#39;, \u0026#39;Yun\u0026#39;, \u0026#39;Yang\u0026#39;, \u0026#39;Ken\u0026#39;, \u0026#39;Gill\u0026#39;] Title: Tekken Tag Tournament - GameId: tektagt Difficulty levels: Min 1 - Max 9 SHA256 sum: 57be777eae0ee9e1c035a64da4c0e7cb7112259ccebe64e7e97029ac7f01b168 Original ROM name: tektagtac.zip Search keywords: [\u0026#39;TEKKEN TAG TOURNAMENT [ASIA] (CLONE)\u0026#39;, \u0026#39;tekken-tag-tournament-asia-clone\u0026#39;, \u0026#39;108661\u0026#39;, \u0026#39;wowroms\u0026#39;] Notes: Rename the rom from tektagtac.zip to tektagt.zip Characters list: [\u0026#39;Xiaoyu\u0026#39;, \u0026#39;Yoshimitsu\u0026#39;, \u0026#39;Nina\u0026#39;, \u0026#39;Law\u0026#39;, \u0026#39;Hwoarang\u0026#39;, \u0026#39;Eddy\u0026#39;, \u0026#39;Paul\u0026#39;, \u0026#39;King\u0026#39;, \u0026#39;Lei\u0026#39;, \u0026#39;Jin\u0026#39;, \u0026#39;Baek\u0026#39;, \u0026#39;Michelle\u0026#39;, \u0026#39;Armorking\u0026#39;, \u0026#39;Gunjack\u0026#39;, \u0026#39;Anna\u0026#39;, \u0026#39;Brian\u0026#39;, \u0026#39;Heihachi\u0026#39;, \u0026#39;Ganryu\u0026#39;, \u0026#39;Julia\u0026#39;, \u0026#39;Jun\u0026#39;, \u0026#39;Kunimitsu\u0026#39;, \u0026#39;Kazuya\u0026#39;, \u0026#39;Bruce\u0026#39;, \u0026#39;Kuma\u0026#39;, \u0026#39;Jack-Z\u0026#39;, \u0026#39;Lee\u0026#39;, \u0026#39;Wang\u0026#39;, \u0026#39;P.Jack\u0026#39;, \u0026#39;Devil\u0026#39;, \u0026#39;True Ogre\u0026#39;, \u0026#39;Ogre\u0026#39;, \u0026#39;Roger\u0026#39;, \u0026#39;Tetsujin\u0026#39;, \u0026#39;Panda\u0026#39;, \u0026#39;Tiger\u0026#39;, \u0026#39;Angel\u0026#39;, \u0026#39;Alex\u0026#39;, \u0026#39;Mokujin\u0026#39;, \u0026#39;Unknown\u0026#39;] ... ROMs Check Checks ROM SHA256 checksum to validate them.\nWithout game_id specification If no game_id is specified, the ROM file provided as first argument, will be verified against all available games. It is executed as shown below:\nimport diambra.arena diambra.arena.check_game_sha_256(path=\u0026#34;path/to/specific/rom/doapp.zip\u0026#34;, game_id=None) Output will be similar to what follows:\nCorrect ROM file for Dead Or Alive ++, sha256 = d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e With game_id specification If game_id is specified, the checksum of the ROM file provided will be compared with the one of the specified game identified by game_id. It is executed as shown below:\nimport diambra.arena diambra.arena.check_game_sha_256(path=\u0026#34;path/to/specific/rom/doapp.zip\u0026#34;, game_id=\u0026#34;umk3\u0026#34;) Output will be similar to what follows:\nExpected SHA256 Checksum: f48216ad82f78cb86e9c07d2507be347f904f4b5ae354a85ae7c34d969d265af Retrieved SHA256 Checksum: d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Environment Spaces Summary Prints out a summary of Environment\u0026rsquo;s Observation and Action spaces showing nesting levels, keys, space types and low/high bounds where applicable. It is executed as shown below:\nfrom diambra.arena.gym_utils import env_spaces_summary ... env_spaces_summary(env=environment) Output will be similar to what follows:\nObservation space: observation_space[\u0026#34;action_attack\u0026#34;]: Discrete(4) observation_space[\u0026#34;action_move\u0026#34;]: Discrete(9) observation_space[\u0026#34;opp_char\u0026#34;]: Discrete(11) observation_space[\u0026#34;opp_health\u0026#34;]: Box() Space type = int32 Space high bound = 208 Space low bound = 0 observation_space[\u0026#34;opp_side\u0026#34;]: Discrete(2) observation_space[\u0026#34;opp_wins\u0026#34;]: Box() Space type = int32 Space high bound = 2 Space low bound = 0 observation_space[\u0026#34;own_char\u0026#34;]: Discrete(11) observation_space[\u0026#34;own_health\u0026#34;]: Box() Space type = int32 Space high bound = 208 Space low bound = 0 observation_space[\u0026#34;own_side\u0026#34;]: Discrete(2) observation_space[\u0026#34;own_wins\u0026#34;]: Box() Space type = int32 Space high bound = 2 Space low bound = 0 observation_space[\u0026#34;frame\u0026#34;]: Box(480, 512, 3) Space type = uint8 Space high bound = [[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]] ... [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]] Space low bound = [[[0 0 0] [0 0 0] [0 0 0] ... [0 0 0] [0 0 0] [0 0 0]] ... [[0 0 0] [0 0 0] [0 0 0] ... [0 0 0] [0 0 0] [0 0 0]]] observation_space[\u0026#34;stage\u0026#34;]: Box() Space type = int8 Space high bound = 8 Space low bound = 1 observation_space[\u0026#34;timer\u0026#34;]: Box() Space type = int8 Space high bound = 40 Space low bound = 0 Action space: action_space = Discrete(12) Space type = int64 Space n = 12 Observation Inspection Prints out a detailed description of Environment\u0026rsquo;s observation content, showing every level of it. The only element that is not printed in the terminal is the game frame that, when viz input argument is set to True, is shown in dedicated graphical windows, one per channel (i.e. RGB channels are shown separately). The wait_key parameter defines how this window(s) behaves: when set equal to 0, it pauses waiting for the user to press a button, while if set different from zero, it waits the prescribed number of milliseconds before continuing.\nThe same method works on both the basic Gym Environment and the wrapped one.\nenv.show_obs(observation=obs, wait_key=1, viz=True) Use of this functionality can be found in this, this, this and this examples.\nBase Environment Output will be similar to what follows:\nFrame(s) visualization window(s): Terminal printout: observation[\u0026#34;frame\u0026#34;]: shape (128, 128, 3) - min 0 - max 255 observation[\u0026#34;stage\u0026#34;]: [1] observation[\u0026#34;timer\u0026#34;]: [30] observation[\u0026#34;P1\u0026#34;][\u0026#34;char\u0026#34;]: 0 / Kasumi observation[\u0026#34;P1\u0026#34;][\u0026#34;health\u0026#34;]: [66] observation[\u0026#34;P1\u0026#34;][\u0026#34;side\u0026#34;]: 0 observation[\u0026#34;P1\u0026#34;][\u0026#34;wins\u0026#34;]: [0] observation[\u0026#34;P2\u0026#34;][\u0026#34;char\u0026#34;]: 3 / Bayman observation[\u0026#34;P2\u0026#34;][\u0026#34;health\u0026#34;]: [184] observation[\u0026#34;P2\u0026#34;][\u0026#34;side\u0026#34;]: 1 observation[\u0026#34;P2\u0026#34;][\u0026#34;wins\u0026#34;]: [0] Wrapped Observation Output will be similar to what follows:\nFrame stack visualization windows: Terminal printout:\nobservation[\u0026#34;frame\u0026#34;]: shape (128, 128, 4) - min 0 - max 255 observation[\u0026#34;stage\u0026#34;]: [0.0] observation[\u0026#34;timer\u0026#34;]: [0.875] observation[\u0026#34;own_char\u0026#34;]: [0 0 0 1 0 0 0 0 0 0 0] / Bayman observation[\u0026#34;own_health\u0026#34;]: [0.8173076923076923] observation[\u0026#34;own_side\u0026#34;]: [0 1] observation[\u0026#34;own_wins\u0026#34;]: [0.] observation[\u0026#34;opp_char\u0026#34;]: [1 0 0 0 0 0 0 0 0 0 0] / Kasumi observation[\u0026#34;opp_health\u0026#34;]: [0.8028846153846154] observation[\u0026#34;opp_side\u0026#34;]: [1 0] observation[\u0026#34;opp_wins\u0026#34;]: [0.] observation[\u0026#34;action\u0026#34;] (reshaped for visualization): [[1 0 0 0 0 0 0 0 0 1 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 1 1 0 0 0] [1 0 0 0 0 0 0 0 0 0 0 1 0] [0 1 0 0 0 0 0 0 0 1 0 0 0] [0 1 0 0 0 0 0 0 0 1 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 0 0] [1 0 0 0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 0 1 0 0 1 0 0 0] [0 0 0 0 0 0 1 0 0 1 0 0 0] [0 0 0 1 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 1 1 0 0 0]] Controller Interface It allows to easily interface a common USB Gamepad or the Keyboard to DIAMBRA Arena environments, to be used for experiments where human input is required, for example Human Expert Demonstration Collection or Competitive Human-Agent. The following code snippet shows a typical usage.\nimport diambra.arena from diambra.arena.utils.controller import get_diambra_controller ... # Environment Initialization ... # Controller initialization controller = get_diambra_controller(env.get_actions_tuples()) controller.start() ... # Player-Environment interaction loop while condition: ... actions = controller.get_actions() ... ... controller.stop() Use of this functionality can be found in this example.\nDepending on the Operating System used, specific permissions may be needed in order to read the keyboard inputs.\n- On Windows, by default no specific permissions are needed. However, if you have some third-party security software you may need to white-list Python.\n- On Linux you need to add the user the input group: sudo usermod -aG input $USER\n- On Mac, it is possible you need to use the settings application to allow your program to access the input devices (see this reference).\nOfficial inputs python package reference guide can be found at this link\n"},{"uri":"https://docs.diambra.ai/gettingstarted/examples/datasetloader/","title":"Dataset Loader","tags":[],"description":"","content":"This example focuses on:\nRecorded episode dataset loader class usage A dedicated section describing the dataset loader is presented here and provides additional details on its usage and purpose.\nfrom diambra.arena.utils.diambra_data_loader import DiambraDataLoader import argparse import os def main(dataset_path_input): if dataset_path_input is not None: dataset_path = dataset_path_input else: base_path = os.path.dirname(os.path.abspath(__file__)) dataset_path = os.path.join(base_path, \u0026#34;dataset\u0026#34;) data_loader = DiambraDataLoader(dataset_path) n_loops = data_loader.reset() while n_loops == 0: observation, action, reward, terminated, truncated, info = data_loader.step() print(\u0026#34;Observation: {}\u0026#34;.format(observation)) print(\u0026#34;Action: {}\u0026#34;.format(action)) print(\u0026#34;Reward: {}\u0026#34;.format(reward)) print(\u0026#34;Terminated: {}\u0026#34;.format(terminated)) print(\u0026#34;Truncated: {}\u0026#34;.format(truncated)) print(\u0026#34;Info: {}\u0026#34;.format(info)) data_loader.render() if terminated or truncated: n_loops = data_loader.reset() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--dataset_path\u0026#39;, type=str, default=None, help=\u0026#39;Path to dataset\u0026#39;) opt = parser.parse_args() print(opt) main(opt.dataset_path) "},{"uri":"https://docs.diambra.ai/envs/games/kof98umh/","title":"The King of Fighers &#39;98 UMH","tags":[],"description":"","content":" Index Game Specific Info Specific Episode Settings Action Spaces Observation Space Global Player Specific Game Specific Info Game ID kof98umh Original ROM Name kof98umh.zip SHA256 Checksum beb7bdea87137832f5f6d731fd1abd0350c0cd6b6b2d57cab2bedbac24fe8d0a Search Keywords The King Of Fighters '98: Ultimate Match HERO, kof98umh, allmyroms Game Resolution\n(H X W X C) 240px X 320px X 3 Number of Moves and Attack Actions 9, 9 (5)\nMoves (0-8): No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left\nAttacks (0-8): No-Attack, Weak Punch, Weak Kick, Strong Punch, Strong Kick, Weak Punch + Weak Kick, Strong Punch + Strong Kick, Weak Punch + Weak Kick + Strong Punch + Strong Kick, Weak Punch + Weak Kick + Strong Punch Max Difficulty (1P Mode) 8 Number of Characters (Selectable) 45 (43) Max Number of Outfits 4 Number of Stages (1P Mode) 7 Specific Episode Settings Game Settings Key Type Default Value(s) Value Range difficulty None U int None [1, 8] Player Settings Key Type Default Value(s) Value Range characters* None U str U tuple of maximum three str None Kyo, Benimaru, Daimon, Terry, Andy, Joe, Ryo, Robert, Yuri, Leona, Ralf, Clark, Athena, Kensou, Chin, Chizuru, Mai, King, Kim, Chang, Choi, Yashiro, Shermie, Chris, Yamazaki, Mary, Billy, Iori, Mature, Vice, Heidern, Takuma, Saisyu, Heavy-D!, Lucky, Brian, Eiji, Kasumi, Shingo, Rugal, Geese, Krauser, Mr.Big outfits* int 1 [1, 4] Additional Player Settings Key Type Default Value(s) Value Range Description fighting_style* None U int None [1, 3] Selects the fighting style.\n1: Advanced, 2: Extra, 3: Ultimate ultimate_style* None U tuple of three int None [1, 2] X [1, 2] X [1, 2] Selects details about ultimate fighting style for Dash, Evade and Bar features.\n1: Advanced, 2: Extra *: must be provided as tuples of two elements (for agent_0 and agent_1 respectively) when using the environments in two players mode.\nAction Spaces Type Space Size (Number of Actions) Discrete 9 (moves) + 9 (attacks) - 1 (no-op counted twice) = 17 MultiDiscrete 9 (moves) X 9 (attacks) = 81 Observation Space Some examples of The King of Fighters '98 Ultimate Match RAM states Global Key Type Value Description frame Box [0, 255] X [240 X 320 X 3] Latest game frame (RGB pixel screen) stage Box [1, 7] Current stage of the game timer Box [0, 60] Round time remaining Player specific Key Type Value Description side Discrete (Binary) [0, 1] Side of the stage where the player is\n0: Left, 1: Right wins Box [0, 3] Number of rounds won by the player char_1 Discrete [0, 44] Index of first character slot\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi char_2 Discrete [0, 44] Index of second character slot\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi char_3 Discrete [0, 44] Index of third character slot\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi char Discrete [0, 44] Index of character in use\n0: Kyo, 1: Benimaru, 2: Daimon, 3: Terry, 4: Andy, 5: Joe, 6: Ryo, 7: Robert, 8: Yuri, 9: Leona, 10: Ralf, 11: Clark, 12: Athena, 13: Kensou, 14: Chin, 15: Chizuru, 16: Mai, 17: King, 18: Kim, 19: Chang, 20: Choi, 21: Yashiro, 22: Shermie, 23: Chris, 24: Yamazaki, 25: Mary, 26: Billy, 27: Iori, 28: Mature, 29: Vice, 30: Heidern, 31: Takuma, 32: Saisyu, 33: Heavy-D!, 34: Lucky, 35: Brian, 36: Eiji, 37: Kasumi, 38: Shingo, 39: Rugal, 40: Geese, 41: Krauser, 42: Mr.Big, 43: Goenitz, 44: Orochi health Box [-1, 119] Health bar value power_bar Box [0, 100] Power bar value special_attacks Box [0, 5] Number of special attacks available bar_type Discrete [0, 7] Index of bar type\n0: Advanced / Ultimate (Dash Advanced, Evade Advanced, Bar Advanced), 1: Extra / Ultimate (Dash Extra, Evade Extra, Bar Extra), 2: Ultimate (Dash Extra, Evade Advanced, Bar Advanced), 3: Ultimate (Dash Advanced, Evade Advanced, Bar Extra), 4: Ultimate (Dash Extra, Evade Advanced, Bar Extra), 5: Ultimate (Dash Advanced, Evade Extra, Bar Advanced), 6: Ultimate (Dash Extra, Evade Extra, Bar Advanced), 7: Ultimate (Dash Advanced, Evade Extra, Bar Extra) "},{"uri":"https://docs.diambra.ai/competitionplatform/argumentsandcommands/","title":"Arguments and Commands","tags":[],"description":"","content":"In case you want to specify command line arguments and/or overriding the image entrypoint at submission time, you can leverage the command line interface. Here are the different use cases covered:\nAdd arguments to a given docker image diambra agent submit \u0026lt;docker image\u0026gt; arg1 arg2 the correspondent submission manifest would use a new args keyword as follows:\n--- image: \u0026lt;docker image\u0026gt; mode: AIvsCOM difficulty: easy args: - arg1 - arg2 Add arguments to a given submission manifest diambra agent submit --submission.manifest manifest.yaml arg1 arg2 arg3 the resulting submission manifest sent to the platform would be\n--- image: diambra/agent-random-1:main mode: AIvsCOM difficulty: easy args: - arg1 - arg2 - arg3 Override entrypoint of a given image diambra agent submit --submission.set-command \u0026lt;docker image\u0026gt; command arg1 arg2 the correspondent submission manifest would be:\n--- image: \u0026lt;docker image\u0026gt; mode: AIvsCOM difficulty: easy command: - command - arg1 - arg2 Override entrypoint of an image specified in a given submission manifest diambra agent submit --submission.set-command --submission.manifest manifest.yaml command arg1 arg2 the resulting submission manifest sent to the platform would be\n--- image: diambra/agent-random-1:main mode: AIvsCOM difficulty: easy command: - command - arg1 - arg2 "},{"uri":"https://docs.diambra.ai/competitionplatform/howtosubmitanagent/customdependenciesimage/","title":"Custom Dependencies Image","tags":[],"description":"","content":"Instead of using the pre-built dependencies docker images we provide, you may want/need to create custom ones. It can easily be done in just a few steps:\nCreate the Dockerfile containing the custom dependencies you need. Any mix of publicly available packages and repository, and copies of libraries you have in your local system work.\nBuild the docker image with your custom dependencies:\ndocker build -t \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; . This will create the docker image and tag it. You can use any public registry, like quay.io or dockerhub, but make sure the image is public.\nPush the image to the registry:\ndocker push \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; Examples of docker files to create dependencies images are on the Arena repo, where we automatically build images for the RL libraries we support.\nOnce these steps are completed, you can submit the agent to the platform using your custom dependencies images. Assuming you are in the very same situation explained in the examples shown in the Submit Your Own Agent page, you would tweak them, respectively, as follows:\nExample 1: Using a Manifest File (Recommended)\nUpdate the image name in the submission manifest:\nmode: AIvsCOM image: \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; command: - python - \u0026#34;/sources/agent.py\u0026#34; - \u0026#34;/sources/models/model.zip\u0026#34; sources: agent.py: https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/trained-agent/your_agent.py models/model.zip: https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/nn-weights/your_model.zip Example 2: Command Line Interface Only\nUpdate the image name command line argument:\ndiambra agent submit \\ --submission.mode AIvsCOM \\ --submission.source agent.py=https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/trained-agent/your_agent.py \\ --submission.source models/model.zip=https://{{.Secrets.token}}@raw.githubusercontent.com/path/to/nn-weights/your_model.zip \\ --submission.secret token=your_gh_token \\ --submission.set-command \\ \u0026lt;registry\u0026gt;/\u0026lt;name\u0026gt;:\u0026lt;tag\u0026gt; \\ python \u0026#34;/sources/agent.py\u0026#34; \u0026#34;/sources/models/model.zip\u0026#34; Please note that the dependencies docker images needs to be public and will be publicly visible on the platform. Make sure you do not include in them any file you want to keep private.\nCurrently, we can only process Docker images built for amd64 CPU architecture. So, if you are using MacOS with M1 or M2 CPUs, you need to explicitly tell Docker to do that at build time as follows:\n1. Open Docker Desktop Dashboard / Preferences (cog icon) / Turn \u0026ldquo;Experimental Features\u0026rdquo; on \u0026amp; apply\n2. Create a new builder instance with docker buildx create --use\n3. Run docker buildx build --platform linux/amd64 --push -t \u0026lt;image-tag\u0026gt; .\nNote that:\n- If you can’t see an “Experimental Features” option, sign up for the Docker developer program\n- You have to push directly to a repository instead of doing it after build\n"},{"uri":"https://docs.diambra.ai/imitationlearning/","title":"Imitation Learning","tags":[],"description":"","content":" Index Episode Recording Wrapper Dataset Loader With the goal of supporting the exploration and research of techniques dealing with offline learning based on data (e.g. offline Reinforcement Learning, Imitation Learning, Behavioral Cloning, etc.), DIAMBRA Arena comes with two, easy-to-use, useful features: an environment wrapper allowing to record episodes (to be used, for example, to save human expert demonstrations), and a loader class allowing to easily access the stored dataset.\nEpisode Recording Wrapper In order to activate the episode recording wrapper, one needs to properly set the RecordingSettings class attributes and provide them as input to the environment creation method, as shown in the next code block.\nfrom diambra.arena import EnvironmentSettings, WrapperSettings, RecordingSettings # Settings specification settings = EnvironmentSettings() # Wrapper settings specification wrapper_settings = WrapperSettings() # Recording settings specification recording_settings = RecordingSettings() recording_settings.setting_1 = value_1 recording_settings.setting_2 = value_2 env = diambra.arena.make(\u0026#34;doapp\u0026#34;, settings, wrappers_settings, recording_settings) The class attributes are described in the following table:\nName Type Default Value(s) Description username None U str None Provides an identifier to be associated with the recorded episode dataset_path None U str None Specifies the path where to save recorded episodes episode_recording_settings.username = \u0026#34;user\u0026#34; episode_recording_settings.dataset_path = \u0026#34;/home/user/DIAMBRA/\u0026#34; Use of this functionality can be found in this example.\nImplementation examples and templates can be found in the code repository, here.\nBy default, this wrapper acts before wrappers are applied. Thus, it will store the original, native, unprocessed observations (and reward and actions) as generated by the base environment. This guarantees a better generality and transferability of the generated dataset, but requires preprocessing at load time.\nDataset Loader DIAMBRA Arena provides a simple dedicated class DiambraDataLoader demonstrating how to load and minimally process recorded episodes, it only requires the dataset folder path as input parameter and can be customized adding the additional processing operations required. The data loader class is created as follows:\nfrom diambra.arena.utils.diambra_data_loader import DiambraDataLoader data_loader = DiambraDataLoader(dataset_path) Use of this functionality can be found in this example.\nImplementation of this class can be found in the code repository, here.\n"},{"uri":"https://docs.diambra.ai/handsonreinforcementlearning/","title":"Hands-on Reinforcement Learning","tags":[],"description":"","content":"Index Learning RL End-To-End DeepRL What is the best path leading a passionate coder to the creation of a trained AI agent capable of effectively playing a video game? It consists in two steps: learning reinforcement learning and applying it.\nLearning RL section below deals with how to get started with RL: it presents resources that cover the basics and the most advanced details of the latest, best-performing algorithms.\nThen, in the End-to-end Deep Reinforcement Learning section, some of the most important tech tools are presented together with a step-by-step guide showing how to successfully train a Deep RL agent in our environments.\nLearning Reinforcement Learning Books The first suggested step is to learn the basics of Reinforcement Learning. The best option to do so is Sutton \u0026amp; Barto\u0026rsquo;s book \u0026ldquo;Reinforcement Learning: An Introduction\u0026rdquo;, that can be considered the reference text for the field. An additional option is Packt\u0026rsquo;s \u0026ldquo;The Reinforcement Learning Workshop\u0026rdquo; that covers theory but also a good amount of practice, being very hands-on and complemented by a GitHub repo with worked exercises.\nReinforcement Learning: An Introduction - Sutton \u0026 Barto • Link The Reinforcement Learning Workshop - Palmas et al. • Link Courses / Video-lectures An additional useful resource is represented by courses and/or video-lectures. The three listed in this paragraph, in particular, are extremely valuable. The first one, \u0026ldquo;DeepMind Reinforcement Learning Lectures at University College London\u0026rdquo;, is a collection of lectures dealing with RL in general, as Sutton \u0026amp; Barto\u0026rsquo;s book, providing the solid foundations of the field. The second one, \u0026ldquo;OpenAI Spinning Up with Deep RL\u0026rdquo;, is a very useful website providing a step-by-step primer focused on Deep RL, guiding the reader from the basics to understanding the most important algorithms down to the implementation details. The third one, \u0026ldquo;Berkeley Deep RL Bootcamp\u0026rdquo;, provides video and slides dealing specifically with Deep RL too, and presents a wide overview of the most important, state-of-the-art methods in the field. These are all extremely useful and available for free.\nDeepMind Reinforcement Learning Lectures at University College London • Link OpenAI Spinning Up with Deep RL • Link Berkeley Deep RL Bootcamp • Link Research Publications After having acquired solid fundamentals, as usual in the whole ML domain, one should rely on publications to keep the pace of field advancements. Conference papers, peer-reviewed journal and open access publications are all options to consider.\nA good starting point is to read the reference paper for all state-of-the-art algorithms implemented in the most important RL libraries (see next section), as found for example here (SB3) and here (RAY RLlib).\nOpen Access\n(Arxiv, etc.) International Conferences\n(ICML, NeurIPS, ICLR, etc.) Peer-reviewed Journals\n(ELSEVIER, Springer, etc.) More Finally, additional sources of useful information to better understand this field, and to get inspired by its great potential, are documentaries presenting notable milestones achieved by some of the best AI labs in the world. They showcase reinforcement learning masterpieces, such as AlphaGo/AlphaZero, OpenAI Five and Gran Turismo Sophy, mastering the games of Go, DOTA 2 and Gran Turismo® 7 respectively.\nDeepMind\nAlphaGo The Movie • Link OpenAI\nArtificial Gamer • Link Sony AI\nGran Turismo® Sophy • Link End-to-End Deep Reinforcement Learning Reinforcement Learning Libraries If one wants to rely on already implemented RL algorithms, focusing his efforts on higher level aspects such as policy network architecture, features selection, hyper-parameters tuning, and so on, the best choice is to leverage state-of-the-art RL libraries as the ones shown below. There are many different options, here we list those that, in our experience, are recognized as the leaders in the field, and have been proven to achieve good performances in DIAMBRA Arena environments.\nThere are multiple advantages related to the use of these libraries, to name a few: they provide high quality RL algorithms, efficiently implemented and continuously tested, they allow to natively parallelize environment execution, and in some cases they even support distributed training using multiple GPUs in a single workstation or even in cluster contexts.\nThe next section provides guidance and examples using some of the options listed down here.\nSheepRL • Link Stable Baselines 3 • Link Ray RLlib • Link Creating an Agent All the examples presented in these sections (plus additional code) showing how to interface DIAMBRA Arena with the major reinforcement learning libraries, can be found in our open source repository DIAMBRA Agents.\nScripted Agents The classical way to create an agent able to play a game is to hand-code the rules governing its behavior. These rules can vary from very simple heuristics to very complex behavioral trees, but they all have in common the need of an expert coder that knows the game and is able to distill the key elements of it to craft the scripted bot.\nThe following are two examples of (very simple) scripted agents interfaced with our environments, and they are available here: DIAMBRA Agents - Basic.\nNo-Action Agent This agent simply performs the \u0026ldquo;No-Action\u0026rdquo; action at every step. By convention it is the action with index 0, and it needs to be a single value for Discrete action spaces, and a tuple of 0s for MultiDiscrete ones, as shown in the snippet below.\n#!/usr/bin/env python3 import diambra.arena from diambra.arena import SpaceTypes, Roles, EnvironmentSettings from diambra.arena.utils.gym_utils import available_games import random import argparse def main(game_id=\u0026#34;random\u0026#34;, test=False): game_dict = available_games(False) if game_id == \u0026#34;random\u0026#34;: game_id = random.sample(game_dict.keys(),1)[0] else: game_id = opt.gameId if opt.gameId in game_dict.keys() else random.sample(game_dict.keys(),1)[0] # Settings settings = EnvironmentSettings() settings.step_ratio = 6 settings.frame_shape = (128, 128, 1) settings.role = Roles.P2 settings.difficulty = 4 settings.action_space = SpaceTypes.MULTI_DISCRETE env = diambra.arena.make(game_id, settings) observation, info = env.reset() while True: action = env.get_no_op_action() observation, reward, terminated, truncated, info = env.step(action) if terminated or truncated: observation, info = env.reset() if info[\u0026#34;env_done\u0026#34;] or test is True: break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--gameId\u0026#39;, type=str, default=\u0026#34;random\u0026#34;, help=\u0026#39;Game ID\u0026#39;) parser.add_argument(\u0026#39;--test\u0026#39;, type=int, default=0, help=\u0026#39;Test mode\u0026#39;) opt = parser.parse_args() print(opt) main(opt.gameId, bool(opt.test)) Random Agent This agent simply performs a random action at every step. In this case, the sampling method takes care of generating an action that is consistent with the environment action space.\n#!/usr/bin/env python3 import diambra.arena from diambra.arena import SpaceTypes, Roles, EnvironmentSettings from diambra.arena.utils.gym_utils import available_games import random import argparse def main(game_id=\u0026#34;random\u0026#34;, test=False): game_dict = available_games(False) if game_id == \u0026#34;random\u0026#34;: game_id = random.sample(game_dict.keys(),1)[0] else: game_id = opt.gameId if opt.gameId in game_dict.keys() else random.sample(game_dict.keys(),1)[0] # Settings settings = EnvironmentSettings() settings.step_ratio = 6 settings.frame_shape = (128, 128, 1) settings.role = Roles.P2 settings.difficulty = 4 settings.action_space = SpaceTypes.MULTI_DISCRETE env = diambra.arena.make(game_id, settings) observation, info = env.reset() while True: action = env.action_space.sample() observation, reward, terminated, truncated, info = env.step(action) if terminated or truncated: observation, info = env.reset() if info[\u0026#34;env_done\u0026#34;] or test is True: break # Close the environment env.close() # Return success return 0 if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--gameId\u0026#39;, type=str, default=\u0026#34;random\u0026#34;, help=\u0026#39;Game ID\u0026#39;) parser.add_argument(\u0026#39;--test\u0026#39;, type=int, default=0, help=\u0026#39;Test mode\u0026#39;) opt = parser.parse_args() print(opt) main(opt.gameId, bool(opt.test)) More complex scripts can be built in similar ways, for example continuously performing user-defined combos moves, or adding some more complex choice mechanics. But this would still require to decide the tactics in advance, properly translating knowledge into code. A different approach would be to leverage reinforcement learning, so that the agent will improve leveraging its own experience.\nDeepRL Trained Agents An alternative approach to scripted agents is adopting reinforcement learning, and the following sections provide examples on how to do that with the most important libraries in the domain.\nDIAMBRA Arena natively provides interfaces to SheepRL, Stable Baselines 3, and Ray RLlib, allowing to easily train models with them on our environments. Each library-dedicated page presents some basic and advanced examples.\nSheepRL Stable Baselines 3 Ray RLlib DIAMBRA Arena provides a working interface with Stable Baselines 2 too, but it is deprecated and will be discontinued in the near future.\n"},{"uri":"https://docs.diambra.ai/competitionplatform/","title":"Competition Platform","tags":[],"description":"","content":" Our competition platform allows you to submit your agents and compete with other coders around the globe in epic video games tournaments!\nIt features a public global leaderboard where users are ranked by the best score achieved by their agents in our different environments.\nIt also offers you the possibility to unlock cool achievements depending on the performances of your agent.\nSubmitted agents are evaluated and their episodes are streamed on our Twitch channel.\nWe aimed at making the submission process as smooth as possible, try it now! You find all the details in the sub-pages linked below.\nBasic Agent Script Submission Evaluation How to Submit an Agent Arguments and Commands Test Your Agent Locally "},{"uri":"https://docs.diambra.ai/competitionplatform/testyouragentlocally/","title":"Test Your Agent Locally","tags":[],"description":"","content":"If you want to test your agent locally before submitting it for evaluation on the platform, you can use the specific feature provided by our command line interface. The pattern of the command is the very same used for submission, except that instead of the submit option you will use test.\nIt can be used to make sure the agent behaves as expected, and to debug it in case it fails, without waiting for the online evaluation pipeline.\nIt works with both plain docker images as well as submission manifests with privately hosted files and secret tokens, using respectively, the following commands:\ndiambra agent test \u0026lt;docker image\u0026gt; or\ndiambra agent test --submission.secret token=\u0026lt;my-secret token\u0026gt; --submission.manifest submission.yaml "},{"uri":"https://docs.diambra.ai/projects/","title":"Projects","tags":[],"description":"","content":" This section contains a collection of projects that have been developed using DIAMBRA.\nIf you want to add yours, you can fork the docs repo and submit a Pull Request or get in touch on our Discord server and send us the material.\nProject List RLZ Tournament Game Painter "},{"uri":"https://docs.diambra.ai/","title":"Home","tags":[],"description":"","content":"DIAMBRA Docs DIAMBRA Arena Index Overview Installation Quickstart Examples RL Libs Compatibility \u0026amp; State-of-the-Art Agents Competition Platform Docs Stucture Support, Feature Requests \u0026amp; Bugs Reports References Citation Terms of Use Overview DIAMBRA Arena is a software package featuring a collection of high-quality environments for Reinforcement Learning research and experimentation. It provides a standard interface to popular arcade emulated video games, offering a Python API fully compliant with OpenAI Gym/Gymnasium format, that makes its adoption smooth and straightforward.\nIt supports all major Operating Systems (Linux, Windows and MacOS) and can be easily installed via Python PIP, as described in the installation section below. It is completely free to use, the user only needs to register on the official website.\nIn addition, its GitHub repository provides a collection of examples covering main use cases of interest that can be run in just a few steps.\nAgent-Environment Interaction Scheme Main Features All environments are episodic Reinforcement Learning tasks, with discrete actions (gamepad buttons) and observations composed by screen pixels plus additional numerical data (RAM values like characters health bars or characters stage side).\nThey all support both single player (1P) as well as two players (2P) mode, making them the perfect resource to explore all the following Reinforcement Learning subfields:\nStandard RL Competitive Multi-Agent Competitive Human-Agent Self-Play Imitation Learning Human-in-the-Loop Available Games Interfaced games have been selected among the most popular fighting retro-games. While sharing the same fundamental mechanics, they provide different challenges, with specific features such as different type and number of characters, how to perform combos, health bars recharging, etc.\nWhenever possible, games are released with all hidden/bonus characters unlocked.\nAdditional details can be found in their dedicated section.\nInstallation Register on our website, it requires just a few clicks and is 100% free\nInstall Docker Desktop (Linux | Windows | MacOS) and make sure you have permissions to run it (see here). On Linux, it\u0026rsquo;s usually enough to run sudo usermod -aG docker $USER, log out and log back in.\nInstall DIAMBRA Command Line Interface: python3 -m pip install diambra\nInstall DIAMBRA Arena: python3 -m pip install diambra-arena\nUsing a virtual environment to isolate your python packages installation is strongly suggested\nQuickstart Download Game ROM(s) and Check Validity Check available games with the following command:\ndiambra arena list-roms Output example:\n[...] Title: Dead Or Alive ++ - GameId: doapp Difficulty levels: Min 1 - Max 4 SHA256 sum: d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Original ROM name: doapp.zip Search keywords: [\u0026#39;DEAD OR ALIVE ++ [JAPAN]\u0026#39;, \u0026#39;dead-or-alive-japan\u0026#39;, \u0026#39;80781\u0026#39;, \u0026#39;wowroms\u0026#39;] Characters list: [\u0026#39;Kasumi\u0026#39;, \u0026#39;Zack\u0026#39;, \u0026#39;Hayabusa\u0026#39;, \u0026#39;Bayman\u0026#39;, \u0026#39;Lei-Fang\u0026#39;, \u0026#39;Raidou\u0026#39;, \u0026#39;Gen-Fu\u0026#39;, \u0026#39;Tina\u0026#39;, \u0026#39;Bass\u0026#39;, \u0026#39;Jann-Lee\u0026#39;, \u0026#39;Ayane\u0026#39;] [...] If you are using Windows 10 \u0026ldquo;N\u0026rdquo; editions and get this error ImportError: DLL load failed while importing cv2, you might need to install the \u0026ldquo;Media Feature Pack\u0026rdquo;.\nSearch ROMs on the web using Search Keywords provided by the game list command reported above. Pay attention, follow game-specific notes reported there, and store all ROMs in the same folder, whose absolute path will be referred in the following as /absolute/path/to/roms/folder/.\nSpecific game ROM files are required, check validity of the downloaded ROMs as follows.\nCheck ROM(s) validity running:\ndiambra arena check-roms /absolute/path/to/roms/folder/romFileName.zip The output for a valid ROM file would look like the following:\nCorrect ROM file for Dead Or Alive ++, sha256 = d95855c7d8596a90f0b8ca15725686567d767a9a3f93a8896b489a160e705c4e Make sure to check out our Terms of Use, and in particular Section 7. By using the software, you accept the in full.\nBase script A Python script to run a complete episode with a random agent requires just a few lines:\n#!/usr/bin/env python3 import diambra.arena def main(): # Environment creation env = diambra.arena.make(\u0026#34;doapp\u0026#34;, render_mode=\u0026#34;human\u0026#34;) # Environment reset observation, info = env.reset(seed=42) # Agent-Environment interaction loop while True: # (Optional) Environment rendering env.render() # Action random sampling actions = env.action_space.sample() # Environment stepping observation, reward, terminated, truncated, info = env.step(actions) # Episode end (Done condition) check if terminated or truncated: observation, info = env.reset() break # Environment shutdown env.close() # Return success return 0 if __name__ == \u0026#39;__main__\u0026#39;: main() To execute the script run:\ndiambra run -r /absolute/path/to/roms/folder/ python script.py To avoid specifying ROMs path at every run, you can define the environment variable DIAMBRAROMSPATH=/absolute/path/to/roms/folder/, either temporarily in your current shell/prompt session, or permanently in your profile (e.g. on linux in ~/.bashrc).\nExamples We provide multiple examples covering the most important use-cases, that can be used as templates and starting points to explore all the features of the software package.\nThey show how to leverage both single and two players modes, how to set up environment wrappers with all their options, how to record human expert demonstrations and how to load them to apply imitation learning.\nEvery example has a dedicated page in this documentation, and the source code is available in the code repository.\nRL Libs Compatibility \u0026amp; State-of-the-Art Agents DIAMBRA Arena is built to maximize compatibility will all major Reinforcement Learning libraries. It natively provides interfaces with the two most import packages: Stable Baselines 3 and Ray RLlib, while Stable Baselines is also available but deprecated. Their usage is illustrated in detail in the dedicated section of this documentation and in the DIAMBRA Agents repository. It can easily be interfaced with any other package in a similar way.\nNative interfaces, installed with the specific options listed below, are tested with the following versions:\nStable Baselines 3 (2.1.*) | pip install diambra-arena[stable-baselines3] Docs-GitHub-Pypi Ray RLlib (2.7.*) | pip install diambra-arena[ray-rllib] Docs-GitHub-Pypi Stable Baselines (2.10.2) | pip install diambra-arena[stable-baselines] Docs-GitHub-Pypi Competition Platform Our competition platform allows you to submit your agents and compete with other coders around the globe in epic video games tournaments!\nIt features a public global leaderboard where users are ranked by the best score achieved by their agents in our different environments.\nIt also offers you the possibility to unlock cool achievements depending on the performances of your agent.\nSubmitted agents are evaluated and their episodes are streamed on our Twitch channel.\nWe aimed at making the submission process as smooth as possible, try it now!\nDocs Structure Getting Started Environments Wrappers Utils Imitation Learning Hands-on Reinforcement Learning Competition Platform Projects Support, Feature Requests \u0026amp; Bugs Reports To receive support, use the dedicated channel in our Discord Server.\nTo request features or report bugs, use GitHub discussions and issue trackers for the repositories:\nDIAMBRA Discussions DIAMBRA Arena Issue Tracker DIAMBRA Agents Issue Tracker References Website: https://diambra.ai GitHub: https://github.com/diambra/ Paper: https://arxiv.org/abs/2210.10595 Linkedin: https://www.linkedin.com/company/diambra Discord: https://diambra.ai/discord Twitch: https://www.twitch.tv/diambra_ai YouTube: https://www.youtube.com/c/diambra_ai Twitter: https://twitter.com/diambra_ai Citation Paper: https://arxiv.org/abs/2210.10595\n@article{Palmas22, author = {{Palmas}, Alessandro}, title = \u0026#34;{DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation}\u0026#34;, journal = {arXiv e-prints}, keywords = {reinforcement learning, transfer learning, multi-agent, games}, year = 2022, month = oct, eid = {arXiv:2210.10595}, pages = {arXiv:2210.10595}, archivePrefix = {arXiv}, eprint = {2210.10595}, primaryClass = {cs.AI} } Terms of Use DIAMBRA Arena software package is subject to our Terms of Use. By using it, you accept them in full.\nDIAMBRA, Inc. © Copyright 2018-2024. All Rights Reserved. "},{"uri":"https://docs.diambra.ai/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://docs.diambra.ai/credits/","title":"Credits","tags":[],"description":"","content":"Contributors Thanks to them for making Open Source Software a better place !\n@matcornic 160 commits @matalo33 48 commits @coliff 19 commits @lierdakil 16 commits @mdavids 10 commits @ozobi 5 commits @Xipas 5 commits @Alan-Cha 4 commits @pdelaby 4 commits @helfper 4 commits @willwade 3 commits @dptelecom 3 commits @LinuxSuRen 3 commits @massimeddu 3 commits @mreithub 3 commits @Chris-Greaves 3 commits @JianLoong 2 commits @lfalin 2 commits @wikijm 2 commits @jice-lavocat 2 commits @jamesbooker 2 commits @ImgBotApp 2 commits @hucste 2 commits @denisvm 2 commits @diemol 2 commits @Oddly 1 commits @exKAZUu 1 commits @taiidani 1 commits @EnigmaCurry 1 commits @pocc 1 commits @HontoNoRoger 1 commits @razonyang 1 commits @stou 1 commits @ripienaar 1 commits @qiwenmin 1 commits @PierreAdam 1 commits @654wak654 1 commits @owulveryck 1 commits @nnja 1 commits @sandrogauci 1 commits @shelane 1 commits @mbbx6spp 1 commits @swenzel 1 commits @tedyoung 1 commits @Thiht 1 commits @editicalu 1 commits @fossabot 1 commits @kamar535 1 commits @mtbt03 1 commits @ngocbichdao 1 commits @nonumeros 1 commits @pgorod 1 commits @proelbtn 1 commits @armsnyder 1 commits @afilini 1 commits @MrAkaki 1 commits @AmirLavasani 1 commits @afs2015 1 commits @arifpedia 1 commits @berryp 1 commits @MrMoio 1 commits @ChrisLasar 1 commits @DCsunset 1 commits @IEvangelist 1 commits @fritzmg 1 commits @bogaertg 1 commits @geoffreybauduin 1 commits @giuliov 1 commits @haitch 1 commits @zeegin 1 commits @RealOrangeOne 1 commits @jared-stehler 1 commits @JohnBlood 1 commits @JohnAllen2tgt 1 commits @kamilchm 1 commits @gwleclerc 1 commits @lloydbenson 1 commits @lil5 1 commits @massimocireddu 1 commits @sykesm 1 commits @nvasudevan 1 commits And a special thanks to @vjeantet for his work on docdock, a fork of hugo-theme-learn. v2.0.0 of this theme is inspired by his work.\nPackages and libraries mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support Tooling Netlify - Continuous deployement and hosting of this documentation Hugo "},{"uri":"https://docs.diambra.ai/tags/","title":"Tags","tags":[],"description":"","content":""}]